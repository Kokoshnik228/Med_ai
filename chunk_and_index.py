#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–®–∞–≥ 6. –ß–∞–Ω–∫–∏–Ω–≥ + –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant (dense: HF/GPU –∏–ª–∏ Ollama)

–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –¥–≤–∞ –±—ç–∫–µ–Ω–¥–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:
  - --emb-backend hf       ‚Üí –ª–æ–∫–∞–ª—å–Ω–æ —á–µ—Ä–µ–∑ FlagEmbedding (GPU, –±—ã—Å—Ç—Ä–æ)
  - --emb-backend ollama   ‚Üí —á–µ—Ä–µ–∑ Ollama /api/embeddings

–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∞–≤–∫–∏:
  ‚Ä¢ –í payload —Ç–µ–ø–µ—Ä—å –ø–∏—à–µ—Ç—Å—è chunk_text (–æ–±—Ä–µ–∑–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞) –∏ —Å—Ç–∞–±–∏–ª—å–Ω—ã–π chunk_id,
    —á—Ç–æ–±—ã –Ω–∞ —ç—Ç–∞–ø–µ retrieve_hybrid –º–æ–∂–Ω–æ –±—ã–ª–æ —Ñ—å—é–∑–∏—Ç—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –ª—É—á—à–∏–µ –ß–ê–ù–ö–ò, –∞ –Ω–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.
  ‚Ä¢ –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é child_w/overlap –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –∫ –±–æ–ª–µ–µ ¬´—É–∑–∫–∏–º¬ª –æ–∫–Ω–∞–º (180/40), parent_w —É–º–µ–Ω—å—à—ë–Ω –¥–æ 500,
    —á—Ç–æ–±—ã –ø–æ–≤—ã—Å–∏—Ç—å —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–ø–∞–¥–∞–Ω–∏–π. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è—Ç—å —Ñ–ª–∞–≥–∞–º–∏/ENV.

–ü—Ä–∏–º–µ—Ä—ã:
  # HF / GPU (bge-m3)
  python chunk_and_index.py \
    --emb-backend hf --hf-model BAAI/bge-m3 \
    --pages-glob "data/*.pages.jsonl" --collection med_kb_v3 \
    --qdrant-url http://localhost:7779 --recreate --batch 512

  # Ollama embeddings
  python chunk_and_index.py \
    --emb-backend ollama --emb-model zylonai/multilingual-e5-large:latest \
    --ollama-url http://localhost:11435 \
    --pages-glob "data/*.pages.jsonl" --collection med_kb_v3 \
    --qdrant-url http://localhost:7779 --only-new --batch 128
"""
from __future__ import annotations
import os
import sys
# ----------------- imports -----------------
import argparse
import numpy as np
import hashlib
import json
import re
import time
import uuid
import unicodedata
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Set

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from tqdm import tqdm

from qdrant_client import QdrantClient
from qdrant_client.models import (
    Distance,
    VectorParams,
    PointStruct,
    Filter,
    FieldCondition,
    MatchValue,
)

try:
    import torch  # type: ignore
except Exception:
    torch = None  # –¥–æ–ø—É—Å—Ç–∏–º–æ, –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º ollama

# ----------------- –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã -----------------
MAX_EMB_CHARS_OLLAMA = 2000  # –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –ª–∏–º–∏—Ç –¥–ª—è /api/embeddings
seen_hashes_global: Set[str] = set()

# ----------------- –£—Ç–∏–ª–∏—Ç—ã ENV/—Ñ–ª–∞–≥–æ–≤ -----------------
def _env_truthy(val: Optional[str], default: bool = False) -> bool:
    if val is None:
        return default
    return val.strip().lower() in ("1", "true", "yes", "on")

def _env_int(name: str, default: int) -> int:
    try:
        return int(os.getenv(name, str(default)))
    except Exception:
        return default

# ----------------- –¢–µ–∫—Å—Ç–æ–≤—ã–µ —É—Ç–∏–ª–∏—Ç—ã -----------------
def _clean_jsonl_line(s: str) -> str:
    """
    –ú—è–≥–∫–∞—è –æ—á–∏—Å—Ç–∫–∞: —É–±–∏—Ä–∞–µ–º NUL –∏ –Ω–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∫–æ–Ω—Ç—Ä–æ–ª—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Æ–Ω–∏–∫–æ–¥.
    –ù–µ —á–∏–Ω–∏—Ç –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ JSON, –Ω–æ —É–±–∏—Ä–∞–µ—Ç –º—É—Å–æ—Ä–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –∏–∑ OCR.
    """
    if not s:
        return s
    s = s.replace("\x00", "")
    s = "".join(ch for ch in s if ch.isprintable() or ch in "\t\r\n")
    s = unicodedata.normalize("NFKC", s)
    return s

def _read_pages_robust(fp: Path) -> tuple[list[dict], int]:
    """
    –ß–∏—Ç–∞–µ—Ç .pages.jsonl –ø–æ—Å—Ç—Ä–æ—á–Ω–æ. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (pages, skipped_count).
    –ü–ª–æ—Ö–∏–µ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º (–Ω–µ –≤–∞–ª–∏–º –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å).
    """
    pages: list[dict] = []
    skipped = 0
    with fp.open("r", encoding="utf-8", errors="ignore") as f:
        for i, line in enumerate(f, 1):
            line = _clean_jsonl_line(line).strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                if isinstance(rec, dict):
                    pages.append({
                        "page": int(rec.get("page", 0) or 0),
                        "text": rec.get("text", "") or "",
                    })
            except Exception as e:
                skipped += 1
                print(f"‚ö†Ô∏è  {fp.name}: –±–∏—Ç–∞—è JSONL-—Å—Ç—Ä–æ–∫–∞ #{i}: {e}", flush=True)
    return pages, skipped

def normalize_for_hash(text: str) -> str:
    t = (text or "").lower()
    t = re.sub(r"\s+", " ", t)
    t = re.sub(r"[^0-9a-z–∞-—è—ë %/.,;:()\-\n]+", " ", t, flags=re.IGNORECASE)
    return t.strip()

def words(text: str) -> List[str]:
    return (text or "").split()

def chunk_words(tokens: List[str], max_len: int, overlap: int) -> List[List[str]]:
    chunks: List[List[str]] = []
    i, n = 0, len(tokens)
    while i < n:
        j = min(i + max_len, n)
        chunks.append(tokens[i:j])
        if j == n:
            break
        i = max(0, j - overlap)
    return chunks

def build_parent_child(
    pages: List[Dict[str, Any]],
    child_w: int,
    child_overlap: int,
    parent_w: int
) -> List[Dict[str, Any]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ child-—á–∞–Ω–∫–æ–≤ —Å –ø–æ–ª—è–º–∏: text, parent_id, page_range."""
    parents: List[Dict[str, Any]] = []
    cur_words: List[str] = []
    cur_pages: List[int] = []

    for p in pages:
        w = words(p.get("text", ""))
        if not w:
            continue
        # –†–µ–∂–µ–º –¥–ª–∏–Ω–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É –∫—É—Å–∫–∞–º–∏ –ø–æ parent_w —Å–ª–æ–≤
        for i in range(0, len(w), parent_w):
            part = w[i:i + parent_w]
            if not part:
                continue
            cur_words.extend(part)
            cur_pages.append(int(p.get("page", 0)))
            if len(cur_words) >= parent_w:
                pid = f"P{len(parents) + 1}"
                parents.append({
                    "parent_id": pid,
                    "text": " ".join(cur_words[:parent_w]),
                    "page_range": (min(cur_pages), max(cur_pages)),
                })
                cur_words, cur_pages = [], []

    if cur_words:
        pid = f"P{len(parents) + 1}"
        parents.append({
            "parent_id": pid,
            "text": " ".join(cur_words),
            "page_range": (min(cur_pages) if cur_pages else 0, max(cur_pages) if cur_pages else 0),
        })

    childs: List[Dict[str, Any]] = []
    for parent in parents:
        toks = words(parent["text"])
        for win in chunk_words(toks, child_w, child_overlap):
            childs.append({
                "parent_id": parent["parent_id"],
                "text": " ".join(win),
                "page_range": parent["page_range"],
            })
    return childs

# ----------------- –û–±—â–∏–µ —É—Ç–∏–ª–∏—Ç—ã -----------------
def l2_unit(vec: List[float]) -> List[float]:
    s = (sum(x * x for x in vec) ** 0.5) or 1.0
    return [x / s for x in vec]

def batched(it: Iterable[Any], batch_size: int) -> Iterable[List[Any]]:
    buf: List[Any] = []
    for x in it:
        buf.append(x)
        if len(buf) >= batch_size:
            yield buf
            buf = []
    if buf:
        yield buf

# ----------------- OLLAMA backend -----------------
def make_session(total_retries: int = 5, backoff: float = 0.5, timeout: int = 60) -> requests.Session:
    sess = requests.Session()
    retries = Retry(
        total=total_retries,
        backoff_factor=backoff,
        status_forcelist=[429, 500, 502, 503, 504],
        allowed_methods=["POST"],
        raise_on_status=False,
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=16, pool_maxsize=32)
    sess.mount("http://", adapter)
    sess.mount("https://", adapter)
    sess.request_timeout = timeout
    return sess

def _safe_trim(text: str, max_chars: int = MAX_EMB_CHARS_OLLAMA) -> str:
    if len(text) <= max_chars:
        return text
    cut = text[:max_chars]
    sp = cut.rfind(" ")
    return (cut if sp < 200 else cut[:sp]) + " ‚Ä¶"

def ollama_embed_one(session: requests.Session, base_url: str, model: str, text: str, timeout: int) -> List[float]:
    url = base_url.rstrip("/") + "/api/embeddings"
    payload = {"model": model, "prompt": _safe_trim(text)}
    r = session.post(url, json=payload, timeout=timeout or getattr(session, "request_timeout", 60))
    if r.status_code >= 500:
        raise RuntimeError(f"Ollama HTTP {r.status_code}")
    r.raise_for_status()
    data = r.json()
    vec = data.get("embedding") or []
    if not isinstance(vec, list) or not vec:
        raise RuntimeError("Empty embedding vector")
    return l2_unit([float(x) for x in vec])

def ollama_get_dim(session: requests.Session, base_url: str, model: str, timeout: int) -> int:
    v = ollama_embed_one(session, base_url, model, "dimension probe", timeout)
    return len(v)

def ollama_embed_batch(session: requests.Session, base_url: str, model: str, texts: List[str], timeout: int) -> List[List[float]]:
    out: List[List[float]] = []
    for t in texts:
        out.append(ollama_embed_one(session, base_url, model, t, timeout))
        time.sleep(0.02)  # –ª—ë–≥–∫–∏–π —Ç—Ä–æ—Ç—Ç–ª–∏–Ω–≥
    return out

# ----------------- HF backend (FlagEmbedding) -----------------
class HFEmbedder:
    def __init__(self, model_name: str, device_hint: Optional[str] = None, use_fp16: bool = False):
        from FlagEmbedding import BGEM3FlagModel

        # –ü—Ä–µ—Ü–∏–∑–∏–æ–Ω–∫–∞ ‚Äî –¥–ª—è –Ω–æ–≤—ã—Ö –∫–∞—Ä—Ç –¥–∞—ë—Ç —Ö–æ—Ä–æ—à–∏–π –±—É—Å—Ç –∏ —ç–∫–æ–Ω–æ–º–∏—é –û–ó–£
        if torch is not None:
            try:
                torch.set_float32_matmul_precision("medium")
            except Exception:
                pass

        # –ò—Å—Ç–æ—á–Ω–∏–∫ –¥–µ—Ñ–æ–ª—Ç–æ–≤: –∞—Ä–≥—É–º–µ–Ω—Ç ‚Üí ENV(HF_DEVICE) ‚Üí auto
        env_dev = os.getenv("HF_DEVICE", "").strip() or None
        requested = (device_hint or env_dev or "auto").lower()
        if requested in ("auto", ""):
            device = "cuda" if (torch and torch.cuda.is_available()) else "cpu"
        else:
            device = requested

        # fp16: –∞—Ä–≥—É–º–µ–Ω—Ç ‚Üí ENV(HF_FP16) ‚Üí auto(–≤–∫–ª. –Ω–∞ cuda)
        fp16_env = _env_truthy(os.getenv("HF_FP16"), default=False)
        fp16 = bool(use_fp16 or fp16_env or device.startswith("cuda"))

        # –õ–æ–≥ –¥–µ–≤–∞–π—Å–∞
        gpu_note = ""
        if device.startswith("cuda"):
            if torch and torch.cuda.is_available():
                try:
                    gpu_note = f" | GPU={torch.cuda.get_device_name(0)}  CUDA_VISIBLE_DEVICES={os.getenv('CUDA_VISIBLE_DEVICES','<unset>')}"
                except Exception:
                    gpu_note = " | GPU=?"
            else:
                print("‚ö†Ô∏è  –ó–∞–ø—Ä–æ—à–µ–Ω CUDA, –Ω–æ torch.cuda –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ CPU.", flush=True)
                device, fp16 = "cpu", False

        print(f"üß† HF embedder: {model_name} on {device} (fp16={fp16}){gpu_note}", flush=True)

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å —Ñ–æ–ª–±—ç–∫–æ–º –Ω–∞ CPU –ø—Ä–∏ –ø—Ä–æ–±–ª–µ–º–∞—Ö CUDA
        try:
            self.model = BGEM3FlagModel(model_name, use_fp16=fp16, device=device)
        except Exception as e:
            if device.startswith("cuda"):
                print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ CUDA ({e}). –§–æ–ª–±—ç–∫ –Ω–∞ CPU.", flush=True)
                self.model = BGEM3FlagModel(model_name, use_fp16=False, device="cpu")
            else:
                raise

        self.default_batch = _env_int("EMB_BATCH", 128)

    def embed_texts(self, texts, batch_size: int | None = None) -> np.ndarray:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç np.ndarray (N, D) —Å –ø–ª–æ—Ç–Ω—ã–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏.
        –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –≤—ã–¥–∞—á–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫ (dict / list / np.array).
        """
        bs = int(batch_size or getattr(self, "default_batch", 128))
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ø—Ä–æ—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ dense-–≤—ã–¥–∞—á—É, —á—Ç–æ–±—ã —ç–∫–æ–Ω–æ–º–∏—Ç—å –ø–∞–º—è—Ç—å
        try:
            out = self.model.encode(
                texts,
                batch_size=max(1, bs),
                return_dense=True,
                return_sparse=False,
                return_colbert_vecs=False,
                normalize_embeddings=True,
            )
        except TypeError:
            # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ –∑–Ω–∞—é—Ç —ç—Ç–∏—Ö —Ñ–ª–∞–≥–æ–≤
            out = self.model.encode(texts, batch_size=max(1, bs))

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞
        if isinstance(out, dict):
            if "dense_vecs" in out and out["dense_vecs"] is not None:
                vecs = out["dense_vecs"]
            elif "embeddings" in out and out["embeddings"] is not None:
                vecs = out["embeddings"]
            elif "sentence_embeddings" in out and out["sentence_embeddings"] is not None:
                vecs = out["sentence_embeddings"]
            else:
                raise ValueError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç encode(): –∫–ª—é—á–∏ {list(out.keys())}")
        else:
            vecs = out

        vecs = np.asarray(vecs, dtype=np.float32)
        if vecs.ndim == 1:
            vecs = vecs.reshape(1, -1)
        # —Å—Ç—Ä–∞—Ö–æ–≤–æ—á–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-9
        vecs = (vecs / norms).astype(np.float32)
        return vecs

    def get_dim(self) -> int:
        vecs = self.embed_texts(["probe"], batch_size=4)
        return int(vecs.shape[-1])

# ----------------- Qdrant helpers -----------------
def ensure_collection(client: QdrantClient, name: str, dim: int, recreate: bool = False) -> None:
    if recreate and client.collection_exists(name):
        print(f"‚ö†Ô∏è –ü–µ—Ä–µ—Å–æ–∑–¥–∞—ë–º –∫–æ–ª–ª–µ–∫—Ü–∏—é {name}...", flush=True)
        client.delete_collection(name)
        time.sleep(2)
    if not client.collection_exists(name):
        client.create_collection(
            collection_name=name,
            vectors_config=VectorParams(size=dim, distance=Distance.COSINE),
        )
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è {name} (dim={dim}, metric=COSINE)", flush=True)
    else:
        print(f"‚ÑπÔ∏è  –ö–æ–ª–ª–µ–∫—Ü–∏—è {name} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç (dim={dim})", flush=True)

def fetch_existing_hashes(client: QdrantClient, collection: str, doc_id: str) -> set[str]:
    existing: set[str] = set()
    scroll_filter = Filter(must=[FieldCondition(key="doc_id", match=MatchValue(value=doc_id))])
    next_offset = None
    while True:
        points, next_offset = client.scroll(
            collection_name=collection,
            scroll_filter=scroll_filter,
            limit=1000,
            offset=next_offset,
            with_payload=True,
            with_vectors=False,
        )
        if not points:
            break
        for p in points:
            th = (p.payload or {}).get("text_hash")
            if isinstance(th, str):
                existing.add(th)
        if next_offset is None:
            break
    return existing

# ----------------- MAIN -----------------
def main() -> None:
    ap = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    ap.add_argument("--pages-glob", required=True)
    ap.add_argument("--qdrant-url", default="http://qdrant:6333")
    ap.add_argument("--collection", default="med_kb_v3")

    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–µ—Ñ–æ–ª—Ç—ã (–±–æ–ª–µ–µ ¬´—É–∑–∫–∏–µ¬ª –æ–∫–Ω–∞)
    ap.add_argument("--child-w", type=int, default=180)
    ap.add_argument("--child-overlap", type=int, default=40)
    ap.add_argument("--parent-w", type=int, default=500)

    # batch –∏–∑ ENV EMB_BATCH (–¥–µ—Ñ. 256 –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω –≤ ENV)
    ap.add_argument("--batch", type=int, default=_env_int("EMB_BATCH", 256))

    ap.add_argument("--recreate", action="store_true")
    ap.add_argument("--only-new", action="store_true")
    ap.add_argument("--qdrant-wait", action="store_true")

    # Embedding backend
    ap.add_argument("--emb-backend", choices=["hf", "ollama"], default="hf")

    # HF (–¥–µ—Ñ–æ–ª—Ç—ã –∏–∑ ENV)
    ap.add_argument("--hf-model", default=os.getenv("HF_MODEL", "BAAI/bge-m3"))
    ap.add_argument("--hf-device", default=os.getenv("HF_DEVICE", None))      # –Ω–∞–ø—Ä–∏–º–µ—Ä, cuda –∏–ª–∏ cuda:0
    ap.add_argument("--hf-fp16", action="store_true", default=_env_truthy(os.getenv("HF_FP16"), default=False))

    # Ollama
    ap.add_argument("--ollama-url", default=os.getenv("OLLAMA_URL", "http://localhost:11435"))
    ap.add_argument("--emb-model", default=os.getenv("EMB_MODEL", "zylonai/multilingual-e5-large:latest"))
    ap.add_argument("--timeout", type=int, default=int(os.getenv("EMB_TIMEOUT", "180")))

    args = ap.parse_args()

    files = sorted(Path().glob(args.pages_glob))
    if not files:
        raise SystemExit(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø–æ –º–∞—Å–∫–µ: {args.pages_glob}")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ Qdrant
    print(f"üîó Qdrant URL: {args.qdrant_url}", flush=True)
    try:
        test = requests.get(args.qdrant_url.rstrip("/") + "/readyz", timeout=5)
        test.raise_for_status()
        print("‚úÖ Qdrant –¥–æ—Å—Ç—É–ø–µ–Ω.", flush=True)
    except Exception as e:
        raise SystemExit(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Qdrant ({args.qdrant_url}): {e}")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    if args.emb_backend == "hf":
        hf = HFEmbedder(args.hf_model, device_hint=args.hf_device, use_fp16=args.hf_fp16)
        dim = hf.get_dim()
        print(f"üî§ HF embeddings: {args.hf_model} (dim={dim}) | batch={args.batch}", flush=True)
        embed_fn = lambda texts: hf.embed_texts(texts, batch_size=args.batch)
    else:
        session = make_session(timeout=args.timeout)
        dim = ollama_get_dim(session, args.ollama_url, args.emb_model, args.timeout)
        print(f"üî§ Ollama embeddings: {args.emb_model} (dim={dim}) @ {args.ollama_url} | batch={args.batch}", flush=True)
        embed_fn = lambda texts: ollama_embed_batch(session, args.ollama_url, args.emb_model, texts, args.timeout)

    # Qdrant –∫–æ–ª–ª–µ–∫—Ü–∏—è
    client = QdrantClient(url=args.qdrant_url, prefer_grpc=False, grpc_port=None, timeout=60)
    ensure_collection(client, args.collection, dim=dim, recreate=args.recreate)

    total_points_upserted = 0
    total_childs_after_dedup = 0

    for fp in files:
        doc_id = fp.stem.replace(".pages", "")
        pages, bad = _read_pages_robust(fp)
        if bad:
            print(f"‚ö†Ô∏è  {doc_id}: –ø—Ä–æ–ø—É—â–µ–Ω–æ –±–∏—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {bad}", flush=True)

        if not any(p.get("text") for p in pages):
            print(f"üìÑ {doc_id}: –Ω–µ—Ç —Ç–µ–∫—Å—Ç–∞ (–ø—Ä–æ–ø—É—Å–∫)", flush=True)
            continue

        # –ß–∞–Ω–∫–∏–Ω–≥
        childs = build_parent_child(pages, args.child_w, args.child_overlap, args.parent_w)
        if not childs:
            print(f"üìÑ {doc_id}: –Ω–µ—Ç —á–∞–Ω–∫–æ–≤ (–ø—Ä–æ–ø—É—Å–∫)", flush=True)
            continue

        # –•—ç—à–∏ –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏
        for c in childs:
            c["text_hash"] = hashlib.sha1(normalize_for_hash(c["text"]).encode("utf-8")).hexdigest()

        # already-in-DB –¥–µ–¥—É–ø (–ø–æ doc_id –∏ text_hash)
        existing_hashes = fetch_existing_hashes(client, args.collection, doc_id) if args.only_new and not args.recreate else set()

        filtered: List[Dict[str, Any]] = []
        skipped_existing = 0
        for c in childs:
            h = c["text_hash"]
            if h in seen_hashes_global:
                continue
            if h in existing_hashes:
                skipped_existing += 1
                continue
            seen_hashes_global.add(h)
            filtered.append(c)
        childs = filtered
        total_childs_after_dedup += len(childs)

        print(f"üìÑ {doc_id}: –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ {len(childs)}, —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö {skipped_existing}", flush=True)
        if not childs:
            continue

        # –ë–∞—Ç—á–µ–≤—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ + upsert
        for chunk in tqdm(batched(childs, args.batch), desc=f"{doc_id}", unit="batch"):
            texts = [c["text"] for c in chunk]
            if not texts:
                continue

            try:
                emb = embed_fn(texts)  # np.ndarray (N, D) –∏–ª–∏ List[List[float]]
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –±–∞—Ç—á–∞ ({len(texts)}): {e}", flush=True)
                continue

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ List[float] –¥–ª—è Qdrant
            if isinstance(emb, np.ndarray):
                emb_rows = [row.astype(np.float32).tolist() for row in emb]
            else:
                emb_rows = [[float(x) for x in row] for row in emb]

            points: List[PointStruct] = []
            for i, c in enumerate(chunk):
                # —Å—Ç–∞–±–∏–ª—å–Ω—ã–π id —Ç–æ—á–∫–∏ (uuid5 –Ω–∞ –æ—Å–Ω–æ–≤–µ doc_id –∏ text_hash)
                unique_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, f"{doc_id}_{c['text_hash']}"))
                # —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —Ñ—å—é–∂–Ω–∞/–¥–µ–¥—É–ø–∞ –Ω–∞ —ç—Ç–∞–ø–µ retrieve
                chunk_id = f"{doc_id}:{c['parent_id']}:{c['text_hash']}"

                payload = {
                    "doc_id": doc_id,
                    "parent_id": c["parent_id"],
                    "page_start": c["page_range"][0],
                    "page_end": c["page_range"][1],
                    "len_words": len(c["text"].split()),
                    "child_len_words": len(c["text"].split()),  # –∞–ª–∏–∞—Å –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏
                    "text_hash": c["text_hash"],
                    "chunk_text": c["text"][:800],               # <‚Äî —Ç–æ—á–Ω—ã–π —Ç–µ–∫—Å—Ç —á–∞–Ω–∫–∞ (—É—Å–µ—á—ë–Ω–Ω—ã–π)
                    "chunk_id": chunk_id,                         # <‚Äî —Å—Ç–∞–±–∏–ª—å–Ω—ã–π chunk-id
                }
                points.append(PointStruct(id=unique_id, vector=emb_rows[i], payload=payload))

            client.upsert(
                collection_name=args.collection,
                points=points,
                wait=bool(args.qdrant_wait),
            )
            total_points_upserted += len(points)

    print("üìä –ò—Ç–æ–≥:", flush=True)
    print(f"  –§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(files)}", flush=True)
    print(f"  –ù–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤: {total_childs_after_dedup}", flush=True)
    print(f"  –î–æ–±–∞–≤–ª–µ–Ω–æ/–æ–±–Ω–æ–≤–ª–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {total_points_upserted}", flush=True)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant –∑–∞–≤–µ—Ä—à–µ–Ω–∞.", flush=True)

if __name__ == "__main__":
    main()
