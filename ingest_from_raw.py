#!/usr/bin/env python3
"""
RAW (PDF/DOCX/TXT) -> data/*.pages.jsonl + data/manifest.json

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ --input-dir (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é raw_docs/).
- PDF: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ OCR (tesseract –∏–ª–∏ easyocr).
- DOCX/TXT: –Ω–∞—Ä–µ–∑–∫–∞ –Ω–∞ ¬´–ø—Å–µ–≤–¥–æ-—Å—Ç—Ä–∞–Ω–∏—Ü—ã¬ª —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1800 —Å–∏–º–≤–æ–ª–æ–≤),
  —á—Ç–æ–±—ã down-stream (BM25/Qdrant/—Ü–∏—Ç–∞—Ç—ã) —Ä–∞–±–æ—Ç–∞–ª –æ–¥–∏–Ω–∞–∫–æ–≤–æ —Å PDF.
- –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ—Å—Ç—å: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–ø–æ SHA1), –µ—Å–ª–∏ –ù–ï —É–∫–∞–∑–∞–Ω --force.
- EasyOCR: ¬´—Ç—ë–ø–ª—ã–π —Å—Ç–∞—Ä—Ç¬ª –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –æ–¥–∏–Ω —Ä–∞–∑; –≤ –≤–æ—Ä–∫–µ—Ä–∞—Ö —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ.

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ app):
  pip install chardet pymupdf pillow python-docx
  # –¥–ª—è OCR (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
  apt-get install -y tesseract-ocr tesseract-ocr-eng tesseract-ocr-rus
  pip install easyocr opencv-python-headless
"""

from __future__ import annotations
import argparse
import json
import sys
import os
import re
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Optional
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
from time import perf_counter

import numpy as np
import chardet

# ---------- –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã ----------
try:
    import fitz  # PyMuPDF
except Exception as e:
    print("[ERR] –¢—Ä–µ–±—É–µ—Ç—Å—è PyMuPDF: pip install pymupdf", file=sys.stderr)
    raise

# python-docx==1.1.2
try:
    from docx import Document
except Exception:
    Document = None  # –æ–±—Ä–∞–±–æ—Ç–∞–µ–º –Ω–∏–∂–µ

try:
    import pytesseract
    from PIL import Image
    TESS_AVAILABLE = True
except Exception:
    from PIL import Image  # Pillow –≤—Å—ë —Ä–∞–≤–Ω–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è
    TESS_AVAILABLE = False

try:
    import easyocr
    EASY_AVAILABLE = True
except Exception:
    EASY_AVAILABLE = False

# OpenCV (–ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è OCR)
try:
    import cv2
    CV_AVAILABLE = True
except Exception:
    CV_AVAILABLE = False


# ================== –£—Ç–∏–ª–∏—Ç—ã ==================

def file_sha1(p: Path) -> str:
    h = hashlib.sha1()
    with p.open("rb") as f:
        for chunk in iter(lambda: f.read(1 << 20), b""):
            h.update(chunk)
    return h.hexdigest()


def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)


def detect_text_file(path: Path) -> str:
    data = path.read_bytes()
    enc = chardet.detect(data).get("encoding") or "utf-8"
    try:
        return data.decode(enc, errors="ignore")
    except Exception:
        return data.decode("utf-8", errors="ignore")


def pixmap_to_pil(pix: "fitz.Pixmap") -> "Image.Image":
    try:
        needs_colorspace_convert = getattr(pix.colorspace, "n", 3) != 3  # –Ω–µ RGB
    except Exception:
        needs_colorspace_convert = False
    if pix.alpha or needs_colorspace_convert:
        pix = fitz.Pixmap(fitz.csRGB, pix)
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
    return img


# --- —Ç–µ–∫—Å—Ç–æ–≤–∞—è —á–∏—Å—Ç–∫–∞ –¥–ª—è OCR/–ø–∞—Ä—Å–µ—Ä–æ–≤ ---
_LATIN_TO_CYR = str.maketrans({
    "A":"–ê","a":"–∞","B":"–í","E":"–ï","e":"–µ","K":"–ö","k":"–∫","M":"–ú","H":"–ù","O":"–û","o":"–æ",
    "P":"–†","p":"—Ä","C":"–°","c":"—Å","T":"–¢","X":"–•","x":"—Ö","Y":"–£","y":"—É"
})
def clean_text(text: str) -> str:
    t = (text or "").replace("\r", "")
    t = re.sub(r"-\n", "", t)                                # —É–±—Ä–∞—Ç—å –ø–µ—Ä–µ–Ω–æ—Å—ã —Å–æ –∑–Ω–∞–∫–æ–º –¥–µ—Ñ–∏—Å–∞
    t = t.replace("\n\n", "<<<PARA>>>").replace("\n", " ").replace("<<<PARA>>>", "\n\n")
    t = re.sub(r"[ \t]+", " ", t)                            # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã
    t = t.translate(_LATIN_TO_CYR)                           # —á–∞—Å—Ç—ã–µ –∑–∞–º–µ–Ω—ã –ª–∞—Ç–∏–Ω–∏—Ü—ã –Ω–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü—É
    return t.strip()


def split_text_to_pages(full_text: str, page_size_chars: int = 1800) -> List[Dict[str, Any]]:
    """–ù–∞—Ä–µ–∑–∞–µ–º –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ ¬´–ø—Å–µ–≤–¥–æ-—Å—Ç—Ä–∞–Ω–∏—Ü—ã¬ª –ø–æ —Å–∏–º–≤–æ–ª–∞–º, —Å—Ç–∞—Ä–∞—è—Å—å —É–≤–∞–∂–∞—Ç—å –∞–±–∑–∞—Ü—ã."""
    text = clean_text(full_text)
    if not text:
        return [{"page": 1, "text": ""}]

    parts: List[str] = []
    buf = []
    cur_len = 0
    # –≥—Ä—É–±–æ –ø–æ –∞–±–∑–∞—Ü–∞–º/–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
    tokens = re.split(r"(\n\n|[.!?]\s+)", text)
    for t in tokens:
        if t is None:
            continue
        if cur_len + len(t) > page_size_chars and buf:
            parts.append("".join(buf).strip())
            buf, cur_len = [t], len(t)
        else:
            buf.append(t)
            cur_len += len(t)
    if buf:
        parts.append("".join(buf).strip())

    pages: List[Dict[str, Any]] = []
    for i, chunk in enumerate(parts, start=1):
        pages.append({"page": i, "text": chunk})

    return pages or [{"page": 1, "text": text[:page_size_chars]}]


# ================== DOCX ==================

def extract_docx_text(path: Path) -> str:
    """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ DOCX (–∞–±–∑–∞—Ü—ã + —Ç–∞–±–ª–∏—Ü—ã)."""
    if Document is None:
        print("[ERR] python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤—å `python-docx` –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.", file=sys.stderr)
        return ""
    try:
        doc = Document(str(path))
    except Exception as e:
        print(f"[ERR] –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å DOCX {path.name}: {e}", file=sys.stderr)
        return ""

    parts: List[str] = []
    # –∞–±–∑–∞—Ü—ã
    for p in doc.paragraphs:
        t = (p.text or "").strip()
        if t:
            parts.append(t)
    # —Ç–∞–±–ª–∏—Ü—ã
    for table in doc.tables:
        for row in table.rows:
            cells = [ (c.text or "").strip() for c in row.cells ]
            line = " | ".join([c for c in cells if c])
            if line:
                parts.append(line)

    return "\n".join(parts).strip()


def ingest_docx(path: Path, page_size_chars: int = 1800) -> List[Dict[str, Any]]:
    """DOCX -> —Å–ø–∏—Å–æ–∫ {page, text} (–∫–∞–∫ —É PDF/TXT)."""
    raw = extract_docx_text(path)
    if not raw:
        return [{"page": 1, "text": ""}]
    return split_text_to_pages(raw, page_size_chars=page_size_chars)


# ================== OCR Backends ==================

def ocr_page_tesseract(img_pil: "Image.Image", lang: str) -> str:
    if not TESS_AVAILABLE:
        return ""
    cfg = "--oem 1 --psm 6"
    return (pytesseract.image_to_string(img_pil, lang=lang, config=cfg) or "").strip()

def ocr_page_easyocr(img_pil: "Image.Image", reader) -> str:
    if reader is None:
        return ""
    arr = np.array(img_pil.convert("RGB"))
    res = reader.readtext(arr, detail=0, paragraph=True)
    return "\n".join([x.strip() for x in res if x]).strip()

def preprocess_pil(img_pil: "Image.Image") -> "Image.Image":
    """–õ—ë–≥–∫–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è OCR (–µ—Å–ª–∏ –µ—Å—Ç—å OpenCV)."""
    if not CV_AVAILABLE:
        return img_pil
    img = np.array(img_pil.convert("L"))  # grayscale
    # CLAHE (–≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã)
    try:
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        img = clahe.apply(img)
    except Exception:
        pass
    # —à—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
    img = cv2.fastNlMeansDenoising(img, h=10)
    # –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏—è
    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY, 35, 15)
    return Image.fromarray(img)


# ================== –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ==================

def ingest_pdf(
    pdf_path: Path,
    *,
    ocr_mode: str,            # "auto"|"always"|"never"
    ocr_backend: str,         # "tesseract"|"easyocr"
    ocr_lang: str,
    dpi: int,
    min_chars: int,
    verbose: bool,
    easy_reader=None
) -> List[Dict[str, Any]]:

    pages: List[Dict[str, Any]] = []
    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å PDF {pdf_path.name}: {e}", file=sys.stderr)
        return pages

    for i, page in enumerate(doc, start=1):
        txt = (page.get_text("text") or "").strip()

        # —Ä–µ—à–∞–µ–º, –¥–µ–ª–∞—Ç—å –ª–∏ OCR
        if ocr_mode == "always":
            do_ocr = True
        elif ocr_mode == "auto":
            do_ocr = (len(txt) < min_chars)
        else:  # "never"
            do_ocr = False

        # —Å–∞–º OCR (–í–ù–ï –≤–µ—Ç–∫–∏ "never")
        if do_ocr:
            if ocr_backend == "tesseract":
                zoom = dpi / 72.0
                mat = fitz.Matrix(zoom, zoom)
                pix = page.get_pixmap(matrix=mat, alpha=False)
                img_pil = pixmap_to_pil(pix)

                img_pil = preprocess_pil(img_pil)
                txt_ocr = ocr_page_tesseract(img_pil, ocr_lang)

            elif ocr_backend == "easyocr":
                pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0), alpha=False)
                img_pil = pixmap_to_pil(pix)

                img_pil = preprocess_pil(img_pil)
                txt_ocr = ocr_page_easyocr(img_pil, easy_reader)

            else:
                txt_ocr = ""

            if len(txt_ocr) > len(txt):
                txt = txt_ocr
                if verbose:
                    print(f"[OCR-{ocr_backend}] {pdf_path.name} p.{i}: len={len(txt)}")
            elif verbose and ocr_mode != "never" and txt_ocr:
                print(f"[OCR-{ocr_backend}] {pdf_path.name} p.{i}: OCR –Ω–µ —É–ª—É—á—à–∏–ª —Ç–µ–∫—Å—Ç (len={len(txt)})")

        if txt:
            txt = clean_text(txt)

        pages.append({"page": i, "text": txt})

    return pages


def ingest_txt(txt_path: Path, page_size_chars: int = 1800) -> List[Dict[str, Any]]:
    text = detect_text_file(txt_path).strip()
    return split_text_to_pages(text, page_size_chars=page_size_chars)


# ================== –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å ==================

def choose_ocr_backend(requested: str) -> str:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π backend OCR —Å —É—á—ë—Ç–æ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫.
    priority: requested ‚Üí fallback tesseract ‚Üí 'none'
    """
    req = (requested or "").lower()
    if req == "easyocr" and EASY_AVAILABLE:
        return "easyocr"
    if req == "tesseract" and TESS_AVAILABLE:
        return "tesseract"
    if EASY_AVAILABLE:
        return "easyocr"
    if TESS_AVAILABLE:
        return "tesseract"
    return "none"  # OCR –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω


def _easyocr_models_ready(model_dir: Path) -> bool:
    try:
        mdir = model_dir / "model"
        return mdir.exists() and any(mdir.iterdir())
    except Exception:
        return False


def process_one_file(
    f: Path,
    out_dir: Path,
    *,
    min_chars: int,
    ocr_mode: str,
    ocr_backend_eff: str,
    ocr_lang: str,
    dpi: int,
    verbose: bool,
    page_size_chars: int,
    easyocr_dir: Path,
    easyocr_use_gpu: bool,
) -> Dict[str, Any]:
    """–ü—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–±–µ–∑ –¥–æ—Å—Ç—É–ø–∞ –∫ manifest). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç entry + –ø—É—Ç—å pages."""
    sha = file_sha1(f)
    stem = f.stem
    doc_id = stem  # —É–Ω–∏–∫–∞–ª–∏–∑–∞—Ü–∏—é –ø–æ doc_id —Ä–µ—à–∞–µ–º –Ω–∞ –≤–µ—Ä—Ö–Ω–µ–º —É—Ä–æ–≤–Ω–µ, –µ—Å–ª–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è
    out_pages = out_dir / f"{doc_id}.pages.jsonl"

    # OCR init (EasyOCR –≤ —ç—Ç–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ; —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –Ω–µ –¥–æ–ø—É—Å–∫–∞–µ–º)
    easy_reader = None
    if ocr_mode != "never" and ocr_backend_eff == "easyocr":
        easy_reader = easyocr.Reader(
            ['ru', 'en'],
            gpu=easyocr_use_gpu,
            model_storage_directory=str(easyocr_dir),
            download_enabled=False,
            verbose=False,
        )

    # –ü–∞—Ä—Å–∏–Ω–≥
    ext = f.suffix.lower()
    if ext == ".pdf":
        pages = ingest_pdf(
            f, ocr_mode=ocr_mode, ocr_backend=ocr_backend_eff, ocr_lang=ocr_lang,
            dpi=dpi, min_chars=min_chars, verbose=verbose, easy_reader=easy_reader
        )
    elif ext == ".docx":
        pages = ingest_docx(f, page_size_chars=page_size_chars)
    else:
        pages = ingest_txt(f, page_size_chars=page_size_chars)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    with out_pages.open("w", encoding="utf-8") as w:
        for p in pages:
            rec = {"doc_id": doc_id, "page": p.get("page", 1), "text": p.get("text", "")}
            w.write(json.dumps(rec, ensure_ascii=False) + "\n")

    empty_pages = sum(1 for p in pages if not (p.get("text") or "").strip())

    return {
        "doc_id": doc_id,
        "source_path": str(f),
        "pages": len(pages),
        "lang": "ru",
        "sha1": sha,
        "ocr_backend": ocr_backend_eff,
        "ocr_mode": ocr_mode,
        "ocr_lang": ocr_lang,
        "dpi": dpi,
        "min_chars": min_chars,
        "empty_pages": empty_pages,
        "out_pages": str(out_pages)
    }


def main():
    ap = argparse.ArgumentParser("RAW -> data/*.pages.jsonl (+manifest) —Å OCR –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π DOCX/TXT")
    ap.add_argument("--input-dir", default="raw_docs", help="–ü–∞–ø–∫–∞ —Å PDF/DOCX/TXT (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ)")
    ap.add_argument("--out-dir", default="data", help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å JSONL –∏ manifest.json")
    ap.add_argument("--force", action="store_true", help="–ü–µ—Ä–µ–ø–∞—Ä—Å–∏—Ç—å –¥–∞–∂–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π")

    # OCR-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ (–º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å —á–µ—Ä–µ–∑ env)
    ap.add_argument("--ocr-mode", choices=["auto","always","never"], default=os.getenv("OCR_MODE", "auto"),
                    help="auto: —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞; always: OCR –Ω–∞ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö; never: –±–µ–∑ OCR")
    ap.add_argument("--ocr-backend", choices=["tesseract","easyocr"], default=os.getenv("OCR_BACKEND", "easyocr"),
                    help="–ñ–µ–ª–∞–µ–º—ã–π –¥–≤–∏–∂–æ–∫ OCR (–±—É–¥–µ—Ç –∞–≤—Ç–æ-—Ñ–æ–ª–ª–±–µ–∫)")
    ap.add_argument("--ocr-lang", default=os.getenv("TESS_LANG", "rus+eng"), help="–Ø–∑—ã–∫–∏ –¥–ª—è tesseract")
    ap.add_argument("--min-chars", type=int, default=int(os.getenv("MIN_CHARS", "60")),
                    help="–ü–æ—Ä–æ–≥ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ OCR –≤ —Ä–µ–∂–∏–º–µ auto")
    ap.add_argument("--dpi", type=int, default=int(os.getenv("OCR_DPI", "300")), help="DPI —Ä–µ–Ω–¥–µ—Ä–∞ –¥–ª—è tesseract")

    # –ü—Ä–æ—á–µ–µ
    ap.add_argument("--workers", type=int, default=0, help="–ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –ø–æ —Ñ–∞–π–ª–∞–º (0=CPU count)")
    ap.add_argument("--page-size-chars", type=int, default=1800, help="–†–∞–∑–º–µ—Ä ¬´–ø—Å–µ–≤–¥–æ-—Å—Ç—Ä–∞–Ω–∏—Ü—ã¬ª –¥–ª—è DOCX/TXT")
    ap.add_argument("--verbose", action="store_true", help="–ü–æ–¥—Ä–æ–±–Ω—ã–π –ª–æ–≥")

    args = ap.parse_args()

    in_dir = Path(args.input_dir)
    out_dir = Path(args.out_dir)
    ensure_dir(out_dir)

    # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ —Å–æ–±–∏—Ä–∞–µ–º —Ñ–∞–π–ª—ã
    allowed = {".pdf", ".docx", ".txt"}
    files: List[Path] = []
    for p in in_dir.rglob("*"):
        if not p.is_file():
            continue
        ext = p.suffix.lower()
        if ext not in allowed:
            continue
        name = p.name
        if name.startswith("~$"):  # –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã MS Office
            continue
        files.append(p)
    files = sorted(files)

    if not files:
        print(f"–í {in_dir} –Ω–µ—Ç pdf/docx/txt", file=sys.stderr)
        return 1

    manifest_path = out_dir / "manifest.json"
    manifest: Dict[str, Any] = {"docs": []}
    if manifest_path.exists():
        try:
            manifest = json.loads(manifest_path.read_text(encoding="utf-8"))
            if not isinstance(manifest, dict) or "docs" not in manifest:
                manifest = {"docs": []}
        except Exception:
            manifest = {"docs": []}

    docs = manifest.get("docs", [])
    by_source: Dict[str, Dict[str, Any]] = {d.get("source_path"): d for d in docs if isinstance(d, dict)}
    existing_ids = {d.get("doc_id") for d in docs if isinstance(d, dict)}

    # –ü–ª–∞–Ω —Ä–∞–±–æ—Ç: –ø–µ—Ä–µ–ø–∞—Ä—Å–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ –∏–ª–∏ --force
    plan: List[Path] = []
    for f in files:
        sha = file_sha1(f)
        prev = by_source.get(str(f))
        if args.force or not prev or prev.get("sha1") != sha:
            plan.append(f)
        elif args.verbose:
            try:
                rel = f.relative_to(in_dir)
            except Exception:
                rel = f
            print(f"‚Üí –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {rel}")

    if not plan:
        print("–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π ‚Äî –Ω–∏—á–µ–≥–æ –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ.")
        return 0

    # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π OCR backend —Å —É—á—ë—Ç–æ–º —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–≥–æ
    ocr_backend_eff = choose_ocr_backend(args.ocr_backend)
    if ocr_backend_eff == "none":
        args.ocr_mode = "never"

    easyocr_dir = Path(os.getenv("EASYOCR_DIR", str(Path.home() / ".EasyOCR"))).expanduser()
    ensure_dir(easyocr_dir / "model")

    # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω easyocr –∏ –º–æ–¥–µ–ª–µ–π –µ—â—ë –Ω–µ—Ç ‚Äì –¥–µ–ª–∞–µ–º ¬´—Ç—ë–ø–ª—ã–π —Å—Ç–∞—Ä—Ç¬ª (–æ–¥–Ω–æ–ø—Ä–æ—Ü–µ—Å—Å–Ω–æ)
    need_easy_warmup = (
        args.ocr_mode != "never"
        and ocr_backend_eff == "easyocr"
        and not _easyocr_models_ready(easyocr_dir)
    )
    if need_easy_warmup:
        print("‚è≥ EasyOCR warmup: –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π (–æ–¥–∏–Ω —Ä–∞–∑)...")
        use_gpu = False
        try:
            import torch
            use_gpu = torch.cuda.is_available()
        except Exception:
            pass
        # –†–∞–∑—Ä–µ—à–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –†–û–í–ù–û –æ–¥–∏–Ω —Ä–∞–∑
        easyocr.Reader(['ru','en'], gpu=use_gpu,
                       model_storage_directory=str(easyocr_dir),
                       download_enabled=True,
                       verbose=False)
        # –ø–µ—Ä–≤—ã–π –ø—Ä–æ–≥–æ–Ω ‚Äî –±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ (–∏—Å–∫–ª—é—á–∞–µ–º –≥–æ–Ω–∫–∏)
        args.workers = 1

    # –ü–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º –ø–æ —Ñ–∞–π–ª–∞–º
    workers = args.workers or max(1, os.cpu_count() or 1)

    # –ï—Å–ª–∏ EasyOCR ‚Äî –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å, —á—Ç–æ–±—ã –¥–∞—Ç—å CUDA —Ä–∞–±–æ—Ç–∞—Ç—å
    easyocr_use_gpu = False
    if args.ocr_mode != "never" and ocr_backend_eff == "easyocr":
        if workers > 1:
            print("‚ö†Ô∏è EasyOCR: –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ workers=1 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ GPU.")
            workers = 1
        try:
            import torch
            easyocr_use_gpu = torch.cuda.is_available()
        except Exception:
            easyocr_use_gpu = False
        print(f"EasyOCR init –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è —Å GPU={easyocr_use_gpu}")

    total = len(plan)
    print(f"üì¶ Ingest started: {total} files (workers={workers}, ocr={args.ocr_mode}/{ocr_backend_eff})")

    results: List[Dict[str, Any]] = []
    t_start = perf_counter()

    if workers <= 1 or len(plan) == 1:
        for i, f in enumerate(plan, 1):
            t0 = perf_counter()
            try:
                r = process_one_file(
                    f, out_dir,
                    min_chars=args.min_chars,
                    ocr_mode=args.ocr_mode,
                    ocr_backend_eff=ocr_backend_eff,
                    ocr_lang=args.ocr_lang,
                    dpi=args.dpi,
                    verbose=args.verbose,
                    page_size_chars=args.page_size_chars,
                    easyocr_dir=easyocr_dir,
                    easyocr_use_gpu=easyocr_use_gpu,
                )
                results.append(r)
                dt = perf_counter() - t0
                print(f"[{i}/{total}] {f.name}: {r['pages']} pages, empty={r['empty_pages']}, ocr={r['ocr_backend']}/{r['ocr_mode']} ({dt:.2f}s)")
            except Exception as e:
                print(f"[ERR] {f.name}: {e}", file=sys.stderr)
    else:
        mp_ctx = mp.get_context("spawn")
        with ProcessPoolExecutor(max_workers=workers, mp_context=mp_ctx) as ex:
            futs = {
                ex.submit(
                    process_one_file, f, out_dir,
                    min_chars=args.min_chars,
                    ocr_mode=args.ocr_mode,
                    ocr_backend_eff=ocr_backend_eff,
                    ocr_lang=args.ocr_lang,
                    dpi=args.dpi,
                    verbose=args.verbose,
                    page_size_chars=args.page_size_chars,
                    easyocr_dir=easyocr_dir,
                    easyocr_use_gpu=False,  # –≤ –º—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–µ ‚Äî –±–µ–∑ CUDA
                ): f for f in plan
            }
            done = 0
            for fut in as_completed(futs):
                f = futs[fut]
                try:
                    r = fut.result()
                    results.append(r)
                    done += 1
                    print(f"[{done}/{total}] {f.name}: {r['pages']} pages, empty={r['empty_pages']}, ocr={r['ocr_backend']}/{r['ocr_mode']}")
                except Exception as e:
                    print(f"[ERR] {f.name}: {e}", file=sys.stderr)

    # –û–±–Ω–æ–≤–ª—è–µ–º manifest –∏ —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º doc_id –ø—Ä–∏ –∫–æ–ª–ª–∏–∑–∏–∏
    for r in results:
        src = r["source_path"]
        doc_id = r["doc_id"]

        if doc_id in existing_ids:
            base = doc_id
            suf = 2
            while f"{base}_{suf}" in existing_ids:
                suf += 1
            new_id = f"{base}_{suf}"
            # –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º —Ñ–∞–π–ª jsonl
            op = Path(r["out_pages"])
            op_renamed = op.with_name(f"{new_id}.pages.jsonl")
            try:
                op.rename(op_renamed)
            except FileNotFoundError:
                pass
            r["doc_id"] = new_id
            r["out_pages"] = str(op_renamed)
            doc_id = new_id
        existing_ids.add(doc_id)

        prev = by_source.get(src)
        entry = {
            "doc_id": doc_id,
            "source_path": src,
            "pages": r["pages"],
            "lang": "ru",
            "sha1": r["sha1"],
            "ocr": (args.ocr_mode != "never" and ocr_backend_eff != "none"),
            "ocr_mode": r["ocr_mode"],
            "ocr_backend": r["ocr_backend"],
            "ocr_lang": r["ocr_lang"],
            "dpi": r["dpi"],
            "min_chars": r["min_chars"],
            "empty_pages": r["empty_pages"],
        }
        if prev:
            prev.update(entry)
        else:
            manifest["docs"].append(entry)

    manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding="utf-8")
    total_dt = perf_counter() - t_start
    print(f"\n–ì–æ—Ç–æ–≤–æ ‚úÖ: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤ = {len(results)} –∑–∞ {total_dt:.2f}s. –û–±–Ω–æ–≤–ª—ë–Ω {manifest_path}")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
