#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAW (PDF/DOCX/TXT) -> data/*.pages.jsonl + data/manifest.json

–û–±–Ω–æ–≤–ª–µ–Ω–∏—è:
- –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –Ω–µ ¬´—Å–Ω–æ—Å–∏—Ç¬ª –≤–µ—Å—å –¥–æ–∫—É–º–µ–Ω—Ç, –µ—Å–ª–∏ –æ–Ω–∞ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∞—Å—å –≤ –Ω–∞—á–∞–ª–µ.
- –î–µ—Ç–µ–∫—Ç–æ—Ä reference-—Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ —Å—Ç—Ä–æ–∫–∞–º-—Ü–∏—Ç–∞—Ç–∞–º) + –º—è–≥–∫–æ–µ —É–¥–∞–ª–µ–Ω–∏–µ ¬´—Ö–≤–æ—Å—Ç–∞¬ª.
- –ù–∞ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö –≤—ã—Ä–µ–∑–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏-—Ü–∏—Ç–∞—Ç—ã.
- –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ OCR-¬´–∫–∞—à–µ¬ª –∏ —á–∞—Å—Ç—ã–º —à–∞–±–ª–æ–Ω–∞–º —Å—Å—ã–ª–æ–∫.

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ app):
  pip install chardet pymupdf pillow python-docx
  # –¥–ª—è OCR (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):
  apt-get install -y tesseract-ocr tesseract-ocr-eng tesseract-ocr-rus
  pip install easyocr opencv-python-headless
"""

from __future__ import annotations
import argparse
import json
import sys
import os
import re
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Tuple
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing as mp
from time import perf_counter

import chardet
import numpy as np

# ---------- –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã ----------
try:
    import fitz  # PyMuPDF
except Exception as e:
    print("[ERR] –¢—Ä–µ–±—É–µ—Ç—Å—è PyMuPDF: pip install pymupdf", file=sys.stderr, flush=True)
    raise

try:
    from docx import Document  # python-docx==1.1.2
except Exception:
    Document = None

try:
    import pytesseract
    from PIL import Image
    TESS_AVAILABLE = True
except Exception:
    from PIL import Image
    TESS_AVAILABLE = False

try:
    import easyocr
    EASY_AVAILABLE = True
except Exception:
    EASY_AVAILABLE = False

try:
    import cv2
    CV_AVAILABLE = True
except Exception:
    CV_AVAILABLE = False

try:
    import torch
    _TORCH_OK = True
except Exception:
    torch = None
    _TORCH_OK = False


# ================== –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –ø–æ—Ä–æ–≥–∏ ==================

def _env_int(name: str, default: int) -> int:
    try:
        return int(os.getenv(name, str(default)))
    except Exception:
        return default

def _env_truthy(name: str, default: bool) -> bool:
    v = os.getenv(name, None)
    if v is None:
        return default
    return v.strip().lower() in ("1","true","yes","on")

# –ª–∏–º–∏—Ç—ã –≤—ã–≤–æ–¥–∏–º—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ (–¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –æ–±—Ä–µ–∑–∫–∏ –±–æ–ª—å—à–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü)
DEFAULT_PAGE_SIZE_CHARS = _env_int("PAGE_SIZE_CHARS", 1800)

# --- –¥–µ—Ç–µ–∫—Ü–∏—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã ---
REFS_MIN_LINES_PAGE   = _env_int("REFS_MIN_LINES_PAGE", 5)      # –º–∏–Ω. —Å—Ç—Ä–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, —á—Ç–æ–±—ã –ø—Ä–∏–∑–Ω–∞—Ç—å –µ—ë –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–π
REFS_RATIO_PAGE       = float(os.getenv("REFS_RATIO_PAGE", "0.55"))  # –¥–æ–ª—è —Å—Ç—Ä–æ–∫-—Ü–∏—Ç–∞—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
REFS_CONSEC_MIN       = _env_int("REFS_CONSEC_MIN", 2)          # –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö —Ä–µ—Ñ–µ—Ä–µ–Ω—Å-—Å—Ç—Ä–∞–Ω–∏—Ü, —á—Ç–æ–±—ã –æ–±—Ä–µ–∑–∞—Ç—å —Ö–≤–æ—Å—Ç
REFS_TAIL_FRACTION    = float(os.getenv("REFS_TAIL_FRACTION", "0.6")) # ¬´–±–ª–∏–∂–µ –∫ –∫–æ–Ω—Ü—É¬ª ‚Äî –ø–æ—Å–ª–µ —ç—Ç–æ–π –¥–æ–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü
REFS_MIN_PAGES_FROM_START = _env_int("REFS_MIN_PAGES_FROM_START", 3)  # –Ω–µ –æ–±—Ä–µ–∑–∞—Ç—å –≤—Å—ë, –µ—Å–ª–∏ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã –Ω–∞—á–∞–ª–∏—Å—å —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ
REFS_MIN_TOTAL_CHARS  = _env_int("REFS_MIN_TOTAL_CHARS", 5000)  # –æ–±—â–µ–µ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–µ –∫–æ–ª-–≤–æ —Å–∏–º–≤–æ–ª–æ–≤ –¥–æ —Å—Ä–µ–∑–∞ —Ö–≤–æ—Å—Ç–∞

# ================== –£—Ç–∏–ª–∏—Ç—ã ==================

def file_sha1(p: Path) -> str:
    h = hashlib.sha1()
    with p.open("rb") as f:
        for chunk in iter(lambda: f.read(1 << 20), b""):
            h.update(chunk)
    return h.hexdigest()

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def detect_text_file(path: Path) -> str:
    data = path.read_bytes()
    enc = chardet.detect(data).get("encoding") or "utf-8"
    try:
        return data.decode(enc, errors="ignore")
    except Exception:
        return data.decode("utf-8", errors="ignore")

def pixmap_to_pil(pix: "fitz.Pixmap") -> "Image.Image":
    try:
        needs_colorspace_convert = getattr(pix.colorspace, "n", 3) != 3
    except Exception:
        needs_colorspace_convert = False
    if pix.alpha or needs_colorspace_convert:
        pix = fitz.Pixmap(fitz.csRGB, pix)
    img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
    return img

# --- –º—è–≥–∫–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏–º–≤–æ–ª–æ–≤ (—á–∞—Å—Ç—ã–µ OCR-–ø–æ–¥–º–µ–Ω—ã –ª–∞—Ç–∏–Ω–∏—Ü–∞<->–∫–∏—Ä–∏–ª–ª–∏—Ü–∞) ---
_LATIN_TO_CYR = str.maketrans({
    "A":"–ê","a":"–∞","B":"–í","E":"–ï","e":"–µ","K":"–ö","k":"–∫","M":"–ú","H":"–ù","O":"–û","o":"–æ",
    "P":"–†","p":"—Ä","C":"–°","c":"—Å","T":"–¢","X":"–•","x":"—Ö","Y":"–£","y":"—É"
})

def clean_text(text: str) -> str:
    t = (text or "").replace("\r", "")
    t = re.sub(r"-\n", "", t)
    t = t.replace("\n\n", "<<<PARA>>>").replace("\n", " ").replace("<<<PARA>>>", "\n\n")
    t = re.sub(r"[ \t]+", " ", t)
    t = t.translate(_LATIN_TO_CYR)
    return t.strip()

# ================== –î–µ—Ç–µ–∫—Ü–∏—è ¬´–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã¬ª ==================

# –∑–∞–≥–æ–ª–æ–≤–∫–∏ ¬´–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã / References¬ª (–≤ —Ç.—á. —à—É–º–Ω—ã–µ)
REFS_HDR_RE = re.compile(
    r'^\s*(?:'
    r'—Å–ø–∏—Å[–æo]–∫\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä[–∞—ã]|–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä[–∞—ã]|–∏—Å—Ç–æ—á–Ω–∏–∫–∏|'
    r'–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω[–∞-—è—ë]+\s+–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä[–∞—ã]|'
    r'references?|bibliograph\w*'
    r')\s*[:\-‚Äì‚Äî]?\s*$',
    re.IGNORECASE | re.MULTILINE
)

# —Å—Ç—Ä–æ–∫–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫—É—é —Å—Å—ã–ª–∫—É?
CITATION_LINE_RE = re.compile(
    r'''(?xi)
    (?:^\s*\d{1,3}[\).\]]\s+)                       # –Ω—É–º–µ—Ä–∞—Ü–∏—è –ø—É–Ω–∫—Ç–∞
    | (?:\b(et\s*al\.?|–µt\s*–∞l\.?)\b)               # et al.
    | (?:\bdoi[:\s/]|10\.\d{3,9}/\S+)               # doi
    | (?:\bEpub\b|\bPublished\b|\bRetrieved\b)      # –ø–æ–º–µ—Ç–∫–∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
    | (?:\b\d{4}\b\s*;\s*\d{1,4}\s*(?:\(\d{1,4}\))?\s*:\s*\d{1,5}(?:[-‚Äì]\d{1,5})?) # 2019;54(6):1157-1170
    ''',
    re.UNICODE
)

def is_citation_line(line: str) -> bool:
    s = (line or "").strip()
    if not s:
        return False
    # –∫–æ—Ä–æ—Ç–∫–∏–µ –æ–≥—Ä—ã–∑–∫–∏ ‚Äî –Ω–µ —Å—á–∏—Ç–∞–µ–º
    if len(s) < 20:
        return False
    return bool(CITATION_LINE_RE.search(s))

def split_lines_keep(text: str) -> List[str]:
    # –º—è–≥–∫–æ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ ¬´—Å—Ç—Ä–æ–∫–∏¬ª –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å—Å—ã–ª–æ–∫
    return [x.strip() for x in re.split(r'(?:\n+|(?<=\.)\s+)', text or "") if x and x.strip()]

def classify_references_page(text: str) -> Tuple[bool, float, int]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (is_refs_page, ratio, n_lines)"""
    lines = split_lines_keep(text)
    if len(lines) < REFS_MIN_LINES_PAGE:
        return (False, 0.0, len(lines))
    hits = sum(1 for ln in lines if is_citation_line(ln))
    ratio = hits / max(len(lines), 1)
    return (ratio >= REFS_RATIO_PAGE, ratio, len(lines))

def drop_citation_lines(text: str) -> str:
    """–£–¥–∞–ª—è–µ—Ç —Å—Ç—Ä–æ–∫–∏, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ –±–∏–±–ª–∏–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∑–∞–ø–∏—Å–∏, –æ—Å—Ç–∞–≤–ª—è–µ—Ç –æ—Å—Ç–∞–ª—å–Ω–æ–π —Ç–µ–∫—Å—Ç."""
    lines = split_lines_keep(text)
    kept = [ln for ln in lines if not is_citation_line(ln)]
    out = "\n".join(kept).strip()
    return out

def decide_tail_cut(page_flags: List[bool], total_pages: int, total_chars_before: List[int]) -> int:
    """
    –ï—Å–ª–∏ –±–ª–∏–∂–µ –∫ –∫–æ–Ω—Ü—É –∏–¥—ë—Ç –±–ª–æ–∫ –∏–∑ ‚â•REFS_CONSEC_MIN —Ä–µ—Ñ–µ—Ä–µ–Ω—Å-—Å—Ç—Ä–∞–Ω–∏—Ü, –≤–µ—Ä–Ω—ë–º
    –∏–Ω–¥–µ–∫—Å –ø–µ—Ä–≤–æ–π —Ç–∞–∫–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è ¬´—Ö–≤–æ—Å—Ç–æ–≤–æ–≥–æ¬ª –æ–±—Ä–µ–∑–∞–Ω–∏—è. –ò–Ω–∞—á–µ -1.
    total_chars_before[i] ‚Äî –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –î–û —Å—Ç—Ä–∞–Ω–∏—Ü—ã i (0-based).
    """
    if total_pages == 0:
        return -1
    start_tail = int(total_pages * REFS_TAIL_FRACTION)
    consec = 0
    first_idx = -1
    for i in range(total_pages):
        if not page_flags[i]:
            consec = 0
            first_idx = -1
            continue
        # —Å—á–∏—Ç–∞–µ–º —Ç–æ–ª—å–∫–æ ¬´–±–ª–∏–∂–µ –∫ –∫–æ–Ω—Ü—É¬ª
        if i < start_tail:
            continue
        consec = consec + 1 if consec > 0 else 1
        if consec == 1:
            first_idx = i
        if consec >= REFS_CONSEC_MIN and total_chars_before[i] >= REFS_MIN_TOTAL_CHARS:
            return max(first_idx, 0)
    return -1

# ================== DOCX/TXT ==================

def extract_docx_text(path: Path) -> str:
    if Document is None:
        print("[ERR] python-docx –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –î–æ–±–∞–≤—å `python-docx` –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.", file=sys.stderr, flush=True)
        return ""
    try:
        doc = Document(str(path))
    except Exception as e:
        print(f"[ERR] –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å DOCX {path.name}: {e}", file=sys.stderr, flush=True)
        return ""
    parts: List[str] = []
    for p in doc.paragraphs:
        t = (p.text or "").strip()
        if t:
            parts.append(t)
    for table in doc.tables:
        for row in table.rows:
            cells = [ (c.text or "").strip() for c in row.cells ]
            line = " | ".join([c for c in cells if c])
            if line:
                parts.append(line)
    return "\n".join(parts).strip()

def split_text_to_pages(full_text: str, page_size_chars: int = DEFAULT_PAGE_SIZE_CHARS) -> List[Dict[str, Any]]:
    text = clean_text(full_text)
    if not text:
        return [{"page": 1, "text": ""}]

    # —Ä–∞–∑—Ä–µ–∂–µ–º –ø—Ä–∏–º–µ—Ä–Ω–æ –ø–æ –∞–±–∑–∞—Ü–∞–º/–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞
    parts: List[str] = []
    buf = []
    cur_len = 0
    tokens = re.split(r"(\n\n|[.!?]\s+)", text)
    for t in tokens:
        if t is None:
            continue
        if cur_len + len(t) > page_size_chars and buf:
            parts.append("".join(buf).strip())
            buf, cur_len = [t], len(t)
        else:
            buf.append(t)
            cur_len += len(t)
    if buf:
        parts.append("".join(buf).strip())

    pages = []
    for i, chunk in enumerate(parts, start=1):
        pages.append({"page": i, "text": chunk})
    return pages or [{"page": 1, "text": text[:page_size_chars]}]

# ================== OCR Backends ==================

def ocr_page_tesseract(img_pil: "Image.Image", lang: str) -> str:
    if not TESS_AVAILABLE:
        return ""
    os.environ.setdefault("OMP_NUM_THREADS", "1")
    os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
    cfg = "--oem 1 --psm 6"
    return (pytesseract.image_to_string(img_pil, lang=lang, config=cfg) or "").strip()

def ocr_page_easyocr(img_pil: "Image.Image", reader) -> str:
    if reader is None:
        return ""
    arr = np.array(img_pil.convert("RGB"))
    res = reader.readtext(arr, detail=0, paragraph=True)
    return "\n".join([x.strip() for x in res if x]).strip()

def preprocess_pil(img_pil: "Image.Image") -> "Image.Image":
    if not CV_AVAILABLE:
        return img_pil
    img = np.array(img_pil.convert("L"))
    try:
        clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
        img = clahe.apply(img)
    except Exception:
        pass
    img = cv2.fastNlMeansDenoising(img, h=10)
    img = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                cv2.THRESH_BINARY, 35, 15)
    return Image.fromarray(img)

# ================== –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ PDF —Å –∞–Ω—Ç–∏-–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π ==================

def ingest_pdf(
    pdf_path: Path,
    *,
    ocr_mode: str,            # "auto"|"always"|"never"
    ocr_backend: str,         # "tesseract"|"easyocr"
    ocr_lang: str,
    dpi: int,
    min_chars: int,
    verbose: bool,
    easy_reader=None
) -> List[Dict[str, Any]]:

    try:
        doc = fitz.open(pdf_path)
    except Exception as e:
        print(f"[WARN] –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å PDF {pdf_path.name}: {e}", file=sys.stderr, flush=True)
        return []

    raw_pages: List[str] = []
    for i, page in enumerate(doc, start=1):
        txt = (page.get_text("text") or "").strip()

        # —Ä–µ—à–∞–µ–º, –¥–µ–ª–∞—Ç—å –ª–∏ OCR
        if ocr_mode == "always":
            do_ocr = True
        elif ocr_mode == "auto":
            do_ocr = (len(txt) < min_chars)
        else:
            do_ocr = False

        if do_ocr:
            if ocr_backend == "tesseract":
                zoom = dpi / 72.0
                mat = fitz.Matrix(zoom, zoom)
                pix = page.get_pixmap(matrix=mat, alpha=False)
                img_pil = pixmap_to_pil(pix)
                img_pil = preprocess_pil(img_pil)
                txt_ocr = ocr_page_tesseract(img_pil, ocr_lang)
            elif ocr_backend == "easyocr":
                pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0), alpha=False)
                img_pil = pixmap_to_pil(pix)
                img_pil = preprocess_pil(img_pil)
                txt_ocr = ocr_page_easyocr(img_pil, easy_reader)
            else:
                txt_ocr = ""
            if len(txt_ocr) > len(txt):
                txt = txt_ocr
                if verbose:
                    print(f"[OCR-{ocr_backend}] {pdf_path.name} p.{i}: len={len(txt)}", flush=True)

        raw_pages.append(txt)

    # --- –≥–ª–æ–±–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã ---
    total_pages = len(raw_pages)
    cleaned_pages: List[str] = []
    refs_flags: List[bool] = []
    total_chars_before: List[int] = []
    acc = 0

    # –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —á–∏—Å—Ç–∏–º, —Å–º–æ—Ç—Ä–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ –¥–æ–ª—é —Å—Å—ã–ª–æ–∫
    header_hits_idx: List[int] = []
    for idx, t in enumerate(raw_pages):
        t0 = t or ""
        # —Å–Ω–∞—á–∞–ª–∞ –ª–µ–≥–∫–∞—è —á–∏—Å—Ç–∫–∞
        t1 = clean_text(t0)
        # –∑–∞–≥–æ–ª–æ–≤–æ–∫ ¬´References¬ª/¬´–°–ø–∏—Å–æ–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã¬ª?
        if REFS_HDR_RE.search(t1):
            header_hits_idx.append(idx)
        # —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ‚Äî –ø–æ—á—Ç–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–∑ —Å—Å—ã–ª–æ–∫?
        is_ref, ratio, n_lines = classify_references_page(t1)
        refs_flags.append(is_ref)
        cleaned_pages.append(t1)
        total_chars_before.append(acc)
        acc += len(t1)

    # —Ä–µ—à–∞–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –æ—Ç—Ä–µ–∑–∞—Ç—å —Ö–≤–æ—Å—Ç
    cut_from = decide_tail_cut(refs_flags, total_pages, total_chars_before)

    pages_out: List[Dict[str, Any]] = []
    for i, text in enumerate(cleaned_pages, start=1):
        if cut_from >= 0 and (i-1) >= cut_from:
            # ¬´—Ö–≤–æ—Å—Ç¬ª –ø–æ—Å–ª–µ –±–ª–æ–∫–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã –±–ª–∏–∂–µ –∫ –∫–æ–Ω—Ü—É ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            continue

        # –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞ –∏ –æ–Ω–∞ —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ (–≤ –Ω–∞—á–∞–ª–µ) ‚Äî –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å—Ç–∏–º —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É,
        # –Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ —Ç—Ä–æ–≥–∞–µ–º —Ü–µ–ª–∏–∫–æ–º
        early_refs = (i <= REFS_MIN_PAGES_FROM_START) and refs_flags[i-1]
        if early_refs:
            continue

        # –µ—Å–ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ ¬´–≤ –æ—Å–Ω–æ–≤–Ω–æ–º –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞¬ª, –Ω–æ –Ω–µ –ø–æ–ø–∞–ª–∞ –ø–æ–¥ —Ä–∞–Ω–Ω–∏–µ –∏–ª–∏ —Ö–≤–æ—Å—Ç–æ–≤—ã–µ –ø—Ä–∞–≤–∏–ª–∞ ‚Äî —É–¥–∞–ª–∏–º —Å—Ç—Ä–æ–∫–∏-—Å—Å—ã–ª–∫–∏
        if refs_flags[i-1]:
            text = drop_citation_lines(text)

        # –∏ –µ—â—ë —Ä–∞–∑ –ª–æ–∫–∞–ª—å–Ω–æ: –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ —Å–ª—É—á–∞–π–Ω—ã–µ —Å—Å—ã–ª–∫–∏ ‚Äî —É–±–µ—Ä—ë–º –∏—Ö
        text = drop_citation_lines(text)

        pages_out.append({"page": i, "text": text})

    return pages_out

# ================== TXT/DOCX —Å –∞–Ω—Ç–∏-–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π ==================

def ingest_txt(txt_path: Path, page_size_chars: int = DEFAULT_PAGE_SIZE_CHARS) -> List[Dict[str, Any]]:
    text = detect_text_file(txt_path).strip()
    text = clean_text(text)
    # –ø–æ—Ä–µ–∂–µ–º –Ω–∞ –ø—Å–µ–≤–¥–æ—Å—Ç—Ä–∞–Ω–∏—Ü—ã
    pages = split_text_to_pages(text, page_size_chars=page_size_chars)

    # –ø—Ä–∏–º–µ–Ω–∏–º —Ç–∞–∫—É—é –∂–µ –ª–æ–≥–∏–∫—É, –∫–∞–∫ –¥–ª—è PDF
    flags = []
    total_chars_before = []
    acc = 0
    for p in pages:
        t = p.get("text","")
        is_ref, _, _ = classify_references_page(t)
        flags.append(is_ref)
        total_chars_before.append(acc)
        acc += len(t)

    cut_from = decide_tail_cut(flags, len(pages), total_chars_before)

    out: List[Dict[str, Any]] = []
    for i, p in enumerate(pages, start=1):
        if cut_from >= 0 and (i-1) >= cut_from:
            continue
        t = p.get("text","")
        early_refs = (i <= REFS_MIN_PAGES_FROM_START) and flags[i-1]
        if early_refs:
            continue
        if flags[i-1]:
            t = drop_citation_lines(t)
        t = drop_citation_lines(t)
        out.append({"page": i, "text": t})
    return out

def ingest_docx(path: Path, page_size_chars: int = DEFAULT_PAGE_SIZE_CHARS) -> List[Dict[str, Any]]:
    raw = extract_docx_text(path)
    if not raw:
        return [{"page": 1, "text": ""}]
    raw = clean_text(raw)
    return ingest_txt_like(raw, page_size_chars=page_size_chars)

def ingest_txt_like(full_text: str, page_size_chars: int = DEFAULT_PAGE_SIZE_CHARS) -> List[Dict[str, Any]]:
    pages = split_text_to_pages(full_text, page_size_chars=page_size_chars)
    flags = []
    total_chars_before = []
    acc = 0
    for p in pages:
        t = p.get("text","")
        is_ref, _, _ = classify_references_page(t)
        flags.append(is_ref)
        total_chars_before.append(acc)
        acc += len(t)
    cut_from = decide_tail_cut(flags, len(pages), total_chars_before)

    out: List[Dict[str, Any]] = []
    for i, p in enumerate(pages, start=1):
        if cut_from >= 0 and (i-1) >= cut_from:
            continue
        t = p.get("text","")
        early_refs = (i <= REFS_MIN_PAGES_FROM_START) and flags[i-1]
        if early_refs:
            continue
        if flags[i-1]:
            t = drop_citation_lines(t)
        t = drop_citation_lines(t)
        out.append({"page": i, "text": t})
    return out or [{"page": 1, "text": ""}]

# ================== –û—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å ==================

def choose_ocr_backend(requested: str) -> str:
    req = (requested or "").lower()
    if req == "easyocr" and EASY_AVAILABLE:
        return "easyocr"
    if req == "tesseract" and TESS_AVAILABLE:
        return "tesseract"
    if EASY_AVAILABLE:
        return "easyocr"
    if TESS_AVAILABLE:
        return "tesseract"
    return "none"

def _easyocr_models_ready(model_dir: Path) -> bool:
    try:
        mdir = model_dir / "model"
        return mdir.exists() and any(mdir.iterdir())
    except Exception:
        return False

def process_one_file(
    f: Path,
    out_dir: Path,
    *,
    min_chars: int,
    ocr_mode: str,
    ocr_backend_eff: str,
    ocr_lang: str,
    dpi: int,
    verbose: bool,
    page_size_chars: int,
    easyocr_dir: Path,
    easyocr_use_gpu: bool,
    easyocr_allow_downloads: bool,
) -> Dict[str, Any]:
    """–ü—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ (–±–µ–∑ –¥–æ—Å—Ç—É–ø–∞ –∫ manifest). –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç entry + –ø—É—Ç—å pages."""
    sha = file_sha1(f)
    stem = f.stem
    doc_id = stem
    out_pages = out_dir / f"{doc_id}.pages.jsonl"

    easy_reader = None
    if ocr_mode != "never" and ocr_backend_eff == "easyocr":
        easy_reader = easyocr.Reader(
            ['ru', 'en'],
            gpu=easyocr_use_gpu,
            model_storage_directory=str(easyocr_dir),
            download_enabled=bool(easyocr_allow_downloads),
            verbose=False,
        )

    ext = f.suffix.lower()
    if ext == ".pdf":
        pages = ingest_pdf(
            f, ocr_mode=ocr_mode, ocr_backend=ocr_backend_eff, ocr_lang=ocr_lang,
            dpi=dpi, min_chars=min_chars, verbose=verbose, easy_reader=easy_reader
        )
    elif ext == ".docx":
        pages = ingest_docx(f, page_size_chars=page_size_chars)
    else:
        pages = ingest_txt(f, page_size_chars=page_size_chars)

    with out_pages.open("w", encoding="utf-8") as w:
        for p in pages:
            rec = {"doc_id": doc_id, "page": p.get("page", 1), "text": p.get("text", "")}
            w.write(json.dumps(rec, ensure_ascii=False) + "\n")

    empty_pages = sum(1 for p in pages if not (p.get("text") or "").strip())

    return {
        "doc_id": doc_id,
        "source_path": str(f),
        "pages": len(pages),
        "lang": "ru",
        "sha1": sha,
        "ocr_backend": ocr_backend_eff,
        "ocr_mode": ocr_mode,
        "ocr_lang": ocr_lang,
        "dpi": dpi,
        "min_chars": min_chars,
        "empty_pages": empty_pages,
        "out_pages": str(out_pages)
    }

def _decide_easyocr_gpu(ocr_gpu_arg: str) -> bool:
    ocr_gpu_arg = (ocr_gpu_arg or "auto").lower()
    if ocr_gpu_arg == "cuda":
        return _TORCH_OK and torch.cuda.is_available()
    if ocr_gpu_arg == "cpu":
        return False
    return _TORCH_OK and torch.cuda.is_available()

def main():
    ap = argparse.ArgumentParser("RAW -> data/*.pages.jsonl (+manifest) —Å OCR –∏ –∞–Ω—Ç–∏-–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–æ–π")
    ap.add_argument("--input-dir", default="raw_docs", help="–ü–∞–ø–∫–∞ —Å PDF/DOCX/TXT (—Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ)")
    ap.add_argument("--out-dir", default="data", help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å JSONL –∏ manifest.json")
    ap.add_argument("--force", action="store_true", help="–ü–µ—Ä–µ–ø–∞—Ä—Å–∏—Ç—å –¥–∞–∂–µ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π")

    # OCR
    ap.add_argument("--ocr-mode", choices=["auto","always","never"], default=os.getenv("OCR_MODE", "auto"))
    ap.add_argument("--ocr-backend", choices=["tesseract","easyocr"], default=os.getenv("OCR_BACKEND", "easyocr"))
    ap.add_argument("--ocr-lang", default=os.getenv("TESS_LANG", "rus+eng"))
    ap.add_argument("--min-chars", type=int, default=int(os.getenv("MIN_CHARS", "60")))
    ap.add_argument("--dpi", type=int, default=int(os.getenv("OCR_DPI", "300")))

    # EasyOCR/GPU
    ap.add_argument("--ocr-gpu", choices=["auto","cpu","cuda"], default=os.getenv("OCR_GPU", "auto"))
    ap.add_argument("--easyocr-dir", default=os.getenv("EASYOCR_DIR", str(Path.home() / ".EasyOCR")))
    ap.add_argument("--easyocr-allow-downloads", action="store_true",
                    default=os.getenv("EASYOCR_ALLOW_DOWNLOADS", "0").lower() in ("1","true","yes"))

    # –ø—Ä–æ—á–µ–µ
    ap.add_argument("--workers", type=int, default=0)
    ap.add_argument("--page-size-chars", type=int, default=DEFAULT_PAGE_SIZE_CHARS)
    ap.add_argument("--verbose", action="store_true")

    args = ap.parse_args()

    in_dir = Path(args.input_dir)
    out_dir = Path(args.out_dir)
    ensure_dir(out_dir)

    allowed = {".pdf", ".docx", ".txt"}
    files: List[Path] = []
    for p in in_dir.rglob("*"):
        if not p.is_file():
            continue
        ext = p.suffix.lower()
        if ext not in allowed:
            continue
        name = p.name
        if name.startswith("~$"):
            continue
        files.append(p)
    files = sorted(files)

    if not files:
        print(f"–í {in_dir} –Ω–µ—Ç pdf/docx/txt", file=sys.stderr, flush=True)
        return 1

    manifest_path = out_dir / "manifest.json"
    manifest: Dict[str, Any] = {"docs": []}
    if manifest_path.exists():
        try:
            manifest = json.loads(manifest_path.read_text(encoding="utf-8"))
            if not isinstance(manifest, dict) or "docs" not in manifest:
                manifest = {"docs": []}
        except Exception:
            manifest = {"docs": []}

    docs = manifest.get("docs", [])
    by_source: Dict[str, Dict[str, Any]] = {d.get("source_path"): d for d in docs if isinstance(d, dict)}
    existing_ids = {d.get("doc_id") for d in docs if isinstance(d, dict)}

    plan: List[Path] = []
    for f in files:
        sha = file_sha1(f)
        prev = by_source.get(str(f))
        if args.force or not prev or prev.get("sha1") != sha:
            plan.append(f)
        elif args.verbose:
            try:
                rel = f.relative_to(in_dir)
            except Exception:
                rel = f
            print(f"‚Üí –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: {rel}", flush=True)

    if not plan:
        print("–ù–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π ‚Äî –Ω–∏—á–µ–≥–æ –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ.", flush=True)
        return 0

    ocr_backend_eff = choose_ocr_backend(args.ocr_backend)
    if ocr_backend_eff == "none":
        args.ocr_mode = "never"

    easyocr_dir = Path(args.easyocr_dir).expanduser()
    ensure_dir(easyocr_dir / "model")

    need_easy_warmup = (
        args.ocr_mode != "never"
        and ocr_backend_eff == "easyocr"
        and not _easyocr_models_ready(easyocr_dir)
    )
    if need_easy_warmup:
        print("‚è≥ EasyOCR warmup: –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π (–æ–¥–∏–Ω —Ä–∞–∑)...", flush=True)
        use_gpu = _decide_easyocr_gpu(args.ocr_gpu)
        easyocr.Reader(['ru','en'], gpu=use_gpu,
                       model_storage_directory=str(easyocr_dir),
                       download_enabled=bool(args.easyocr_allow_downloads),
                       verbose=False)
        args.workers = 1

    workers = args.workers or max(1, os.cpu_count() or 1)
    easyocr_use_gpu = False
    if args.ocr_mode != "never" and ocr_backend_eff == "easyocr":
        if workers > 1:
            print("‚ö†Ô∏è EasyOCR: –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ workers=1 –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ GPU.", flush=True)
            workers = 1
        easyocr_use_gpu = _decide_easyocr_gpu(args.ocr_gpu)
        print(f"EasyOCR init –ø–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è —Å GPU={easyocr_use_gpu}", flush=True)

    total = len(plan)
    print(f"üì¶ Ingest started: {total} files (workers={workers}, ocr={args.ocr_mode}/{ocr_backend_eff})", flush=True)

    results: List[Dict[str, Any]] = []
    t_start = perf_counter()

    if workers <= 1 or len(plan) == 1:
        for i, f in enumerate(plan, 1):
            t0 = perf_counter()
            try:
                r = process_one_file(
                    f, out_dir,
                    min_chars=args.min_chars,
                    ocr_mode=args.ocr_mode,
                    ocr_backend_eff=ocr_backend_eff,
                    ocr_lang=args.ocr_lang,
                    dpi=args.dpi,
                    verbose=args.verbose,
                    page_size_chars=args.page_size_chars,
                    easyocr_dir=easyocr_dir,
                    easyocr_use_gpu=easyocr_use_gpu,
                    easyocr_allow_downloads=False,
                )
                results.append(r)
                dt = perf_counter() - t0
                print(f"[{i}/{total}] {f.name}: {r['pages']} pages, empty={r['empty_pages']}, ocr={r['ocr_backend']}/{r['ocr_mode']} ({dt:.2f}s)", flush=True)
            except Exception as e:
                print(f"[ERR] {f.name}: {e}", file=sys.stderr, flush=True)
    else:
        mp_ctx = mp.get_context("spawn")
        with ProcessPoolExecutor(max_workers=workers, mp_context=mp_ctx) as ex:
            futs = {
                ex.submit(
                    process_one_file, f, out_dir,
                    min_chars=args.min_chars,
                    ocr_mode=args.ocr_mode,
                    ocr_backend_eff=ocr_backend_eff,
                    ocr_lang=args.ocr_lang,
                    dpi=args.dpi,
                    verbose=args.verbose,
                    page_size_chars=args.page_size_chars,
                    easyocr_dir=easyocr_dir,
                    easyocr_use_gpu=False,
                    easyocr_allow_downloads=False,
                ): f for f in plan
            }
            done = 0
            for fut in as_completed(futs):
                f = futs[fut]
                try:
                    r = fut.result()
                    results.append(r)
                    done += 1
                    print(f"[{done}/{total}] {f.name}: {r['pages']} pages, empty={r['empty_pages']}, ocr={r['ocr_backend']}/{r['ocr_mode']}", flush=True)
                except Exception as e:
                    print(f"[ERR] {f.name}: {e}", file=sys.stderr, flush=True)

    # –æ–±–Ω–æ–≤–ª—è–µ–º manifest –∏ —É–Ω–∏–∫–∞–ª–∏–∑–∏—Ä—É–µ–º doc_id –ø—Ä–∏ –∫–æ–ª–ª–∏–∑–∏–∏
    for r in results:
        src = r["source_path"]
        doc_id = r["doc_id"]

        if doc_id in existing_ids:
            base = doc_id
            suf = 2
            while f"{base}_{suf}" in existing_ids:
                suf += 1
            new_id = f"{base}_{suf}"
            op = Path(r["out_pages"])
            op_renamed = op.with_name(f"{new_id}.pages.jsonl")
            try:
                op.rename(op_renamed)
            except FileNotFoundError:
                pass
            r["doc_id"] = new_id
            r["out_pages"] = str(op_renamed)
            doc_id = new_id
        existing_ids.add(doc_id)

        prev = by_source.get(src)
        entry = {
            "doc_id": doc_id,
            "source_path": src,
            "pages": r["pages"],
            "lang": "ru",
            "sha1": r["sha1"],
            "ocr": (r["ocr_mode"] != "never" and r["ocr_backend"] != "none"),
            "ocr_mode": r["ocr_mode"],
            "ocr_backend": r["ocr_backend"],
            "ocr_lang": r["ocr_lang"],
            "dpi": r["dpi"],
            "min_chars": r["min_chars"],
            "empty_pages": r["empty_pages"],
        }
        if prev:
            prev.update(entry)
        else:
            manifest["docs"].append(entry)

    manifest_path.write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding="utf-8")
    total_dt = perf_counter() - t_start
    print(f"\n–ì–æ—Ç–æ–≤–æ ‚úÖ: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤ = {len(results)} –∑–∞ {total_dt:.2f}s. –û–±–Ω–æ–≤–ª—ë–Ω {manifest_path}", flush=True)
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
