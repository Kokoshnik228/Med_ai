#!/usr/bin/env python3
from __future__ import annotations
import json
import re
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml, requests
from pyserini.search.lucene import LuceneSearcher

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel, Field

# ----- app -----
app = FastAPI(title="med_ai RAG API", version="0.8")
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

# ----- config -----
ROOT = Path(__file__).resolve().parent
CONF_DIR = ROOT / "config"
DEFAULT_YAML = CONF_DIR / "default.yaml"
LOCAL_YAML   = CONF_DIR / "local.yaml"

def build_context_citations(ctx_items, max_out: int = 5):
    return [f"{it['doc_id']} —Å—Ç—Ä.{it['page_start']}-{it['page_end']}" for it in ctx_items[:max_out]]
def rrf_fusion(rank_lists: List[List[str]], k: int = 60) -> Dict[str, float]:
    """Reciprocal Rank Fusion –ø–æ —Å–ø–∏—Å–∫–∞–º doc_id."""
    scores: Dict[str, float] = {}
    for ranks in rank_lists:
        for r, did in enumerate(ranks, start=1):
            scores[did] = scores.get(did, 0.0) + 1.0 / (k + r)
    return scores

def bm25_search(index_dir: str, query: str, topk: int) -> List[Dict[str, Any]]:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫—É—Å–∫–∏ BM25: {doc_id, page, text}."""
    searcher = LuceneSearcher(index_dir)
    hits = searcher.search(query, k=topk)
    out: List[Dict[str, Any]] = []
    for h in hits:
        doc = searcher.doc(h.docid)
        contents = doc.contents() or ""
        ext_id = doc.id()  # "{doc_id}_p{page}_c{child}"
        m = re.match(r"(.+?)_p(\d+)_c(\d+)$", ext_id)
        doc_id = m.group(1) if m else ext_id
        page = int(m.group(2)) if m else 1
        out.append({"doc_id": doc_id, "page": page, "text": contents})
    return out



def looks_meaningless(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–µ–∫—Å—Ç."""
    text = text.strip().lower()
    if len(text) < 20:
        return True
    # –µ—Å–ª–∏ —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã –∏ –º–∞–ª–æ —Ä–∞–∑–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–∞–∞–∞–∞–∞–∞" –∏–ª–∏ "qwerty")
    if re.fullmatch(r"[a-z–∞-—è—ë\s]+", text) and len(set(text)) < 5:
        return True
    return False


def load_config() -> Dict[str, Any]:
    def load(p: Path) -> Dict[str, Any]:
        try:
            data = yaml.safe_load(p.read_text(encoding="utf-8")) if p.exists() else {}
            return data if isinstance(data, dict) else {}
        except Exception:
            return {}
    DEFAULTS = {
        "app": {"data_dir": "data", "bm25_index_dir": "index/bm25_idx"},
        "qdrant": {"url": "http://localhost:7777", "collection": "med_kb_v3"},
        "ollama": {"base_url": "http://localhost:11434", "model": "llama3.1:8b"},
        "retrieval": {"k": 8},
        "embedding": {"model": "BAAI/bge-m3"},
        "prompt": {
    "system": (
        "–¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï. "
        "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∫–∞–∫ –≤—Ä–∞—á–µ–±–Ω—ã–π –∫–µ–π—Å: –≤ –Ω—ë–º –º–æ–≥—É—Ç –±—ã—Ç—å –∂–∞–ª–æ–±—ã, –∞–Ω–∞–º–Ω–µ–∑, –æ—Å–º–æ—Ç—Ä, –¥–∏–∞–≥–Ω–æ–∑ –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è. "
        "–†–∞—Å–ø–æ–∑–Ω–∞–π –¥–∏–∞–≥–Ω–æ–∑ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –º–µ—Ä—ã, —Å–æ–ø–æ—Å—Ç–∞–≤—å –∏—Ö —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π."
        "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∑–∞ –≤—Ä–∞—á–µ–º —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–æ, —Ç—ã –¥–æ–ª–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω—è—Ç—å –µ–≥–æ —Ä–µ—á—å"
        "–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û –í–ê–õ–ò–î–ù–´–ô JSON —Å–æ —Å—Ö–µ–º–æ–π:\n"
        "{score, subscores, critical_errors[], recommendations[], citations[], disclaimer}\n"
        "- score: —á–∏—Å–ª–æ 0..100 (—Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è).\n"
        "- subscores: –∫–∞—Ä—Ç–∞ –ø–æ–¥–æ—Ü–µ–Ω–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä: dosing, diagnosis_match, interactions‚Ä¶).\n"
        "- critical_errors: —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ {type, explain}.\n"
        "- recommendations: —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ {what_to_change, rationale}.\n"
        "- citations: —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ç–æ–ª—å–∫–æ –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –ö–û–ù–¢–ï–ö–°–¢–ê.\n"
        "- disclaimer: –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.\n"
        "–ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–µ—Ç ‚Äî —Å–Ω–∏–∂–∞–π score, –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–µ –≤ disclaimer. –í–ù–ï–®–ù–ò–ï –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π."
    ),
    "user_template": (
        "[–ö–ï–ô–°]\n{case_text}\n\n"
        "[–ö–û–ù–¢–ï–ö–°–¢]\n{ctx}\n\n"
        "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω –≤–∞–ª–∏–¥–Ω—ã–π JSON –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ. –í—Å–µ —Ç–µ–∫—Å—Ç—ã –≤–Ω—É—Ç—Ä–∏ ‚Äî –Ω–∞ —Ä—É—Å—Å–∫–æ–º. "
        "–ò—Å—Ç–æ—á–Ω–∏–∫–∏ —É–∫–∞–∑—ã–≤–∞–π —Ç–æ–ª—å–∫–æ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
    )
}

    }
    base  = load(DEFAULT_YAML)
    local = load(LOCAL_YAML)
    def merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
        out = dict(a)
        for k, v in (b or {}).items():
            if isinstance(v, dict) and isinstance(out.get(k), dict):
                out[k] = merge(out[k], v)
            else:
                out[k] = v
        return out
    return merge(DEFAULTS, merge(base, local))

CONFIG = load_config()
def cfg(*path: str, default: Any=None) -> Any:
    cur: Any = CONFIG
    for p in path:
        if not isinstance(cur, dict) or p not in cur:
            return default
        cur = cur[p]
    return cur

# ----- utils -----
def l2_unit(vec: List[float]) -> List[float]:
    s = (sum(x*x for x in vec) ** 0.5) or 1.0
    return [x/s for x in vec]

def load_pages_text(pages_dir: Path, doc_id: str, p_start: int, p_end: int) -> str:
    jf = pages_dir / f"{doc_id}.pages.jsonl"
    if not jf.exists(): return ""
    out: List[str] = []
    for line in jf.read_text(encoding="utf-8", errors="ignore").splitlines():
        try:
            rec = json.loads(line); pg = int(rec.get("page", 0))
            if p_start <= pg <= p_end:
                out.append(rec.get("text","") or "")
        except Exception:
            continue
    return "\n".join(out)
def embed_query_ollama(text: str) -> List[float]:
    """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ Ollama –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å."""
    base = cfg("ollama","base_url", default="http://localhost:11434") or "http://localhost:11434"
    url = base.rstrip("/") + "/api/embeddings"
    payload = {
        "model": cfg("embedding","model", default="zylonai/multilingual-e5-large"),
        "prompt": text,
    }
    r = requests.post(url, json=payload, timeout=60)
    r.raise_for_status()
    vec = r.json().get("embedding", [])
    return l2_unit(vec)


def retrieve_light(query: str, k: int) -> List[Dict[str,Any]]:
    results: List[Dict[str,Any]] = []
    
    if k <= 0: return results
    try:
        from qdrant_client import QdrantClient
        from FlagEmbedding import BGEM3FlagModel
        model = BGEM3FlagModel(cfg("embedding","model", default="BAAI/bge-m3"), use_fp16=True)
        vec = model.encode([query], return_dense=True, return_sparse=False)["dense_vecs"][0]
        vec = l2_unit(vec)
        client = QdrantClient(url=cfg("qdrant","url", default="http://localhost:7777"))
        pts = client.query_points(collection_name=cfg("qdrant","collection", default="med_kb_v3"),
                                  query=vec, limit=max(k,20)).points
        pages_dir = Path(cfg("app","data_dir", default="data"))
        seen=set()
        for p in pts:
            pl = p.payload or {}
            did = pl.get("doc_id") or "unknown"
            if did in seen: continue
            seen.add(did)
            a = int(pl.get("page_start",1) or 1)
            b = int(pl.get("page_end",a) or a)
            text = load_pages_text(pages_dir, did, a, b) or load_pages_text(pages_dir, did, 1, 1)
            if not (pages_dir / f"{did}.pages.jsonl").exists():
               continue
            results.append({"doc_id": did, "page_start": a, "page_end": b, "text": (text or "")[:2000]})
            if len(results)>=k: break
    except Exception:
        pass
    return results

def retrieve_hybrid(query: str, k: int) -> List[Dict[str, Any]]:
    """RRF —Å–ª–∏—è–Ω–∏–µ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ Qdrant –∏ BM25. –ù–µ –±–æ–ª–µ–µ 2 –∫—É—Å–æ—á–∫–æ–≤ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç."""
    results: List[Dict[str, Any]] = []
    if k <= 0:
        return results

    # --- Qdrant –∫–∞–Ω–¥–∏–¥–∞—Ç—ã (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ doc_id –∏ –∏—Ö –ø–µ—Ä–≤—ã–π payload) ---
    qd_docids: List[str] = []
    qd_payload_first: Dict[str, Dict[str, Any]] = {}
    try:
        from qdrant_client import QdrantClient
        from FlagEmbedding import BGEM3FlagModel
        model = BGEM3FlagModel(cfg("embedding","model", default="BAAI/bge-m3"), use_fp16=True)
        vec = model.encode([query], return_dense=True, return_sparse=False)["dense_vecs"][0]
        vec = l2_unit(vec)
        client = QdrantClient(url=cfg("qdrant","url", default="http://localhost:7777"))
        pts = client.query_points(
            collection_name=cfg("qdrant","collection", default="med_kb_v3"),
            query=vec, limit=max(k*6, 50)
        ).points
        for p in pts:
            pl = p.payload or {}
            did = pl.get("doc_id") or "unknown"
            if did not in qd_payload_first:
                qd_payload_first[did] = pl
                qd_docids.append(did)
    except Exception:
        pass

    # --- BM25 –∫–∞–Ω–¥–∏–¥–∞—Ç—ã (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ doc_id –∏ –∏—Ö –ø–µ—Ä–≤—ã–π hit) ---
    bm25_dir = cfg("app","bm25_index_dir", default="index/bm25_idx")
    bm_docids: List[str] = []
    bm_first: Dict[str, Dict[str, Any]] = {}
    try:
        bm_hits = bm25_search(bm25_dir, query, topk=max(k*6, 50))
        for h in bm_hits:
            did = h["doc_id"]
            if did not in bm_first:
                bm_first[did] = h
                bm_docids.append(did)
    except Exception:
        pass

    # --- RRF —Å–ª–∏—è–Ω–∏–µ —Å–ø–∏—Å–∫–æ–≤ doc_id ---
    fused = rrf_fusion([qd_docids, bm_docids])
    top_docids = [did for did,_ in sorted(fused.items(), key=lambda x: x[1], reverse=True)]

    # --- –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥ (–¥–æ k —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤, –º–∞–∫—Å–∏–º—É–º 2 –Ω–∞ doc_id) ---
    pages_dir = Path(cfg("app","data_dir", default="data"))
    per_doc_limit = 2
    used_per_doc: Dict[str, int] = {}

    for did in top_docids:
        if len(results) >= k:
            break

        # 1) –°–Ω–∞—á–∞–ª–∞ ‚Äî BM25 —Ñ—Ä–∞–≥–º–µ–Ω—Ç (—Ç–æ—á–Ω–µ–µ –ø–æ —Ç–µ—Ä–º–∏–Ω–∞–º)
        if did in bm_first and used_per_doc.get(did, 0) < per_doc_limit:
            h = bm_first[did]
            results.append({
                "doc_id": did,
                "page_start": h["page"],
                "page_end": h["page"],
                "text": (h["text"] or "")[:2000]
            })
            used_per_doc[did] = used_per_doc.get(did, 0) + 1
            if len(results) >= k:
                break

        # 2) –ó–∞—Ç–µ–º ‚Äî Qdrant —Ñ—Ä–∞–≥–º–µ–Ω—Ç (–ø–æ embedding-–æ–∫–Ω—É —Å—Ç—Ä–∞–Ω–∏—Ü)
        if did in qd_payload_first and used_per_doc.get(did, 0) < per_doc_limit:
            pl = qd_payload_first[did]
            a = int(pl.get("page_start", 1) or 1)
            b = int(pl.get("page_end", a) or a)
            text = load_pages_text(pages_dir, did, a, b) or load_pages_text(pages_dir, did, 1, 1)
            results.append({
                "doc_id": did,
                "page_start": a,
                "page_end": b,
                "text": (text or "")[:2000]
            })
            used_per_doc[did] = used_per_doc.get(did, 0) + 1

    return results


def safe_json_extract(s: str) -> Dict[str,Any]:
    try:
        return json.loads(s)
    except Exception:
        pass
    i=s.find("{"); j=s.rfind("}")
    if 0<=i<j:
        try:
            return json.loads(s[i:j+1])
        except Exception:
            pass
    return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [], "disclaimer": "–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –Ω–µ —É–¥–∞–ª—Å—è."}

def normalize_result(r: Dict[str, Any]) -> Dict[str, Any]:
    out: Dict[str, Any] = {
        "score": 0, "subscores": {}, "critical_errors": [],
        "recommendations": [], "citations": [], "disclaimer": ""
    }
    # score
    try:
        sc = r.get("score", 0)
        out["score"] = max(0, min(100, float(sc))) if isinstance(sc, (int,float)) else 0.0
    except Exception:
        out["score"] = 0.0
    # subscores
    subs = r.get("subscores") or {}
    if isinstance(subs, dict):
        clean = {}
        for k,v in subs.items():
            try: clean[str(k)] = max(0, min(100, float(v)))
            except Exception: pass
        out["subscores"] = clean
    # critical_errors
    ce = r.get("critical_errors") or []
    clean_ce = []
    if isinstance(ce, list):
        for it in ce:
            if isinstance(it, dict):
                clean_ce.append({"type": str(it.get("type","general")),
                                 "explain": str(it.get("explain", it.get("message","")))})
            elif isinstance(it, str):
                clean_ce.append({"type":"general","explain":it})
    out["critical_errors"] = clean_ce
    # recommendations
    recs = r.get("recommendations") or []
    clean_recs = []
    if isinstance(recs, list):
        for it in recs:
            if isinstance(it, dict):
                w = str(it.get("what_to_change") or it.get("action") or it.get("recommendation") or it.get("text",""))
                ra = str(it.get("rationale") or it.get("reason",""))
                if w or ra: clean_recs.append({"what_to_change": w, "rationale": ra})
            elif isinstance(it, str):
                clean_recs.append({"what_to_change": it, "rationale": ""})
    out["recommendations"] = clean_recs
    # citations
    cits = r.get("citations") or []
    if isinstance(cits, list):
        out["citations"] = [str(x) for x in cits if isinstance(x,(str,int,float))]
    elif isinstance(cits,(str,int,float)):
        out["citations"] = [str(cits)]
    # disclaimer
    disc = r.get("disclaimer")
    out["disclaimer"] = str(disc) if disc is not None else ""
    return out
    
def extract_medical_query(case_text: str) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –≤—Ä–∞—á–µ–±–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –∫—Ä–∞—Ç–∫–∏–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å.
    –ù–∞–ø—Ä–∏–º–µ—Ä:
    - –∏–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ –∞–Ω–∞–º–Ω–µ–∑–∞ –¥–æ—Å—Ç–∞—ë—Ç –¥–∏–∞–≥–Ω–æ–∑, –∂–∞–ª–æ–±—ã, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è.
    """
    case_text = case_text.lower()

    # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö –∑–æ–Ω
    complaints = re.findall(r"–∂–∞–ª–æ–±[–∞–∏]\s*([^.\n]*)", case_text)
    diagnosis = re.findall(r"–¥–∏–∞–≥–Ω–æ–∑[:\s]*([a-z–∞-—è0-9\s\.\-,]+)", case_text)
    treatment = re.findall(r"–Ω–∞–∑–Ω–∞—á–µ–Ω[–æ—ã]?\s*([^.\n]*)", case_text)

    query_parts = []
    if complaints: query_parts.append(complaints[0])
    if diagnosis: query_parts.append(diagnosis[0])
    if treatment: query_parts.append(treatment[0])

    # fallback ‚Äî –∏—â–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    if not query_parts:
        key_terms = re.findall(r"(–≥–µ–º–æ—Ä—Ä–æ–π|—Ç—Ä–µ—â–∏–Ω[–∞—ã]|–ø—Ä–æ—Å—Ç–∞—Ç–∏—Ç|–¥–∏–∞—Ä–µ—è|–∑–∞–ø–æ—Ä|–≥–∏–ø–µ—Ä—Ç–µ–Ω–∑–∏—è|–±–æ–ª—å|–∫—Ä–æ–≤–æ—Ç–µ—á–µ–Ω\w+)", case_text)
        query_parts.extend(key_terms)

    query = " ".join(query_parts).strip()
    return query or case_text[:200]


def call_ollama_json(ollama_url: Optional[str], model: str, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
    try:
        if not ollama_url:
            ollama_url = cfg("ollama","base_url", default="http://localhost:11434") or "http://localhost:11434"
        payload = {
            "model": str(model),
            "prompt": str(user_prompt),
            "system": str(system_prompt),
            "format": "json",
            "options": {},   # –±–µ–∑ –∫–∞—Å—Ç–æ–º–Ω—ã—Ö options
            "stream": False,
        }
        resp = requests.post(f"{ollama_url.rstrip('/')}/api/generate", json=payload, timeout=180)
        try:
            resp.raise_for_status()
        except requests.HTTPError:
            return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [],
                    "citations": [], "disclaimer": f"LLM HTTP {resp.status_code}: {resp.text[:200]}"}
        if resp.headers.get("content-type","").startswith("application/json"):
            obj = resp.json()
            response_field = obj.get("response","") if isinstance(obj, dict) else ""
        else:
            response_field = resp.text
        raw = json.dumps(response_field, ensure_ascii=False) if isinstance(response_field,(dict,list)) else str(response_field or "").strip()
        if not raw:
            return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [],
                    "citations": [], "disclaimer": "LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç."}
        return safe_json_extract(raw)
    except Exception as e:
        return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [],
                "citations": [], "disclaimer": f"–û—à–∏–±–∫–∞ LLM ({type(e).__name__}): {e}"}

# ----- routes -----
class AnalyzeReq(BaseModel):
    case_text: str
    query: Optional[str] = None
    k: int = Field(default_factory=lambda: cfg("retrieval","k", default=8))
    model: str = Field(default_factory=lambda: cfg("ollama","model", default="llama3.1:8b"))
    ollama_url: Optional[str] = Field(default_factory=lambda: cfg("ollama","base_url", default="http://localhost:11434"))

@app.get("/health")
def health():
    return {"status":"ok", "qdrant": cfg("qdrant","url", default="http://localhost:7777"),
            "model": cfg("ollama","model", default="llama3.1:8b")}

@app.post("/config/reload")
def config_reload():
    global CONFIG
    CONFIG = load_config()
    return {"status":"reloaded"}

@app.post("/analyze")
def analyze_ep(req: AnalyzeReq):
    try:
        print("üöÄ /analyze –≤—ã–∑–≤–∞–Ω")
        print(f"üìã –¢–µ–∫—Å—Ç –∫–µ–π—Å–∞: {req.case_text[:150]}...")

        # --- 1Ô∏è‚É£ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ---
        if looks_meaningless(req.case_text):
            print("‚ö†Ô∏è –¢–µ–∫—Å—Ç –±–µ—Å—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π ‚Äî –≤–æ–∑–≤—Ä–∞—Ç –±–µ–∑ –∞–Ω–∞–ª–∏–∑–∞")
            return {
                "result": {
                    "score": 0,
                    "subscores": {},
                    "critical_errors": [],
                    "recommendations": [],
                    "citations": [],
                    "disclaimer": "–¢–µ–∫—Å—Ç –∫–µ–π—Å–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
                }
            }

        # --- 2Ô∏è‚É£ –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã ---
        if req.query:
            q = req.query
        else:
            q = extract_medical_query(req.case_text)
        print(f"üîç –ò–∑–≤–ª–µ—á—ë–Ω –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {q}")

        if not q:
            q = (req.case_text[:200] if req.case_text else "–≥–∏–ø–µ—Ä—Ç–µ–Ω–∑–∏—è –ª–µ—á–µ–Ω–∏–µ")

        print("üì° –ó–∞–ø—Ä–æ—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (retrieve_hybrid)...")
        ctx_items = retrieve_hybrid(q, req.k)
        print(f"‚úÖ –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω: {len(ctx_items)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")

        if not ctx_items:
            print("‚ùå –ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –≤—ã—Ö–æ–¥–∏–º.")
            return {
                "result": {
                    "score": 0,
                    "subscores": {},
                    "critical_errors": [],
                    "recommendations": [],
                    "citations": [],
                    "disclaimer": "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π ‚Äî –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –∫–µ–π—Å.",
                }
            }

        # --- 3Ô∏è‚É£ –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç ---
        ctx = "".join(
            [
                f"### [{i}] DOC {it['doc_id']} P{it['page_start']}-{it['page_end']}\n{it.get('text','')}\n\n"
                for i, it in enumerate(ctx_items, 1)
            ]
        ) or "(–∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω)"
        print("üß± –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω, –¥–ª–∏–Ω–∞:", len(ctx))

        system = cfg("prompt", "system", default="–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–µ–π—Å. –í–µ—Ä–Ω–∏ JSON.")
        user_t = cfg("prompt", "user_template", default="[–ö–ï–ô–°]\n{case_text}\n\n[–ö–û–ù–¢–ï–ö–°–¢]\n{ctx}")
        user = user_t.format(case_text=req.case_text, ctx=ctx)

        # --- 4Ô∏è‚É£ –í—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏ ---
        print("üß† –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ Ollama...")
        resp = call_ollama_json(req.ollama_url, req.model, system, user)
        print("ü§ñ –ú–æ–¥–µ–ª—å –æ—Ç–≤–µ—Ç–∏–ª–∞:", str(resp)[:300])

        data = normalize_result(resp)
        print("üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω:", data.get("score"))

        # --- 5Ô∏è‚É£ –ü–æ–Ω–∏–∂–∞–µ–º –æ—Ü–µ–Ω–∫—É, –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π ---
        ctx_len = sum(len(it.get("text", "")) for it in ctx_items)
        print("üìè –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:", ctx_len)

        if ctx_len < 500:
            data["score"] = max(0, data.get("score", 0) * 0.5)
            data["disclaimer"] += " (–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ‚Äî –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —Å–Ω–∏–∂–µ–Ω–∞.)"
        elif ctx_len < 1500:
            data["score"] = max(0, data.get("score", 0) * 0.8)
            data["disclaimer"] += " (–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω ‚Äî –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —á–∞—Å—Ç–∏—á–Ω–æ —Å–Ω–∏–∂–µ–Ω–∞.)"

        # --- 6Ô∏è‚É£ –ò—Å—Ç–æ—á–Ω–∏–∫–∏ ---
        data["citations"] = build_context_citations(ctx_items, max_out=5)
        if not data.get("citations"):
            data["citations"] = [
                f"{it['doc_id']} —Å—Ç—Ä.{it['page_start']}-{it['page_end']}"
                for it in ctx_items[:5]
            ]

        # --- 7Ô∏è‚É£ –ù–∞–∫–∞–∑–∞–Ω–∏–µ –∑–∞ –æ—à–∏–±–∫–∏ ---
        crit_count = len(data.get("critical_errors", []))
        if crit_count > 0:
            data["score"] = max(0, data["score"] - 10 * crit_count)
            data["disclaimer"] += f" (–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {crit_count} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫.)"

        print("‚úÖ –ì–æ—Ç–æ–≤–æ. –ò—Ç–æ–≥–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞:", data["score"])
        return {"result": data, "citations_used": [x["doc_id"] for x in ctx_items]}

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ analyze_ep: {e}")
        return {
            "result": {
                "score": None,
                "subscores": {},
                "critical_errors": [],
                "recommendations": [],
                "citations": [],
                "disclaimer": f"–û—à–∏–±–∫–∞ API: {e}",
            }
        }


# ----- simple UI at "/" -----
UI_HTML = """<!doctype html><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤—Ä–∞—á–∞ (MVP)</title>
<style>
body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;background:#f6f7fb;margin:0;color:#101828}
.wrap{max-width:1100px;margin:20px auto;padding:16px}
.card{background:#fff;border:1px solid #e5e7eb;border-radius:16px;box-shadow:0 1px 3px rgba(16,24,40,.08);padding:16px;margin-bottom:16px}
h1{font-size:20px;margin:0 0 8px}
label{font-weight:600;font-size:14px;margin:6px 0;display:block}
input,select,textarea{width:100%;border:1px solid #d0d5dd;border-radius:10px;padding:10px;font-size:14px}
textarea{min-height:180px}
.row{display:grid;grid-template-columns:1fr 1fr;gap:12px}
.btn{background:#2563eb;color:#fff;border:none;border-radius:10px;padding:10px 14px;font-weight:600;cursor:pointer}
.btn:disabled{opacity:.6;cursor:not-allowed}
.badge{display:inline-block;border:1px solid #d0d5dd;border-radius:999px;padding:2px 8px;font-size:12px;margin-left:8px}
.mono{font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px}
.small{font-size:12px;color:#475467}
.err{color:#b91c1c}
.grid2{display:grid;grid-template-columns:1fr 1fr;gap:8px}
</style>
<div class="wrap">
  <div class="card">
    <h1>AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤—Ä–∞—á–∞ (MVP) <span id="score" class="badge">–æ—Ü–µ–Ω–∫–∞: ‚Äî</span></h1>
    <div class="small">API: <span id="api"></span></div>
  </div>
  <div class="card">
    <label>–¢–µ–∫—Å—Ç –∫–µ–π—Å–∞</label>
    <textarea id="caseText" placeholder="–í—Å—Ç–∞–≤—å—Ç–µ –∫–µ–π—Å: –∂–∞–ª–æ–±—ã, –∞–Ω–∞–º–Ω–µ–∑, –¥–∏–∞–≥–Ω–æ–∑, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è..."></textarea>
    <div class="row">
      <div>
        <label>–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)</label>
        <input id="query" placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä: –≥–∏–ø–µ—Ä—Ç–æ–Ω–∏—á–µ—Å–∫–∞—è –±–æ–ª–µ–∑–Ω—å –ª–µ—á–µ–Ω–∏–µ —ç–Ω–∞–ª–∞–ø—Ä–∏–ª">
      </div>
      <div>
        <label>–ú–æ–¥–µ–ª—å / K</label>
        <div class="row" style="grid-template-columns:2fr 1fr;gap:8px">
          <select id="model"><option>llama3.1:8b</option><option>llama3.1:70b</option></select>
          <input id="k" type="number" value="8" min="0" max="20">
        </div>
      </div>
    </div>
    <div style="margin-top:10px;display:flex;gap:8px;align-items:center">
      <button id="run" class="btn">–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å</button>
      <button id="reindex" class="btn" style="background:#059669">üîÑ –û–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É</button>
      <span id="busy" class="small" style="display:none">‚è≥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è‚Ä¶</span>
      <span id="error" class="small err"></span>
    </div>
  </div>
  <div class="card">
    <h3 style="margin:0 0 6px">–†–µ–∑—É–ª—å—Ç–∞—Ç</h3>
    <div class="grid2" id="subs"></div>
    <div><h4>–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏</h4><ul id="crit"></ul></div>
    <div><h4>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h4><ul id="recs"></ul></div>
    <div><h4>–ò—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ü–∏—Ç–∞—Ç—ã)</h4><ul id="cits"></ul></div>
    <details><summary class="small">–°—ã—Ä–æ–π JSON</summary><pre id="raw" class="mono"></pre></details>
  </div>
</div>
<script>
const API = window.location.origin; document.getElementById('api').textContent = API;
const el=id=>document.getElementById(id); const show=(n,on)=>n.style.display=on?'':'none';
function colorForScore(s){ if(typeof s!=='number') return ''; if(s>=85) return '#dcfce7'; if(s>=65) return '#fef9c3'; return '#fee2e2'; }
function renderResult(r){
  const sc=r.score??'‚Äî'; const sb=document.getElementById('score'); sb.textContent='–æ—Ü–µ–Ω–∫–∞: '+sc; sb.style.background=colorForScore(sc);
  const subs=el('subs'); subs.innerHTML=''; Object.entries(r.subscores||{}).forEach(([k,v])=>{ const d=document.createElement('div'); d.className='card'; d.style.margin=0; d.innerHTML=`<div class="small">${labelMap[k] || k}</div><div style="font-weight:700">${v??'‚Äî'}</div>`; subs.appendChild(d); });
  const crit=el('crit'); crit.innerHTML=''; (r.critical_errors||[]).forEach(x=>{ const li=document.createElement('li'); li.textContent=`${x.type}: ${x.explain}`; crit.appendChild(li); });
  const recs=el('recs'); recs.innerHTML=''; (r.recommendations||[]).forEach(x=>{ const li=document.createElement('li'); li.textContent=`${x.what_to_change} ‚Äî ${x.rationale}`; recs.appendChild(li); });
  const cits=el('cits'); cits.innerHTML=''; (r.citations||[]).forEach(x=>{ const li=document.createElement('li'); li.textContent=String(x); cits.appendChild(li); });
  el('raw').textContent=JSON.stringify(r,null,2);
}
  const labelMap = {
    "diagnosis": "–î–∏–∞–≥–Ω–æ–∑",
    "diagnosis_match": "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–∏–∞–≥–Ω–æ–∑—É",
    "therapy": "–¢–µ—Ä–∞–ø–∏—è",
    "med_choice": "–í—ã–±–æ—Ä –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞",
    "dosage": "–î–æ–∑–∏—Ä–æ–≤–∫–∞",
    "dosing": "–î–æ–∑–∏—Ä–æ–≤–∫–∞",
    "interactions": "–õ–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è",
    "contraindications": "–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è",
    "monitoring": "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥",
    "evidence": "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å"
  };

async function run(){ el('error').textContent=''; show(el('busy'),true); el('run').disabled=true;
  try{
    const body={ case_text: el('caseText').value||'', query: el('query').value||null, k: parseInt(el('k').value||'8',10), model: el('model').value||'llama3.1:8b' };
    const res=await fetch(API+'/analyze',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify(body)});
    const txt=await res.text(); let data; try{ data=JSON.parse(txt); }catch(e){ throw new Error('–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å JSON –æ—Ç–≤–µ—Ç–∞: '+txt.slice(0,200)); }
    renderResult(data.result || data);
  }catch(e){ el('error').textContent='–û—à–∏–±–∫–∞: '+(e?.message||e); }
  finally{ show(el('busy'),false); el('run').disabled=false; }
}
el('run').onclick=run;

el('reindex').onclick = async () => {
  show(el('busy'), true);
  el('error').textContent = '';
  try {
    const res = await fetch(API + '/reindex', { method: 'POST' });
    const data = await res.json();
    el('error').textContent = data.message || '';
  } catch(e) {
    el('error').textContent = '–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: ' + e.message;
  } finally {
    show(el('busy'), false);
  }
};
// üü¢ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –∫–∞–∂–¥—ã–µ 3 —Å–µ–∫
async function checkReindexStatus() {
  try {
    const res = await fetch(API + '/reindex/status');
    const data = await res.json();
    const msg = data.message || '';
    const state = data.state;
    if (state === 'running') {
      el('busy').textContent = 'üîÑ ' + msg;
      show(el('busy'), true);
    } else if (state === 'done') {
      el('busy').textContent = '‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞';
      setTimeout(() => show(el('busy'), false), 4000);
    } else if (state === 'error') {
      el('busy').textContent = '‚ùå ' + msg;
    }
  } catch(e) {
    console.error(e);
  }
}
setInterval(checkReindexStatus, 3000);

</script>
"""
import subprocess, threading
index_status = {"state": "idle", "message": "–û–∂–∏–¥–∞–Ω–∏–µ"}
import subprocess, threading, time

index_status = {"state": "idle", "message": "–û–∂–∏–¥–∞–Ω–∏–µ"}

@app.post("/reindex")
def reindex_ep(full: bool = False):
    """–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ; full=True ‚Äî –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è."""
    def run_reindex():
        global index_status
        try:
            index_status.update({"state": "running", "message": "üìò –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–ø—É—â–µ–Ω–∞..."})
            print("‚öôÔ∏è –ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

            index_status["message"] = "üìÑ –®–∞–≥ 1: –ø–∞—Ä—Å–∏–Ω–≥ PDF –∏ —Å–æ–∑–¥–∞–Ω–∏–µ JSON..."
            subprocess.run([
                "python", "ingest_from_raw.py",
                "--input-dir", "raw_docs",
                "--out-dir", "data"
            ], check=True)

            index_status["message"] = "üìö –®–∞–≥ 2: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞..."
            subprocess.run([
                "python", "build_bm25.py",
                "--pages-glob", "data/*.pages.jsonl",
                "--out-json", "index/bm25_json",
                "--index-dir", "index/bm25_idx"
            ], check=True)

            index_status["message"] = "üß† –®–∞–≥ 3: –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant..."
            cmd = [
                "python", "chunk_and_index.py",
                "--pages-glob", "data/*.pages.jsonl",
                "--collection", "med_kb_v3",
                "--qdrant-url", "http://localhost:7777",
            ]
            if full:
                cmd.append("--recreate")
            else:
                cmd.append("--only-new")  # —Ä–µ–∂–∏–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã—Ö —á–∞–Ω–∫–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏ —Ç–∞–∫ –≤–∫–ª—é—á—ë–Ω)
            subprocess.run(cmd, check=True)

            index_status.update({"state": "done", "message": "‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞."})
            print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
        except Exception as e:
            index_status.update({"state": "error", "message": f"‚ùå –û—à–∏–±–∫–∞: {e}"})
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏: {e}")

    threading.Thread(target=run_reindex, daemon=True).start()
    return {"status": "started", "message": ""}




@app.get("/reindex/status")
def reindex_status():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏."""
    return index_status

@app.get("/", response_class=HTMLResponse)
def ui_root():
    return HTMLResponse(UI_HTML)

