#!/usr/bin/env python3
from __future__ import annotations
import json
import os
import re
import socket
import subprocess
import threading
import time
from pathlib import Path
from typing import Any, Dict, List, Optional
from glob import glob
import importlib  # ‚Üê –¥–ª—è –≥–æ—Ä—è—á–µ–π –ø–æ–¥–≥—Ä—É–∑–∫–∏ runtime_settings.py

# RAG utils
from rag.bm25_utils import bm25_search, retrieve_hybrid, embed_query_hf

import requests
import yaml
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel, Field

# ================================
# .env –∑–∞–≥—Ä—É–∑–∫–∞ (dev/prod)
# ================================
try:
    from dotenv import load_dotenv
    _env_mode = (os.getenv("APP_ENV") or "dev").strip().lower()
    _env_file = Path(".env.dev" if _env_mode == "dev" else ".env.prod")
    if _env_file.exists():
        load_dotenv(dotenv_path=_env_file)
        print(f"üîß Loaded env: {_env_file}")
    else:
        print(f"‚ö†Ô∏è Env file not found: {_env_file} (fallback to process env)")
except Exception as _e:
    print(f"‚ö†Ô∏è dotenv load skipped: {_e}")

# –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è env
if os.getenv("EMBEDDING_MODEL") and not os.getenv("HF_MODEL"):
    os.environ["HF_MODEL"] = os.getenv("EMBEDDING_MODEL", "")
if not os.getenv("EMB_BACKEND"):
    os.environ["EMB_BACKEND"] = "hf"

# –ê–≤—Ç–æ–≤—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è HF
if not os.getenv("HF_DEVICE"):
    try:
        import torch
        os.environ["HF_DEVICE"] = "cuda" if torch.cuda.is_available() else "auto"
    except Exception:
        os.environ["HF_DEVICE"] = "auto"

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è HTTP-—Å–µ—Å—Å–∏—è (keep-alive)
_HTTP = requests.Session()
_HTTP.headers.update({"Connection": "keep-alive"})

# ================================
# FastAPI
# ================================
app = FastAPI(title="med_ai RAG API", version="1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"]
)

# ================================
# Config
# ================================
ROOT = Path(__file__).resolve().parent
CONF_DIR = ROOT / "config"
DEFAULT_YAML = CONF_DIR / "default.yaml"
LOCAL_YAML = CONF_DIR / "local.yaml"

def load_runtime_overrides() -> Dict[str, Any]:
    """
    –ü–æ–¥—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç config/runtime_settings.py (dict RUNTIME), –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –±–µ–∑ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏.
    """
    try:
        import config.runtime_settings as rs  # type: ignore
        importlib.reload(rs)
        data = getattr(rs, "RUNTIME", None)
        if isinstance(data, dict):
            print("üîÅ runtime_settings.py loaded")
            return data
    except Exception as e:
        print(f"‚ö†Ô∏è runtime overrides not loaded: {e}")
    return {}

def load_config() -> Dict[str, Any]:
    def _load_yaml(p: Path) -> Dict[str, Any]:
        try:
            data = yaml.safe_load(p.read_text(encoding="utf-8")) if p.exists() else {}
            return data if isinstance(data, dict) else {}
        except Exception:
            return {}

    DEFAULTS = {
        "app": {"data_dir": "data", "bm25_index_dir": "index/bm25_idx"},
        "qdrant": {
            "url": os.getenv("QDRANT_URL", "http://localhost:7779"),
            "collection": os.getenv("QDRANT_COLLECTION", "med_kb_v3"),
        },
        "ollama": {
            "base_url": os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434"),
            "model": os.getenv("MODEL_ID", os.getenv("LLM_MODEL", "llama3.1:8b")),
            # ‚Üì –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ¬´–∫–∞–∫ –¥–æ–ª–≥–æ/–º–Ω–æ–≥–æ –¥—É–º–∞–µ—Ç¬ª
            "max_tokens": int(os.getenv("LLM_MAX_TOKENS", "2048")),
            "timeout_s": int(os.getenv("LLM_TIMEOUT", "180")),
            "temperature": float(os.getenv("LLM_TEMPERATURE", "0.4")),
            "top_p": float(os.getenv("LLM_TOP_P", "0.95")),
            "num_ctx": int(os.getenv("LLM_NUM_CTX", "6144")),
        },
        "retrieval": {"k": 8},
        "embedding": {
            "backend": os.getenv("EMB_BACKEND", "hf"),
            "model": os.getenv("HF_MODEL", os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")),
            "device": os.getenv("HF_DEVICE", "auto"),
            "fp16": False,
        },
        "chunking": {"child_w": 200, "child_overlap": 35, "parent_w": 800},
        "prompt": {
            "system": (
                "–¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï. "
                "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∫–∞–∫ –≤—Ä–∞—á–µ–±–Ω—ã–π –∫–µ–π—Å: –≤ –Ω—ë–º –º–æ–≥—É—Ç –±—ã—Ç—å –∂–∞–ª–æ–±—ã, –∞–Ω–∞–º–Ω–µ–∑, –æ—Å–º–æ—Ç—Ä, –¥–∏–∞–≥–Ω–æ–∑ –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è. "
                "–†–∞—Å–ø–æ–∑–Ω–∞–π –¥–∏–∞–≥–Ω–æ–∑ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –º–µ—Ä—ã, —Å–æ–ø–æ—Å—Ç–∞–≤—å –∏—Ö —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. "
                "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∑–∞ –≤—Ä–∞—á–µ–º —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–æ, —Ç—ã –¥–æ–ª–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω—è—Ç—å –µ–≥–æ —Ä–µ—á—å. "
                "–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û –í–ê–õ–ò–î–ù–´–ô JSON —Å–æ —Å—Ö–µ–º–æ–π:\n"
                "{score, subscores, critical_errors[], recommendations[], citations[], disclaimer}\n"
                "- score: —á–∏—Å–ª–æ 0..100 (—Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è).\n"
                "- subscores: –∫–∞—Ä—Ç–∞ –ø–æ–¥–æ—Ü–µ–Ω–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä: dosing, diagnosis_match, interactions‚Ä¶).\n"
                "- critical_errors: —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ {type, explain}.\n"
                "- recommendations: —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ {what_to_change, rationale}.\n"
                "- citations: —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ç–æ–ª—å–∫–æ –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –ö–û–ù–¢–ï–ö–°–¢–ê.\n"
                "- disclaimer: –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.\n"
                "–ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–µ—Ç ‚Äî —Å–Ω–∏–∂–∞–π score, –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–µ –≤ disclaimer. –í–ù–ï–®–ù–ò–ï –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π."
            ),
            "user_template": (
                "[–ö–ï–ô–°]\n{case_text}\n\n"
                "[–ö–û–ù–¢–ï–ö–°–¢]\n{ctx}\n\n"
                "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω –≤–∞–ª–∏–¥–Ω—ã–π JSON –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ.–ë–µ–∑ Markdown. –í—Å–µ —Ç–µ–∫—Å—Ç—ã –≤–Ω—É—Ç—Ä–∏ ‚Äî –Ω–∞ —Ä—É—Å—Å–∫–æ–º. "
                "–ò—Å—Ç–æ—á–Ω–∏–∫–∏ —É–∫–∞–∑—ã–≤–∞–π —Ç–æ–ª—å–∫–æ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ë–µ–∑ –ø–æ—è—Å–Ω—è—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥."
            ),
        },
    }

    base = _load_yaml(DEFAULT_YAML)
    local = _load_yaml(LOCAL_YAML)
    runtime = load_runtime_overrides()  # ‚Üê —Å–ª–æ–π Python-–Ω–∞—Å—Ç—Ä–æ–µ–∫

    def merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
        out = dict(a)
        for k, v in (b or {}).items():
            if isinstance(v, dict) and isinstance(out.get(k), dict):
                out[k] = merge(out[k], v)
            else:
                out[k] = v
        return out

    # –ü–æ—Ä—è–¥–æ–∫ –≤–∞–∂–µ–Ω: DEFAULTS <- default.yaml <- local.yaml <- runtime_settings.py
    return merge(DEFAULTS, merge(base, merge(local, runtime)))

CONFIG = load_config()

# –ø–æ—Å–ª–µ CONFIG = load_config()

WARMUP_DONE = False

@app.on_event("startup")
def warmup_bm25():
    """–û—Ç–∫—Ä—ã–≤–∞–µ–º Lucene –∏–Ω–¥–µ–∫—Å –æ–¥–∏–Ω —Ä–∞–∑, —á—Ç–æ–±—ã —É—Å–∫–æ—Ä–∏—Ç—å –ø–µ—Ä–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã."""
    global WARMUP_DONE
    if WARMUP_DONE or os.getenv("BM25_WARMUP_DISABLED") == "1":
        return
    try:
        from rag.bm25_utils import bm25_search
        idx = cfg("app", "bm25_index_dir", default="index/bm25_idx")
        bm25_search(idx, "—Ç–µ—Å—Ç", topk=1)  # –ø—Ä–æ–≥—Ä–µ–≤ JVM + –∏–Ω–¥–µ–∫—Å–∞
        print("üî• BM25 warmed up")
        WARMUP_DONE = True
    except Exception as e:
        print(f"‚ö†Ô∏è BM25 warmup skipped: {e}")

def cfg(*path: str, default: Any = None) -> Any:
    cur: Any = CONFIG
    for p in path:
        if not isinstance(cur, dict) or p not in cur:
            return default
        cur = cur[p]
    return cur

def cfg_int(*path, default: int, allow_zero: bool = False) -> int:
    v = cfg(*path, default=None)
    try:
        v = int(v)
        if (not allow_zero and v <= 0) or (allow_zero and v < 0):
            raise ValueError
        return v
    except Exception:
        return int(default)

def cfg_float(*path, default: float) -> float:
    v = cfg(*path, default=None)
    try:
        return float(v)
    except Exception:
        return float(default)

def cfg_str(*path, default: str) -> str:
    v = cfg(*path, default=None)
    return str(v) if (v is not None and str(v).strip() != "") else str(default)

# ================================
# Utils
# ================================
def looks_meaningless(text: str) -> bool:
    t = (text or "").strip().lower()
    if len(t) < 3:
        return True
    if re.fullmatch(r"[a-z]\d{1,2}(\.\d+)?", t):
        return False
    if not re.search(r"[a-z–∞-—è—ë0-9]", t):
        return True
    letters_only = re.fullmatch(r"[a-z–∞-—è—ë\s]+", t)
    if letters_only and len(set(t)) < 5 and len(t) < 20:
        return True
    return False

def load_pages_text(pages_dir: Path, doc_id: str, p_start: int, p_end: int) -> str:
    jf = pages_dir / f"{doc_id}.pages.jsonl"
    if not jf.exists():
        return ""
    out: List[str] = []
    for line in jf.read_text(encoding="utf-8", errors="ignore").splitlines():
        try:
            rec = json.loads(line)
            pg = int(rec.get("page", 0))
            if p_start <= pg <= p_end:
                out.append(rec.get("text", "") or "")
        except Exception:
            continue
    return "\n".join(out)

def build_context_citations(ctx_items, max_out: int = 5):
    return [f"{it['doc_id']} —Å—Ç—Ä.{it['page_start']}-{it['page_end']}" for it in ctx_items[:max_out]]

def build_ctx_string(ctx_items, max_chars: int = 8000, per_text_limit: int = 800) -> str:
    parts, total = [], 0
    for i, it in enumerate(ctx_items, 1):
        txt = (it.get("text", "") or "")[:per_text_limit]
        chunk = f"### [{i}] DOC {it['doc_id']} P{it['page_start']}-{it['page_end']}\n{txt}\n\n"
        if total + len(chunk) > max_chars:
            break
        parts.append(chunk)
        total += len(chunk)
    return "".join(parts)

def _approx_tokens(s: str) -> int:
    # –≥—Ä—É–±–æ: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞
    return max(1, len(s) // 4)

# ================================
# Qdrant client (REST)
# ================================
def _qdrant_client_rest(url_override: Optional[str] = None):
    from qdrant_client import QdrantClient
    url = (url_override or cfg("qdrant", "url", default="http://qdrant:6333"))
    if "qdrant:" in url:
        try:
            socket.gethostbyname("qdrant")
        except socket.gaierror:
            url = "http://localhost:7779"
    return QdrantClient(url=url, timeout=10, prefer_grpc=False, grpc_port=None)

# ================================
# LLM —á–µ—Ä–µ–∑ Ollama
# ================================
def _trim_code_fences(txt: str) -> str:
    txt = re.sub(r"^\s*```(?:json)?\s*", "", txt, flags=re.IGNORECASE)
    txt = re.sub(r"\s*```\s*$", "", txt)
    return txt.strip()

def safe_json_extract(s: str) -> Dict[str, Any]:
    import json as _json, re

    def _default():
        return {
            "score": None, "subscores": {}, "critical_errors": [],
            "recommendations": [], "citations": [],
            "disclaimer": "–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –Ω–µ —É–¥–∞–ª—Å—è."
        }

    if not s:
        return _default()

    s1 = re.sub(r"```(?:json)?", "", s, flags=re.IGNORECASE).replace("```", "").strip()

    try:
        obj = _json.loads(s1)
        if isinstance(obj, str):
            try:
                return _json.loads(obj)
            except Exception:
                pass
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    start, depth, best = None, 0, None
    for i, ch in enumerate(s1):
        if ch == "{":
            if depth == 0:
                start = i
            depth += 1
        elif ch == "}" and depth > 0:
            depth -= 1
            if depth == 0 and start is not None:
                cand = (start, i + 1)
                if not best or (cand[1] - cand[0]) > (best[1] - best[0]):
                    best = cand
    if best:
        chunk = s1[best[0]:best[1]]
        try:
            return _json.loads(chunk)
        except Exception:
            chunk2 = re.sub(r",\s*([}\]])", r"\1", chunk)
            try:
                return _json.loads(chunk2)
            except Exception:
                pass

    try:
        unescaped = s1.encode("utf-8", "ignore").decode("unicode_escape")
        return _json.loads(unescaped)
    except Exception:
        pass

    return _default()

def normalize_result(r: Dict[str, Any]) -> Dict[str, Any]:
    out: Dict[str, Any] = {
        "score": 0, "subscores": {}, "critical_errors": [],
        "recommendations": [], "citations": [], "disclaimer": ""
    }
    try:
        sc = r.get("score", 0) if isinstance(r, dict) else 0
        out["score"] = max(0, min(100, float(sc))) if isinstance(sc, (int, float)) else 0.0
    except Exception:
        out["score"] = 0.0

    subs = r.get("subscores") if isinstance(r, dict) else {}
    if isinstance(subs, dict):
        clean = {}
        for k, v in subs.items():
            try:
                clean[str(k)] = max(0, min(100, float(v)))
            except Exception:
                pass
        out["subscores"] = clean

    ce = r.get("critical_errors") if isinstance(r, dict) else []
    clean_ce = []
    if isinstance(ce, list):
        for it in ce:
            if isinstance(it, dict):
                clean_ce.append({"type": str(it.get("type", "general")),
                                 "explain": str(it.get("explain", it.get("message", "")))})
            elif isinstance(it, str):
                clean_ce.append({"type": "general", "explain": it})
    out["critical_errors"] = clean_ce

    recs = r.get("recommendations") if isinstance(r, dict) else []
    clean_recs = []
    if isinstance(recs, list):
        for it in recs:
            if isinstance(it, dict):
                w = str(it.get("what_to_change") or it.get("action") or it.get("recommendation") or it.get("text", ""))
                ra = str(it.get("rationale") or it.get("reason", ""))
                if w or ra:
                    clean_recs.append({"what_to_change": w, "rationale": ra})
            elif isinstance(it, str):
                clean_recs.append({"what_to_change": it, "rationale": ""})
    out["recommendations"] = clean_recs

    cits = r.get("citations") if isinstance(r, dict) else []
    if isinstance(cits, list):
        out["citations"] = [str(x) for x in cits if isinstance(x, (str, int, float))]
    elif isinstance(cits, (str, int, float)):
        out["citations"] = [str(cits)]

    disc = r.get("disclaimer") if isinstance(r, dict) else ""
    out["disclaimer"] = str(disc) if disc is not None else ""
    return out

def _ns_to_ms(ns: int) -> int:
    try:
        return int(round(float(ns) / 1_000_000.0))
    except Exception:
        return 0

def _ollama_generate_stream(ollama_url, payload, connect_timeout_s=3.0, read_timeout_s=50.0) -> str:
    # —Å—Ç—Ä–∏–º JSON-–æ–±—ä–µ–∫—Ç–æ–≤ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ; —Å–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ–ª–µ "response"
    with _HTTP.post(
        f"{ollama_url.rstrip('/')}/api/generate",
        json={**payload, "stream": True},
        timeout=(float(connect_timeout_s), float(read_timeout_s)),
        stream=True
    ) as r:
        r.raise_for_status()
        buf = []
        for ln in r.iter_lines(decode_unicode=True):
            if not ln:
                continue
            try:
                chunk = json.loads(ln)
                if "response" in chunk:
                    buf.append(chunk["response"])
            except Exception:
                continue
        return "".join(buf)

def call_ollama_json(
    ollama_url: Optional[str],
    model: str,
    system_prompt: str,
    user_prompt: str,
    *,
    connect_timeout_s: float = 3.0,
    read_timeout_s: float = 50.0,
    num_ctx: int = 6144,
    num_predict: int = 160,
    temperature: float = 0.2,
    extra_options: Optional[Dict[str, Any]] = None,
    options: Optional[Dict[str, Any]] = None,
    **_ignored_kwargs,
) -> Dict[str, Any]:
    """–í—ã–∑–æ–≤ Ollama /api/generate —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ç–∞–π–º–∞—É—Ç–æ–≤, —Å—Ç—Ä–∏–º-fallback –∏ –º–µ—Ç—Ä–∏–∫."""
    import json as _json
    try:
        if not ollama_url:
            ollama_url = cfg("ollama", "base_url", default="http://host.docker.internal:11434") or "http://host.docker.internal:11434"

        opts = {
            "num_ctx": int(num_ctx),
            "num_predict": int(num_predict),
            "temperature": float(temperature),
        }
        if options:
            bad = {"gpu_layers", "num_gpu", "main_gpu"}
            opts.update({k:v for k,v in options.items() if k not in bad})
        if extra_options:
            opts.update({k:v for k,v in extra_options.items() if k not in bad})

        payload = {
            "model": model,
            "prompt": user_prompt,
            "system": system_prompt,
            "format": "json",
            "options": opts,
            "keep_alive": -1,
            "stream": False,
        }

        try:
            resp = _HTTP.post(
                f"{ollama_url.rstrip('/')}/api/generate",
                json=payload,
                timeout=(float(connect_timeout_s), float(read_timeout_s)),
            )
            resp.raise_for_status()
        except requests.exceptions.ReadTimeout:
            # ‚¨áÔ∏è —Å—Ç—Ä–∏–º-fallback
            try:
                print("‚è© switch to streaming fallback")
                raw_stream = _ollama_generate_stream(
                    ollama_url, payload,
                    connect_timeout_s=connect_timeout_s, read_timeout_s=read_timeout_s
                )
                raw_stream = _trim_code_fences(raw_stream or "")
                if not raw_stream:
                    return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [], "disclaimer": f"LLM timeout: —á—Ç–µ–Ω–∏–µ >{read_timeout_s} —Å. (stream fallback empty)"}
                return safe_json_extract(raw_stream)
            except Exception as e2:
                return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [], "disclaimer": f"LLM timeout (stream fallback failed): {e2}"}

        # –ª–æ–≥ –º–µ—Ç—Ä–∏–∫ (–µ—Å–ª–∏ –Ω–µ —Å—Ç—Ä–∏–º)
        meta_logged = False
        if str(resp.headers.get("content-type", "")).startswith("application/json"):
            obj = resp.json()
            if isinstance(obj, dict):
                meta = {
                    "load_ms":   _ns_to_ms(obj.get("load_duration", 0)),
                    "prompt_ms": _ns_to_ms(obj.get("prompt_eval_duration", 0)),
                    "gen_ms":    _ns_to_ms(obj.get("eval_duration", 0)),
                    "total_ms":  _ns_to_ms(obj.get("total_duration", 0)),
                    "prompt_tok": obj.get("prompt_eval_count"),
                    "gen_tok":    obj.get("eval_count"),
                }
                print(f"üß™ OLLAMA META: {meta}")
                response_field = obj.get("response", "")
                meta_logged = True
            else:
                response_field = ""
        else:
            response_field = resp.text or ""

        raw = _json.dumps(response_field, ensure_ascii=False) if isinstance(response_field, (dict, list)) else f"{response_field}".strip()
        if not raw:
            return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [], "disclaimer": "LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç."}
        if not meta_logged:
            print("üß™ OLLAMA META: (no meta in response headers)")
        raw = _trim_code_fences(raw)
        return safe_json_extract(raw)

    except requests.exceptions.ConnectTimeout:
        return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [], "disclaimer": f"LLM timeout: —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ >{connect_timeout_s} —Å."}
    except Exception as e:
        return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [], "disclaimer": f"–û—à–∏–±–∫–∞ LLM ({type(e).__name__}): {e}"}

# ================================
# API models
# ================================
class AnalyzeReq(BaseModel):
    case_text: str
    query: Optional[str] = None
    k: int = Field(default_factory=lambda: cfg("retrieval", "k", default=8))
    model: str = Field(default_factory=lambda: cfg("ollama", "model", default="llama3.1:8b"))
    ollama_url: Optional[str] = Field(default_factory=lambda: cfg("ollama", "base_url", default="http://host.docker.internal:11434"))

# ================================
# Helpers
# ================================
def _resolve(name: str, default: str) -> str:
    return (os.getenv(name) or cfg(*name.lower().split("_"), default=None) or default)

# ================================
# Routes
# ================================
@app.get("/health")
def health():
    qdrant_url = _resolve("QDRANT_URL", "http://localhost:7779")
    collection = _resolve("QDRANT_COLLECTION", "med_kb_v3")
    emb_backend = os.getenv("EMB_BACKEND") or cfg("embedding", "backend", default="hf")
    hf_model = os.getenv("HF_MODEL") or cfg("embedding", "model", default="BAAI/bge-m3")
    device = os.getenv("HF_DEVICE") or cfg("embedding", "device", default="auto")
    return {
        "status": "ok",
        "app_env": os.getenv("APP_ENV", "dev"),
        "qdrant": qdrant_url,
        "qdrant_collection": collection,
        "llm_model": cfg("ollama", "model", default="llama3.1:8b"),
        "embed_backend": emb_backend,
        "embed_model": hf_model,
        "embed_device": device,
    }

@app.get("/debug/config")
def debug_config():
    return {
        "ollama": {
            "base_url": cfg_str("ollama", "base_url", default="http://host.docker.internal:11434"),
            "model": cfg_str("ollama", "model", default="llama3.1:8b"),
            "max_tokens": cfg_int("ollama", "max_tokens", default=2048),
            "timeout_s": cfg_int("ollama", "timeout_s", default=180),
            "temperature": cfg_float("ollama", "temperature", default=0.4),
            "top_p": cfg_float("ollama", "top_p", default=0.95),
            "num_ctx": cfg_int("ollama", "num_ctx", default=6144),
        },
        "qdrant": {
            "url": cfg_str("qdrant", "url", default="http://qdrant:6333"),
            "collection": cfg_str("qdrant", "collection", default="med_kb_v3"),
        },
        "retrieval": {"k": cfg_int("retrieval", "k", default=8)},
        "chunking": {
            "child_w": cfg_int("chunking", "child_w", default=200),
            "child_overlap": cfg_int("chunking", "child_overlap", default=35),
            "parent_w": cfg_int("chunking", "parent_w", default=800),
        },
    }

@app.post("/config/reload")
def config_reload():
    global CONFIG
    CONFIG = load_config()  # ‚Üê –ø–æ–¥—Ç—è–Ω–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π runtime_settings.py
    return {"status": "reloaded"}

@app.post("/analyze")
def analyze_ep(req: AnalyzeReq):
    try:
        print("üöÄ /analyze")
        t0 = time.perf_counter()

        if looks_meaningless(req.case_text):
            return {"result": {
                "score": 0, "subscores": {}, "critical_errors": [],
                "recommendations": [], "citations": [],
                "disclaimer": "–¢–µ–∫—Å—Ç –∫–µ–π—Å–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
            }}

        # --- –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å ---
        def _smart_query(case_text: str) -> str:
            m = re.search(r"\b([A-Za-z]\d{1,2}(?:\.\d+)?)\b", case_text)
            if m:
                return m.group(1)
            t = re.sub(r"\s+", " ", (case_text or "")).strip()
            return t[:200]

        query = req.query or _smart_query(req.case_text)
        print("üîç query =", query)

        # --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ---
        t_r0 = time.perf_counter()
        ctx_items = retrieve_hybrid(
            query, req.k,
            bm25_index_dir = cfg("app", "bm25_index_dir", default="index/bm25_idx"),
            qdrant_url     = cfg("qdrant", "url",        default="http://qdrant:6333"),
            qdrant_collection = cfg("qdrant", "collection", default="med_kb_v3"),
            pages_dir      = cfg("app", "data_dir",      default="data"),

            # —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
            hf_model  = cfg("embedding", "model",  default=os.getenv("HF_MODEL", "BAAI/bge-m3")),
            hf_device = cfg("embedding", "device", default=os.getenv("HF_DEVICE", "auto")),
            hf_fp16   = bool(cfg("embedding", "fp16",   default=False)),

            # —Å–∫–æ–ª—å–∫–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –º–∞–∫—Å–∏–º—É–º
            per_doc_limit = int(os.getenv("PER_DOC_LIMIT", cfg("app", "per_doc_limit", default=2))),

            # –†–ï–†–ê–ù–ö–ï–† ‚Äî –±–µ—Ä—ë–º –∏–∑ runtime_settings
            reranker_enabled = bool(cfg("reranker", "enabled", default=False)),
            reranker_model   = cfg("reranker", "model",   default=os.getenv("RERANKER_MODEL", "BAAI/bge-reranker-v2-m3")),
            reranker_topn    = int(cfg("reranker", "top_n", default=50)),
            reranker_device  = cfg("reranker", "device", default="cpu"),
)

        t_r1 = time.perf_counter()
        if not ctx_items:
            return {"result": {
                "score": 0, "subscores": {}, "critical_errors": [],
                "recommendations": [], "citations": [],
                "disclaimer": "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π ‚Äî –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –∫–µ–π—Å.",
            }}

        ctx = build_ctx_string(ctx_items, max_chars=8000, per_text_limit=800)
        print(f"üìè lengths: case={len(req.case_text)} ctx={len(ctx)} k={req.k}")

        # --- –ü—Ä–æ–º–ø—Ç ---
        DEFAULT_SYSTEM = (
            "–¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï. "
            "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∫–∞–∫ –≤—Ä–∞—á–µ–±–Ω—ã–π –∫–µ–π—Å –∏ –≤–µ—Ä–Ω–∏ –°–¢–†–û–ì–û –í–ê–õ–ò–î–ù–´–ô JSON —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ."
        )
        DEFAULT_USER_TPL = (
            "[–ö–ï–ô–°]\n{case_text}\n\n[–ö–û–ù–¢–ï–ö–°–¢]\n{ctx}\n\n"
            "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω –≤–∞–ª–∏–¥–Ω—ã–π JSON –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ."
        )
        system = cfg("prompt", "system", default=DEFAULT_SYSTEM) or DEFAULT_SYSTEM
        user_t = cfg("prompt", "user_template", default=DEFAULT_USER_TPL) or DEFAULT_USER_TPL
        user = user_t.format(case_text=req.case_text, ctx=ctx)

        # ‚îÄ‚îÄ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –±—é–¥–∂–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–Ω–æ –Ω–µ –≤—ã—à–µ num_ctx –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞)
        num_ctx_cap = cfg_int("ollama", "num_ctx", default=6144)
        total_est = _approx_tokens(system) + _approx_tokens(user)
        num_ctx = min(num_ctx_cap, max(3072, total_est + 256))

        # ‚îÄ‚îÄ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ¬´–∫–∞–∫ –¥–æ–ª–≥–æ –∏ –∫–∞–∫ –º–Ω–æ–≥–æ –¥—É–º–∞–µ–º¬ª
        llm_timeout   = cfg_int("ollama", "timeout_s", default=180)
        llm_max_tok   = cfg_int("ollama", "max_tokens", default=2048)
        llm_temp      = cfg_float("ollama", "temperature", default=0.4)
        llm_top_p     = cfg_float("ollama", "top_p", default=0.95)

        print(f"ü§ñ LLM url={req.ollama_url or cfg('ollama','base_url', default='N/A')} "
              f"model={req.model} num_ctx={num_ctx} max_tokens={llm_max_tok} timeout={llm_timeout}s")

        # --- –í—ã–∑–æ–≤ LLM (–ø–æ–ø—ã—Ç–∫–∞ 1) ---
        t_l0 = time.perf_counter()
        resp = call_ollama_json(
            req.ollama_url, req.model, system, user,
            read_timeout_s=float(llm_timeout),
            num_ctx=num_ctx,
            num_predict=int(llm_max_tok),
            temperature=float(llm_temp),
            options={"top_p": float(llm_top_p), "repeat_penalty": 1.05},
        )
        data = normalize_result(resp)
        t_l1 = time.perf_counter()

        def _is_empty(d):
            return (
                (d.get("score") in (None, 0)) and
                not d.get("subscores") and
                not d.get("critical_errors") and
                not d.get("recommendations")
            )
        timed_out = isinstance(resp, dict) and isinstance(resp.get("disclaimer"), str) and "timeout" in resp["disclaimer"]

        # --- –ï—Å–ª–∏ —Ç–∞–π–º–∞—É—Ç/–ø—É—Å—Ç–æ ‚Äî fast-retry ---
        if timed_out or _is_empty(data):
            print("‚è© fast-retry: shrinking context and num_predict")
            ctx_small = build_ctx_string(ctx_items[:min(3, len(ctx_items))], max_chars=6000, per_text_limit=700)
            user_small = user_t.format(case_text=req.case_text, ctx=ctx_small)
            total_est_small = _approx_tokens(system) + _approx_tokens(user_small)
            num_ctx_small = min(num_ctx_cap, max(3072, total_est_small + 128))
            retry_tokens = min(180, llm_max_tok)

            resp2 = call_ollama_json(
                req.ollama_url, req.model, system, user_small,
                read_timeout_s=float(llm_timeout),
                num_ctx=num_ctx_small,
                num_predict=int(retry_tokens),
                temperature=max(0.0, float(llm_temp) * 0.9),
                options={"top_p": float(llm_top_p), "repeat_penalty": 1.05},
            )
            data2 = normalize_result(resp2)
            if not _is_empty(data2):
                data = data2

            if "disclaimer" in data and isinstance(data["disclaimer"], str):
                if "timeout" in data["disclaimer"]:
                    data["disclaimer"] += " (–≤—ã–ø–æ–ª–Ω–µ–Ω fast-retry, —Å–æ–∫—Ä–∞—Ç–∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç/–æ—Ç–≤–µ—Ç)"
                else:
                    data["disclaimer"] = (data["disclaimer"] + " ") if data["disclaimer"] else ""
                    data["disclaimer"] += "–í—ã–ø–æ–ª–Ω–µ–Ω fast-retry: –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ —É–º–µ–Ω—å—à–µ–Ω—ã."

            if _is_empty(data) and not data.get("recommendations"):
                data["recommendations"] = [{
                    "what_to_change": "–£–º–µ–Ω—å—à–∏—Ç–µ K (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ 4‚Äì6) –∏–ª–∏ —É–∫–æ—Ä–æ—Ç–∏—Ç–µ –∫–µ–π—Å",
                    "rationale": "–°–ª–∏—à–∫–æ–º —Ç—è–∂—ë–ª—ã–π –ø—Ä–æ–º–ø—Ç –∑–∞–º–µ–¥–ª—è–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏."
                }]

        # --- –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ---
        ctx_len = sum(len(it.get("text", "")) for it in ctx_items)
        if ctx_len < 500:
            data["score"] = max(0, data.get("score", 0) * 0.5)
            data["disclaimer"] += " (–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ‚Äî –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —Å–Ω–∏–∂–µ–Ω–∞.)"
        elif ctx_len < 1500:
            data["score"] = max(0, data.get("score", 0) * 0.8)
            data["disclaimer"] += " (–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω ‚Äî –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —á–∞—Å—Ç–∏—á–Ω–æ —Å–Ω–∏–∂–µ–Ω–∞.)"

        # --- –¶–∏—Ç–∞—Ç—ã ---
        data["citations"] = build_context_citations(ctx_items, max_out=5) or [
            f"{it['doc_id']} —Å—Ç—Ä.{it['page_start']}-{it['page_end']}" for it in ctx_items[:5]
        ]

        # --- –®—Ç—Ä–∞—Ñ –∑–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ ---
        crit_count = len(data.get("critical_errors", []))
        if crit_count > 0:
            data["score"] = max(0, data["score"] - 10 * crit_count)
            data["disclaimer"] += f" (–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {crit_count} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫.)"

        t2 = time.perf_counter()
        print(f"‚è±Ô∏è perf: retrieval={int((t_r1-t_r0)*1000)}ms, llm={int((t_l1-t_l0)*1000)}ms, total={int((t2-t0)*1000)}ms")

        return {"result": data, "citations_used": [x["doc_id"] for x in ctx_items]}

    except Exception as e:
        import traceback
        print("‚ùå –û—à–∏–±–∫–∞ analyze_ep:\n", traceback.format_exc())
        return {
            "result": {
                "score": None, "subscores": {}, "critical_errors": [],
                "recommendations": [], "citations": [],
                "disclaimer": f"–û—à–∏–±–∫–∞ API: {e}",
            }
        }

# ================================
# UI
# ================================
UI_HTML = """<!doctype html><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤—Ä–∞—á–∞ (MVP)</title>
<style>
body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;background:#f6f7fb;margin:0;color:#101828}
.wrap{max-width:1100px;margin:20px auto;padding:16px}
.card{background:#fff;border:1px solid #e5e7eb;border-radius:16px;box-shadow:0 1px 3px rgba(16,24,40,.08);padding:16px;margin-bottom:16px}
h1{font-size:20px;margin:0 0 8px}
label{font-weight:600;font-size:14px;margin:6px 0;display:block}
input,select,textarea{width:100%;border:1px solid #d0d5dd;border-radius:10px;padding:10px;font-size:14px}
textarea{min-height:180px}
.row{display:grid;grid-template-columns:1fr 1fr;gap:12px}
.btn{background:#2563eb;color:#fff;border:none;border-radius:10px;padding:10px 14px;font-weight:600;cursor:pointer}
.btn:disabled{opacity:.6;cursor:not-allowed}
.badge{display:inline-block;border:1px solid #d0d5dd;border-radius:999px;padding:2px 8px;font-size:12px;margin-left:8px}
.mono{font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px}
.small{font-size:12px;color:#475467}
.err{color:#b91c1c}
.grid2{display:grid;grid-template-columns:1fr 1fr;gap:8px}
</style>
<div class="wrap">
  <div class="card">
    <h1>AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤—Ä–∞—á–∞ (MVP) <span id="score" class="badge">–æ—Ü–µ–Ω–∫–∞: ‚Äî</span></h1>
    <div class="small">API: <span id="api"></span></div>
  </div>
  <div class="card">
    <label>–¢–µ–∫—Å—Ç –∫–µ–π—Å–∞</label>
    <textarea id="caseText" placeholder="–í—Å—Ç–∞–≤—å—Ç–µ –∫–µ–π—Å: –∂–∞–ª–æ–±—ã, –∞–Ω–∞–º–Ω–µ–∑, –¥–∏–∞–≥–Ω–æ–∑, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è..."></textarea>
    <div class="row">
      <div>
        <label>–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)</label>
        <input id="query" placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä: –≥–∏–ø–µ—Ä—Ç–æ–Ω–∏—á–µ—Å–∫–∞—è –±–æ–ª–µ–∑–Ω—å –ª–µ—á–µ–Ω–∏–µ —ç–Ω–∞–ª–∞–ø—Ä–∏–ª">
      </div>
      <div>
        <label>–ú–æ–¥–µ–ª—å / K</label>
        <div class="row" style="grid-template-columns:2fr 1fr;gap:8px">
          <select id="model"><option>llama3.1:8b</option><option>llama3.1:70b</option></select>
          <input id="k" type="number" value="6" min="0" max="20">
        </div>
      </div>
    </div>
    <div style="margin-top:10px;display:flex;gap:8px;align-items:center">
      <button id="run" class="btn">–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å</button>
      <button id="reindex" class="btn" style="background:#059669">üîÑ –û–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É</button>
      <span id="busy" class="small" style="display:none">‚è≥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è‚Ä¶</span>
      <span id="error" class="small err"></span>
    </div>
  </div>
  <div class="card">
    <h3 style="margin:0 0 6px">–†–µ–∑—É–ª—å—Ç–∞—Ç</h3>
    <div class="grid2" id="subs"></div>
    <div><h4>–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏</h4><ul id="crit"></ul></div>
    <div><h4>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h4><ul id="recs"></ul></div>
    <div><h4>–ò—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ü–∏—Ç–∞—Ç—ã)</h4><ul id="cits"></ul></div>
    <details><summary class="small">–°—ã—Ä–æ–π JSON</summary><pre id="raw" class="mono"></pre></details>
  </div>
</div>
<script>
const API = window.location.origin; document.getElementById('api').textContent = API;
const el=id=>document.getElementById(id); const show=(n,on)=>n.style.display=on?'':'none';
function colorForScore(s){ if(typeof s!=='number') return ''; if(s>=85) return '#dcfce7'; if(s>=65) return '#fef9c3'; return '#fee2e2'; }
function renderResult(r){
  const sc=r.score??'‚Äî'; const sb=document.getElementById('score'); sb.textContent='–æ—Ü–µ–Ω–∫–∞: '+sc; sb.style.background=colorForScore(sc);
  const subs=el('subs'); subs.innerHTML=''; Object.entries(r.subscores||{}).forEach(([k,v])=>{
    const d=document.createElement('div'); d.className='card'; d.style.margin=0;
    d.innerHTML=`<div class="small">${labelMap[k] || k}</div><div style="font-weight:700">${v??'‚Äî'}</div>`;
    subs.appendChild(d);
  });
  const crit=el('crit'); crit.innerHTML=''; (r.critical_errors||[]).forEach(x=>{ const li=document.createElement('li'); li.textContent=`${x.type}: ${x.explain}`; crit.appendChild(li); });
  const recs=el('recs'); recs.innerHTML=''; (r.recommendations||[]).forEach(x=>{ const li=document.createElement('li'); li.textContent=`${x.what_to_change} ‚Äî ${x.rationale}`; recs.appendChild(li); });
  const cits=el('cits'); cits.innerHTML=''; (r.citations||[]).forEach(x=>{ const li=document.createElement('li'); li.textContent=String(x); cits.appendChild(li); });
  el('raw').textContent=JSON.stringify(r,null,2);
}
const labelMap = {
  "diagnosis": "–î–∏–∞–≥–Ω–æ–∑",
  "diagnosis_match": "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–∏–∞–≥–Ω–æ–∑—É",
  "therapy": "–¢–µ—Ä–∞–ø–∏—è",
  "med_choice": "–í—ã–±–æ—Ä –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞",
  "dosage": "–î–æ–∑–∏—Ä–æ–≤–∫–∞",
  "dosing": "–î–æ–∑–∏—Ä–æ–≤–∫–∞",
  "interactions": "–õ–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è",
  "contraindications": "–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è",
  "monitoring": "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥",
  "evidence": "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å"
};
async function run(){ el('error').textContent=''; show(el('busy'),true); el('run').disabled=true;
  try{
    const body={ case_text: el('caseText').value||'', query: el('query').value||null, k: parseInt(el('k').value||'6',10), model: el('model').value||'llama3.1:8b' };
    const res=await fetch(API+'/analyze',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify(body)});
    const txt=await res.text(); let data; try{ data=JSON.parse(txt); }catch(e){ throw new Error('–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å JSON –æ—Ç–≤–µ—Ç–∞: '+txt.slice(0,200)); }
    renderResult(data.result || data);
  }catch(e){ el('error').textContent='–û—à–∏–±–∫–∞: '+(e?.message||e); }
  finally{ show(el('busy'),false); el('run').disabled=false; }
}
el('run').onclick=run;

el('reindex').onclick = async () => {
  show(el('busy'), true);
  el('error').textContent = '';
  try {
    const res = await fetch(API + '/reindex', { method: 'POST' });
    const data = await res.json();
    el('error').textContent = data.message || '';
  } catch(e) {
    el('error').textContent = '–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: ' + e.message;
  } finally {
    show(el('busy'), false);
  }
};

async function checkReindexStatus() {
  try {
    const res = await fetch(API + '/reindex/status');
    const data = await res.json();
    const msg = data.message || '';
    const state = data.state;
    if (state === 'running') {
      el('busy').textContent = 'üîÑ ' + msg;
      show(el('busy'), true);
    } else if (state === 'done') {
      el('busy').textContent = '‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞';
      setTimeout(() => show(el('busy'), false), 4000);
    } else if (state === 'error') {
      el('busy').textContent = '‚ùå ' + msg;
    }
  } catch(e) { console.error(e); }
}
setInterval(checkReindexStatus, 3000);
</script>
"""

@app.get("/", response_class=HTMLResponse)
def ui_root():
    return HTMLResponse(UI_HTML)

# ================================
# Reindex
# ================================
index_status = {"state": "idle", "message": "–û–∂–∏–¥–∞–Ω–∏–µ"}


@app.get("/reindex/status")
def reindex_status():
    # –≤—Å–µ–≥–¥–∞ –æ–±—ä–µ–∫—Ç, —á—Ç–æ–±—ã —Ñ—Ä–æ–Ω—Ç –Ω–µ –ø–∞–¥–∞–ª
    return index_status

def run_reindex(*, full: bool = False):
    import os as _os
    import time as _time
    import socket as _socket
    import subprocess as _subprocess
    from pathlib import Path

    global index_status

    # --- helpers ---
    def _nz(val, default):
        s = (val or "").strip() if isinstance(val, str) else val
        return s if s not in (None, "", "None") else default

    def _as_int(val, fallback: int) -> int:
        try:
            if isinstance(val, (int, float)) and not isinstance(val, bool):
                return int(val)
            if isinstance(val, str) and val.strip() and val.strip().lower() != "none":
                return int(float(val.strip()))
        except Exception:
            pass
        return int(fallback)

    def _normalize_qdrant_url(url: str) -> str:
        try:
            if "qdrant:" in url:
                _socket.gethostbyname("qdrant")
        except Exception:
            return "http://localhost:7779"
        return url

    # —à—Ç–∞–º–ø —Å–≤–µ–∂–µ—Å—Ç–∏ BM25
    STAMP_BM25 = Path("index/.bm25_last_build")
    def _latest_pages_mtime() -> float:
        pages = list(Path("data").glob("*.pages.jsonl"))
        return max((p.stat().st_mtime for p in pages), default=0.0)

    def _bm25_needs_rebuild() -> bool:
        last_pages = _latest_pages_mtime()
        if last_pages == 0.0:
            return False
        if not STAMP_BM25.exists():
            return True
        return last_pages > STAMP_BM25.stat().st_mtime

    def _touch_bm25_stamp():
        STAMP_BM25.parent.mkdir(parents=True, exist_ok=True)
        STAMP_BM25.write_text(str(_time.time()), encoding="utf-8")

    try:
        env = _os.environ.copy()
        env["QDRANT__PREFER_GRPC"] = "false"

        # --- –®–∞–≥ 1: Ingest (–≤—Å–µ–≥–¥–∞) ---
        index_status.update({"state": "running", "message": "üìÑ –®–∞–≥ 1: –ø–∞—Ä—Å–∏–Ω–≥ RAW ‚Üí JSONL (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ)..."})
        print("‚ñ∂Ô∏è ingest_from_raw.py ...")
        cmd_ingest = ["python", "ingest_from_raw.py", "--input-dir", "raw_docs", "--out-dir", "data"]
        if full:
            cmd_ingest.append("--force")
        _subprocess.run(cmd_ingest, check=True, env=env)

        # --- –†–µ–∑–æ–ª–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤ ---
        qdrant_url = _normalize_qdrant_url(
            _nz(_os.getenv("QDRANT_URL") or cfg("qdrant", "url", default="http://qdrant:6333"),
                "http://localhost:7779")
        )
        collection = _nz(_os.getenv("QDRANT_COLLECTION") or cfg("qdrant", "collection", default="med_kb_v3"), "med_kb_v3")
        emb_backend = _nz(_os.getenv("EMB_BACKEND") or cfg("embedding", "backend", default="hf"), "hf")
        hf_model = _nz(
            _os.getenv("HF_MODEL") or
            cfg("embedding", "hf_model", default=cfg("embedding", "model", default="BAAI/bge-m3")),
            "BAAI/bge-m3"
        )

        child_w       = _as_int(_os.getenv("CHILD_W"),       cfg("chunking", "child_w",       default=200))
        child_overlap = _as_int(_os.getenv("CHILD_OVERLAP"), cfg("chunking", "child_overlap", default=35))
        parent_w      = _as_int(_os.getenv("PARENT_W"),      cfg("chunking", "parent_w",      default=800))

        print(
            "üîß RESOLVED ‚Üí "
            f"QDRANT_URL={qdrant_url}  QDRANT_COLLECTION={collection}  "
            f"EMB_BACKEND={emb_backend}  HF_MODEL={hf_model}  "
            f"child_w={child_w} child_overlap={child_overlap} parent_w={parent_w}"
        )

        # --- –®–∞–≥ 2: BM25 (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏–ª–∏ –ø—Ä–∏ full) ---
        if full or _bm25_needs_rebuild():
            index_status["message"] = "üìö –®–∞–≥ 2: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞..."
            print("‚ñ∂Ô∏è build_bm25.py ...")
            _subprocess.run(
                [
                    "python", "build_bm25.py",
                    "--pages-glob", "data/*.pages.jsonl",
                    "--out-json",   "index/bm25_json",
                    "--index-dir",  "index/bm25_idx",
                    # –µ—Å–ª–∏ –¥–æ–±–∞–≤–∏—à—å –≤ build_bm25.py –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π —Ä–µ–∂–∏–º, —Å—é–¥–∞ –º–æ–∂–Ω–æ –¥–æ–∫–∏–Ω—É—Ç—å —Ñ–ª–∞–≥:
                    # "--only-new",
                ],
                check=True, env=env
            )
            _touch_bm25_stamp()
        else:
            index_status["message"] = "‚è≠Ô∏è  –®–∞–≥ 2 –ø—Ä–æ–ø—É—â–µ–Ω: –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è BM25 –Ω–µ—Ç"
            print(index_status["message"])

        # --- –®–∞–≥ 3: Dense ‚Üí Qdrant ---
        index_status["message"] = "üß† –®–∞–≥ 3: –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant (dense)..."
        cmd_qdr = [
            "python", "chunk_and_index.py",
            "--pages-glob",    "data/*.pages.jsonl",
            "--collection",    collection,
            "--qdrant-url",    qdrant_url,
            "--emb-backend",   emb_backend,
            "--hf-model",      hf_model,
            "--batch",         "512",
            "--child-w",       str(child_w),
            "--child-overlap", str(child_overlap),
            "--parent-w",      str(parent_w),
        ]
        cmd_qdr.append("--recreate" if full else "--only-new")
        print("‚ñ∂Ô∏è CMD:", " ".join(cmd_qdr))
        _subprocess.run(cmd_qdr, check=True, env=env)

        index_status.update({"state": "done", "message": "‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞."})
        print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    except _subprocess.CalledProcessError as e:
        index_status.update({"state": "error", "message": f"‚ùå –ü—Ä–æ—Ü–µ—Å—Å —É–ø–∞–ª: {e}"})
        print(index_status["message"])
    except Exception as e:
        index_status.update({"state": "error", "message": f"‚ùå –û—à–∏–±–∫–∞: {e}"})
        print(index_status["message"])

@app.post("/reindex")
def reindex_ep(full: bool = False):
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–µ –∏ –í–°–ï–ì–î–ê –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±—ä–µ–∫—Ç —Å message
    threading.Thread(target=run_reindex, kwargs={"full": bool(full)}, daemon=True).start()
    return {"status": "started", "message": "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–ø—É—â–µ–Ω–∞", "full": bool(full)}