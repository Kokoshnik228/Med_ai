#!/usr/bin/env python3
from __future__ import annotations
from config.runtime_settings import settings


import json
import os
import re
import socket
import subprocess
import threading
import time
import importlib
from pathlib import Path
from typing import Any, Dict, List, Optional

# ================================
# .env -> runtime_settings (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫)
# ================================
try:
    from dotenv import load_dotenv
    env_mode = (os.getenv("APP_ENV") or "dev").strip().lower()
    env_file = Path(".env.dev" if env_mode == "dev" else ".env.prod")
    if env_file.exists():
        load_dotenv(dotenv_path=env_file)
        print(f"üîß Loaded env: {env_file}")
except Exception as e:
    print(f"‚ö†Ô∏è dotenv load skipped: {e}")

# runtime settings: –ø–æ—Å–ª–µ .env
from config.runtime_settings import settings  # noqa: E402
try:
    # –≤–∞–∂–Ω–æ–µ: –ø—Ä–æ–¥–∞–≤–ª–∏–≤–∞–µ–º CONTROL –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –æ–±—ä–µ–∫—Ç
    settings.apply_env(force=True)
except Exception:
    pass

# ================================
# –ò–º–ø–æ—Ä—Ç—ã –ø–æ—Å–ª–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è
# ================================
from glob import glob  # noqa: F401
import requests
import yaml
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse
from pydantic import BaseModel, Field

# pydantic v1/v2 —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
try:
    from pydantic import field_validator  # type: ignore
    _P_V2 = True
except Exception:
    from pydantic import validator as field_validator  # type: ignore
    _P_V2 = False

# RAG utils
from rag.bm25_utils import bm25_search, retrieve_hybrid, embed_query_hf  # noqa: F401

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è HTTP-—Å–µ—Å—Å–∏—è (keep-alive)
_HTTP = requests.Session()
_HTTP.headers.update({"Connection": "keep-alive"})

# ================================
# FastAPI
# ================================
app = FastAPI(title="med_ai RAG API", version="1.0")
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], allow_credentials=True,
    allow_methods=["*"], allow_headers=["*"]
)

# [api_app.py] ‚Äî –ø–æ–ª–æ–∂–∏ —ç—Ç–æ —Ä—è–¥–æ–º —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º–∏ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏
def _as_int(v, default):
    try:
        return int(str(v))
    except Exception:
        return default

def _as_float(v, default):
    try:
        return float(str(v))
    except Exception:
        return default

def _llm_conf_from_settings() -> dict:
    """
    –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤—ã–∑–æ–≤–∞ LLM.
    –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ–∑–∏—Ä–æ–≤–∞–Ω—ã: ENV > settings > –¥–µ—Ñ–æ–ª—Ç.
    –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–ª—é—á–µ–π, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ KeyError.
    """
    # –±–∞–∑–æ–≤—ã–µ –¥–µ—Ñ–æ–ª—Ç—ã
    d_model       = getattr(settings, "LLM_MODEL", "llama3.1:8b")
    d_base_url    = getattr(settings, "LLM_BASE_URL", "http://host.docker.internal:11434")
    d_num_ctx     = getattr(settings, "LLM_NUM_CTX", 4096)
    d_max_tokens  = getattr(settings, "LLM_MAX_TOKENS", 300)
    d_timeout_s   = getattr(settings, "LLM_TIMEOUT", 60)
    d_temp        = 0.4
    d_top_p       = 0.95
    d_repeat_pen  = 1.05
    d_gpu_layers  = -1
    d_keep_alive  = "30m"

    conf = {
        "model":        os.getenv("LLM_MODEL", d_model),
        "base_url":     os.getenv("LLM_BASE_URL", d_base_url),

        # –ª–∏–º–∏—Ç—ã
        "num_ctx_cap":  _as_int(os.getenv("LLM_NUM_CTX"), d_num_ctx),
        "max_tokens":   _as_int(os.getenv("LLM_MAX_TOKENS"), d_max_tokens),
        "timeout_s":    _as_int(os.getenv("LLM_TIMEOUT"), d_timeout_s),

        # —Å—ç–º–ø–ª–∏–Ω–≥
        "temperature":      _as_float(os.getenv("LLM_TEMPERATURE"), d_temp),
        "top_p":            _as_float(os.getenv("LLM_TOP_P"), d_top_p),
        "repeat_penalty":   _as_float(os.getenv("LLM_REPEAT_PENALTY"), d_repeat_pen),

        # GPU/—Å–µ–∞–Ω—Å
        "gpu_layers":   _as_int(os.getenv("LLM_NUM_GPU_LAYERS"), d_gpu_layers),
        "keep_alive":   os.getenv("LLM_KEEP_ALIVE", d_keep_alive),
    }
    return conf
# ================================
# Config (yaml + runtime overrides)
# ================================
ROOT = Path(__file__).resolve().parent
os.chdir(ROOT)
print(f"üìÇ CWD set to: {Path.cwd()}")
CONF_DIR = ROOT / "config"
DEFAULT_YAML = CONF_DIR / "default.yaml"
LOCAL_YAML = CONF_DIR / "local.yaml"


def load_runtime_overrides() -> Dict[str, Any]:
    """–ü–æ–¥—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç config/runtime_settings.py (dict RUNTIME), –º–æ–∂–Ω–æ –º–µ–Ω—è—Ç—å –±–µ–∑ –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∏."""
    try:
        import config.runtime_settings as rs  # type: ignore
        importlib.reload(rs)
        data = getattr(rs, "RUNTIME", None)
        if isinstance(data, dict):
            print("üîÅ runtime_settings.py loaded")
            return data
    except Exception as e:
        print(f"‚ö†Ô∏è runtime overrides not loaded: {e}")
    return {}

def env_bool(name: str, default: bool) -> bool:
    v = os.getenv(name)
    if v is None:
        return bool(default)
    v = str(v).strip().lower()
    if v in ("1", "true", "yes", "y", "on"):
        return True
    if v in ("0", "false", "no", "n", "off", ""):
        return False
    # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π: –ø–æ–ø—Ä–æ–±—É–µ–º –∫–∞–∫ int
    try:
        return bool(int(v))
    except Exception:
        return bool(default)

def _env_flag(name: str, default: bool) -> bool:
    v = os.getenv(name)
    if v is None:
        return bool(default)
    return str(v).strip().lower() in ("1", "true", "yes", "on")

def load_config() -> Dict[str, Any]:
    
    def _load_yaml(p: Path) -> Dict[str, Any]:
        try:
            data = yaml.safe_load(p.read_text(encoding="utf-8")) if p.exists() else {}
            return data if isinstance(data, dict) else {}
        except Exception:
            return {}

    DEFAULTS = {
        "app": {"data_dir": "data", "bm25_index_dir": "index/bm25_idx"},
        "qdrant": {
            "url": os.getenv("QDRANT_URL", "http://qdrant:6333"),
            "collection": os.getenv("QDRANT_COLLECTION", "med_kb_v3"),
        },
        "ollama": {
            "base_url": os.getenv("LLM_BASE_URL", "http://host.docker.internal:11434"),
            "model": os.getenv("LLM_MODEL", os.getenv("MODEL_ID", "llama3.1:8b")),
            "max_tokens": int(os.getenv("LLM_MAX_TOKENS", "2048")),
            "timeout_s": int(os.getenv("LLM_TIMEOUT", "60")),
            "temperature": float(os.getenv("LLM_TEMPERATURE", "0.4")),
            "top_p": float(os.getenv("LLM_TOP_P", "0.95")),
            "num_ctx": int(os.getenv("LLM_NUM_CTX", "6144")),
        },


        "retrieval": {"k": settings.RETR_TOP_K},
        "embedding": {
            "backend": os.getenv("EMB_BACKEND", settings.EMB_BACKEND or "hf"),
            "model": os.getenv("HF_MODEL", settings.HF_MODEL or "BAAI/bge-m3"),
            "device": os.getenv("HF_DEVICE", settings.HF_DEVICE or "auto"),
            "fp16": _env_flag("HF_FP16", bool(getattr(settings, "HF_FP16", True))),
        },

        
        "chunking": {"child_w": 200, "child_overlap": 35, "parent_w": 800},
        "prompt": {
            "system": (
                "–¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï. "
                "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∫–∞–∫ –≤—Ä–∞—á–µ–±–Ω—ã–π –∫–µ–π—Å: –≤ –Ω—ë–º –º–æ–≥—É—Ç –±—ã—Ç—å –∂–∞–ª–æ–±—ã, –∞–Ω–∞–º–Ω–µ–∑, –æ—Å–º–æ—Ç—Ä, –¥–∏–∞–≥–Ω–æ–∑ –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è. "
                "–†–∞—Å–ø–æ–∑–Ω–∞–π –¥–∏–∞–≥–Ω–æ–∑ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã–µ –º–µ—Ä—ã, —Å–æ–ø–æ—Å—Ç–∞–≤—å –∏—Ö —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. "
                "–ù–µ –ø–æ–≤—Ç–æ—Ä—è–π –∑–∞ –≤—Ä–∞—á–µ–º —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–æ, —Ç—ã –¥–æ–ª–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–æ–ø–æ–ª–Ω—è—Ç—å –µ–≥–æ —Ä–µ—á—å. "
                "–í–µ—Ä–Ω–∏ –°–¢–†–û–ì–û –í–ê–õ–ò–î–ù–´–ô JSON —Å–æ —Å—Ö–µ–º–æ–π:\n"
                "{score, subscores, critical_errors[], recommendations[], citations[], disclaimer}\n"
                "- score: —á–∏—Å–ª–æ 0..100 (—Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è).\n"
                "- subscores: –∫–∞—Ä—Ç–∞ –ø–æ–¥–æ—Ü–µ–Ω–æ–∫ (–Ω–∞–ø—Ä–∏–º–µ—Ä: dosing, diagnosis_match, interactions‚Ä¶).\n"
                "- critical_errors: —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ {type, explain}.\n"
                "- recommendations: —Å–ø–∏—Å–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤ {what_to_change, rationale}.\n"
                "- citations: —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫-–∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Ç–æ–ª—å–∫–æ –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –ö–û–ù–¢–ï–ö–°–¢–ê.\n"
                "- disclaimer: –∫–æ—Ä–æ—Ç–∫–æ–µ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.\n"
                "–ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–µ—Ç ‚Äî —Å–Ω–∏–∂–∞–π score, –¥–æ–±–∞–≤–ª—è–π –ø–æ—è—Å–Ω–µ–Ω–∏–µ –≤ disclaimer. –í–ù–ï–®–ù–ò–ï –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π."
            ),
            "user_template": (
                "[–ö–ï–ô–°]\n{case_text}\n\n"
                "[–ö–û–ù–¢–ï–ö–°–¢]\n{ctx}\n\n"
                "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω –≤–∞–ª–∏–¥–Ω—ã–π JSON –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ.–ë–µ–∑ Markdown. –í—Å–µ —Ç–µ–∫—Å—Ç—ã –≤–Ω—É—Ç—Ä–∏ ‚Äî –Ω–∞ —Ä—É—Å—Å–∫–æ–º. "
                "–ò—Å—Ç–æ—á–Ω–∏–∫–∏ —É–∫–∞–∑—ã–≤–∞–π —Ç–æ–ª—å–∫–æ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ë–µ–∑ –ø–æ—è—Å–Ω—è—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥."
            ),
        },
    }

    base = _load_yaml(DEFAULT_YAML)
    local = _load_yaml(LOCAL_YAML)
    runtime = load_runtime_overrides()

    def merge(a: Dict[str, Any], b: Dict[str, Any]) -> Dict[str, Any]:
        out = dict(a)
        for k, v in (b or {}).items():
            if isinstance(v, dict) and isinstance(out.get(k), dict):
                out[k] = merge(out[k], v)
            else:
                out[k] = v
        return out

    return merge(DEFAULTS, merge(base, merge(local, runtime)))

CONFIG = load_config()

def cfg(*path: str, default: Any = None) -> Any:
    cur: Any = CONFIG
    for p in path:
        if not isinstance(cur, dict) or p not in cur:
            return default
        cur = cur[p]
    return cur

def cfg_int(*path, default: int, allow_zero: bool = False) -> int:
    v = cfg(*path, default=None)
    try:
        v = int(v)
        if (not allow_zero and v <= 0) or (allow_zero and v < 0):
            raise ValueError
        return v
    except Exception:
        return int(default)

def cfg_float(*path, default: float) -> float:
    v = cfg(*path, default=None)
    try:
        return float(v)
    except Exception:
        return float(default)

def cfg_str(*path, default: str) -> str:
    v = cfg(*path, default=None)
    return str(v) if (v is not None and str(v).strip() != "") else str(default)

# ================================
# Warmup BM25 (–æ–¥–∏–Ω —Ä–∞–∑)
# ================================
WARMUP_DONE = False

@app.on_event("startup")
def warmup_bm25():
    global WARMUP_DONE
    if WARMUP_DONE or os.getenv("BM25_WARMUP_DISABLED") == "1":
        return
    try:
        idx = settings.BM25_INDEX_DIR
        bm25_search(idx, "—Ç–µ—Å—Ç", topk=1)  # –ø—Ä–æ–≥—Ä–µ–≤ JVM + –∏–Ω–¥–µ–∫—Å–∞
        print("üî• BM25 warmed up")
        WARMUP_DONE = True
    except Exception as e:
        print(f"‚ö†Ô∏è BM25 warmup skipped: {e}")

# ================================
# Utils
# ================================
def looks_meaningless(text: str) -> bool:
    t = (text or "").strip().lower()
    if len(t) < 3:
        return True
    if re.fullmatch(r"[a-z]\d{1,2}(\.\d+)?", t):
        return False
    if not re.search(r"[a-z–∞-—è—ë0-9]", t):
        return True
    letters_only = re.fullmatch(r"[a-z–∞-—è—ë\s]+", t)
    if letters_only and len(set(t)) < 5 and len(t) < 20:
        return True
    return False

def build_context_citations(ctx_items, max_out: int = 5):
    return [f"{it['doc_id']} —Å—Ç—Ä.{it['page_start']}-{it['page_end']}" for it in ctx_items[:max_out]]

def build_ctx_string(ctx_items, max_chars: int = 8000, per_text_limit: int = 800) -> str:
    parts, total = [], 0
    for i, it in enumerate(ctx_items, 1):
        txt = (it.get("text", "") or "")[:per_text_limit]
        chunk = f"### [{i}] DOC {it['doc_id']} P{it['page_start']}-{it['page_end']}\n{txt}\n\n"
        if total + len(chunk) > max_chars:
            break
        parts.append(chunk)
        total += len(chunk)
    return "".join(parts)

def _approx_tokens(s: str) -> int:
    return max(1, len(s) // 4)

# ================================
# Qdrant client (REST)
# ================================
def _qdrant_client_rest(url_override: Optional[str] = None):
    from qdrant_client import QdrantClient
    url = (url_override or cfg("qdrant", "url", default=settings.QDRANT_URL))
    if "qdrant:" in url:
        try:
            socket.gethostbyname("qdrant")
        except socket.gaierror:
            url = "http://localhost:7779"
    return QdrantClient(url=url, timeout=10, prefer_grpc=False, grpc_port=None)

# ================================
# LLM —á–µ—Ä–µ–∑ Ollama
# ================================
def _trim_code_fences(txt: str) -> str:
    txt = re.sub(r"^\s*```(?:json)?\s*", "", txt, flags=re.IGNORECASE)
    txt = re.sub(r"\s*```\s*$", "", txt)
    return txt.strip()

def safe_json_extract(s: str) -> Dict[str, Any]:
    import json as _json, re

    def _default():
        return {
            "score": None, "subscores": {}, "critical_errors": [],
            "recommendations": [], "citations": [],
            "disclaimer": "–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –Ω–µ —É–¥–∞–ª—Å—è."
        }

    if not s:
        return _default()

    s1 = re.sub(r"```(?:json)?", "", s, flags=re.IGNORECASE).replace("```", "").strip()

    try:
        obj = _json.loads(s1)
        if isinstance(obj, str):
            try:
                return _json.loads(obj)
            except Exception:
                pass
        if isinstance(obj, dict):
            return obj
    except Exception:
        pass

    start, depth, best = None, 0, None
    for i, ch in enumerate(s1):
        if ch == "{":
            if depth == 0:
                start = i
            depth += 1
        elif ch == "}" and depth > 0:
            depth -= 1
            if depth == 0 and start is not None:
                cand = (start, i + 1)
                if not best or (cand[1] - cand[0]) > (best[1] - best[0]):
                    best = cand
    if best:
        chunk = s1[best[0]:best[1]]
        try:
            return _json.loads(chunk)
        except Exception:
            chunk2 = re.sub(r",\s*([}\]])", r"\1", chunk)
            try:
                return _json.loads(chunk2)
            except Exception:
                pass

    try:
        unescaped = s1.encode("utf-8", "ignore").decode("unicode_escape")
        return _json.loads(unescaped)
    except Exception:
        pass

    return _default()

def normalize_result(r: Dict[str, Any]) -> Dict[str, Any]:
    out: Dict[str, Any] = {
        "score": 0, "subscores": {}, "critical_errors": [],
        "recommendations": [], "citations": [], "disclaimer": ""
    }
    try:
        sc = r.get("score", 0) if isinstance(r, dict) else 0
        out["score"] = max(0, min(100, float(sc))) if isinstance(sc, (int, float)) else 0.0
    except Exception:
        out["score"] = 0.0

    subs = r.get("subscores") if isinstance(r, dict) else {}
    if isinstance(subs, dict):
        clean = {}
        for k, v in subs.items():
            try:
                clean[str(k)] = max(0, min(100, float(v)))
            except Exception:
                pass
        out["subscores"] = clean

    ce = r.get("critical_errors") if isinstance(r, dict) else []
    clean_ce = []
    if isinstance(ce, list):
        for it in ce:
            if isinstance(it, dict):
                clean_ce.append({"type": str(it.get("type", "general")),
                                 "explain": str(it.get("explain", it.get("message", "")))})
            elif isinstance(it, str):
                clean_ce.append({"type": "general", "explain": it})
    out["critical_errors"] = clean_ce

    recs = r.get("recommendations") if isinstance(r, dict) else []
    clean_recs = []
    if isinstance(recs, list):
        for it in recs:
            if isinstance(it, dict):
                w = str(it.get("what_to_change") or it.get("action") or it.get("recommendation") or it.get("text", ""))
                ra = str(it.get("rationale") or it.get("reason", ""))
                if w or ra:
                    clean_recs.append({"what_to_change": w, "rationale": ra})
            elif isinstance(it, str):
                clean_recs.append({"what_to_change": it, "rationale": ""})
    out["recommendations"] = clean_recs

    cits = r.get("citations") if isinstance(r, dict) else []
    if isinstance(cits, list):
        out["citations"] = [str(x) for x in cits if isinstance(x, (str, int, float))]
    elif isinstance(cits, (str, int, float)):
        out["citations"] = [str(cits)]

    disc = r.get("disclaimer") if isinstance(r, dict) else ""
    out["disclaimer"] = str(disc) if disc is not None else ""
    return out

def _ns_to_ms(ns: int) -> int:
    try:
        return int(round(float(ns) / 1_000_000.0))
    except Exception:
        return 0

# --- Ollama HTTP helpers (stream-first) ---

# --- Ollama HTTP helpers: stream-first, no JSON format on stream ---

# == ollama_io.py == (–∏–ª–∏ –≥–¥–µ —É —Ç–µ–±—è –∂–∏–≤—É—Ç —ç—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–∏)
import json
import requests
from typing import Optional, Dict, Any

# –æ–±—â–∏–π http-—Å–µ—Å—Å
_HTTP = requests.Session()

def _trim_code_fences(s: str) -> str:
    s = (s or "").strip()
    if s.startswith("```"):
        s = s.strip("` \n")
        # —É–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å —Ç–∏–ø–∞ json
        s = s.split("\n", 1)[-1]
    return s.strip()

def safe_json_extract(s: str) -> Dict[str, Any]:
    s = _trim_code_fences(s)
    try:
        obj = json.loads(s)
        if isinstance(obj, dict):
            return obj
        # –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ ‚Äì –∑–∞–≤–µ—Ä–Ω—ë–º –≤ –æ–±—ä–µ–∫—Ç
        return {"result": obj}
    except Exception:
        # –≤–µ—Ä–Ω—ë–º ¬´–æ–±—ë—Ä—Ç–∫—É¬ª —Å –¥–∏—Å–∫–ª–µ–π–º–µ—Ä–æ–º, —á—Ç–æ–±—ã –Ω–µ –ø–∞–¥–∞—Ç—å
        return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [],
                "disclaimer": "LLM –≤–µ—Ä–Ω—É–ª –Ω–µ-JSON (stream relax)."}

def _ollama_generate_stream(
    ollama_url: str,
    payload: Dict[str, Any],
    *,
    per_chunk_timeout_s: float = 30.0,
    connect_timeout_s: float = 3.0,
    enforce_json_on_stream: bool = False,
) -> str:
    """
    –°—Ç—Ä–∏–º–∏–º —Ç–æ–∫–µ–Ω—ã. –í–ù–ò–ú–ê–ù–ò–ï: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ù–ï —Å—Ç–∞–≤–∏–º format=json –Ω–∞ —Å—Ç—Ä–∏–º–µ,
    —á—Ç–æ–±—ã –Ω–µ –∂–¥–∞—Ç—å ¬´—Ü–µ–ª—ã–π JSON¬ª –∏ –ø–æ–ª—É—á–∞—Ç—å —Ç–æ–∫–µ–Ω—ã —Å—Ä–∞–∑—É.
    """
    # –ì–æ—Ç–æ–≤–∏–º payload –î–õ–Ø –°–¢–†–ò–ú–ê: –£–ë–ò–†–ê–ï–ú format, –µ—Å–ª–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –∂—ë—Å—Ç–∫–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è.
    stream_payload = dict(payload)
    stream_payload["stream"] = True
    if not enforce_json_on_stream and "format" in stream_payload:
        stream_payload.pop("format", None)

    with _HTTP.post(
        f"{ollama_url.rstrip('/')}/api/generate",
        json=stream_payload,
        timeout=(float(connect_timeout_s), float(per_chunk_timeout_s)),
        stream=True,
    ) as r:
        r.raise_for_status()
        buf = []
        for ln in r.iter_lines(decode_unicode=True):
            if not ln:
                continue
            try:
                chunk = json.loads(ln)
                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç Ollama stream: {"response": "..." , "done": false}
                if "response" in chunk and chunk["response"]:
                    buf.append(chunk["response"])
                # –º–æ–∂–Ω–æ –ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å ¬´–ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω¬ª –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
            except Exception:
                # –µ—Å–ª–∏ –ø—Ä–∏–ª–µ—Ç–µ–ª–æ —á—Ç–æ-—Ç–æ –Ω–µ JSON (—Ä–µ–¥–∫–æ) ‚Äî –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                continue
        return "".join(buf)


def call_ollama_json(
    ollama_url: Optional[str],
    model: str,
    system_prompt: str,
    user_prompt: str,
    *,
    connect_timeout_s: float = 3.0,
    read_timeout_s: float = 90.0,
    stream_chunk_timeout_s: float = 30.0,
    num_ctx: int = 6144,
    num_predict: int = 160,
    temperature: float = 0.2,
    extra_options: Optional[Dict[str, Any]] = None,
    options: Optional[Dict[str, Any]] = None,
    keep_alive: Optional[str] = None,
    **_ignored_kwargs,
) -> Dict[str, Any]:
    """
    1) –ü—ã—Ç–∞–µ–º—Å—è —Å—Ç—Ä–∏–º–∏—Ç—å –ë–ï–ó format=json (–±—ã—Å—Ç—Ä–æ–µ –ø–æ—è–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤).
    2) –ï—Å–ª–∏ —Ç–∞–π–º–∞—É—Ç/–æ—à–∏–±–∫–∞ ‚Äî –¥–µ–ª–∞–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –±–ª–æ–∫–∏—Ä—É—é—â—É—é –ø–æ–ø—ã—Ç–∫—É —Å format=json.
    """
    try:
        if not ollama_url:
            # —Ç–≤–æ–π –¥–µ—Ñ–æ–ª—Ç, –∫–∞–∫ —Ä–∞–Ω—å—à–µ
            ollama_url = "http://host.docker.internal:11434"

        # —Å–æ–±–∏—Ä–∞–µ–º options (—Ñ–∏–ª—å—Ç—Ä—É–µ–º ¬´–æ–ø–∞—Å–Ω—ã–µ¬ª –∫–ª—é—á–∏)
        bad = {"gpu_layers", "num_gpu", "main_gpu"}  # –º—ã —Å–∞–º–∏ —è–≤–Ω–æ –ø—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º gpu_layers, –µ—Å–ª–∏ –Ω–∞–¥–æ
        opts = {
            "num_ctx": int(num_ctx),
            "num_predict": int(num_predict),
            "temperature": float(temperature),
        }
        if options:
            opts.update({k: v for k, v in options.items() if k not in bad})
        if extra_options:
            opts.update({k: v for k, v in extra_options.items() if k not in bad})

        base_payload = {
            "model": model,
            "prompt": user_prompt,
            "system": system_prompt,
            # –í–ù–ò–ú–ê–ù–ò–ï: format='json' –ù–ï —Å—Ç–∞–≤–∏–º –Ω–∞ —Å—Ç—Ä–∏–º–µ; –¥–æ–±–∞–≤–∏–º —Ç–æ–ª—å–∫–æ –≤ –±–ª–æ–∫–∏—Ä—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–µ
            "options": opts,
        }
        if keep_alive is not None:
            base_payload["keep_alive"] = keep_alive

        # --- 1) STREAM-FIRST (–±–µ–∑ format=json) ---
        try:
            print("LLM STREAM: start")
            text = _ollama_generate_stream(
                ollama_url,
                base_payload,
                per_chunk_timeout_s=stream_chunk_timeout_s,
                connect_timeout_s=connect_timeout_s,
                enforce_json_on_stream=False,  # –∫—Ä–∏—Ç–∏—á–Ω–æ
            )
            text = _trim_code_fences(text)
            if text:
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON ‚Äî –µ—Å–ª–∏ –Ω–µ –≤—ã–π–¥–µ—Ç, safe_json_extract –≤–µ—Ä–Ω—ë—Ç –≤–µ–∂–ª–∏–≤—É—é –æ–±—ë—Ä—Ç–∫—É.
                return safe_json_extract(text)
            else:
                print("LLM STREAM: empty stream result")
        except requests.exceptions.ReadTimeout:
            print(f"LLM STREAM: ReadTimeout (chunk {stream_chunk_timeout_s}s), fallback to short blocking call")
        except Exception as e:
            print(f"LLM STREAM: error={type(e).__name__}: {e}, fallback to blocking")

        # --- 2) –ë–ª–æ–∫–∏—Ä—É—é—â–∞—è –∫–æ—Ä–æ—Ç–∫–∞—è –ø–æ–ø—ã—Ç–∫–∞ c format=json ---
        short_payload = dict(base_payload)
        short_opts = dict(opts)
        # —Å–∏–ª—å–Ω–æ —É–∂–º—ë–º –¥–ª–∏–Ω—É –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è, —á—Ç–æ–±—ã –Ω–µ –∂–¥–∞—Ç—å
        short_opts["num_predict"] = min(80, int(opts.get("num_predict", 120)))
        short_payload["options"] = short_opts
        short_payload["format"] = "json"  # —Ç—É—Ç —É–∂–µ –º–æ–∂–Ω–æ —Ç—Ä–µ–±–æ–≤–∞—Ç—å ¬´–≤–∞–ª–∏–¥–Ω—ã–π JSON¬ª —Ü–µ–ª–∏–∫–æ–º
        try:
            resp = _HTTP.post(
                f"{ollama_url.rstrip('/')}/api/generate",
                json=short_payload,
                timeout=(float(connect_timeout_s), float(read_timeout_s)),
            )
            resp.raise_for_status()
            if str(resp.headers.get("content-type", "")).startswith("application/json"):
                obj = resp.json()
                # –û—Ç–≤–µ—Ç Ollama –≤ blocking —Ä–µ–∂–∏–º–µ: {"response": "<—Å—Ç—Ä–æ–∫–∞>", "done": true, ...}
                s = obj.get("response", "") if isinstance(obj, dict) else ""
            else:
                s = resp.text or ""
            s = _trim_code_fences(s)
            if not s:
                return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [],
                        "disclaimer": "LLM –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç (blocking fallback)."}
            return safe_json_extract(s)
        except requests.exceptions.ReadTimeout as e:
            return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [],
                    "disclaimer": f"LLM timeout: {e} (blocking fallback)"}
        except Exception as e:
            return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [],
                    "disclaimer": f"–û—à–∏–±–∫–∞ LLM ({type(e).__name__}): {e}"}

    except requests.exceptions.ConnectTimeout:
        return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [],
                "disclaimer": f"LLM timeout: —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ >{connect_timeout_s} c."}
    except Exception as e:
        return {"score": None, "subscores": {}, "critical_errors": [], "recommendations": [], "citations": [],
                "disclaimer": f"–û—à–∏–±–∫–∞ LLM ({type(e).__name__}): {e}"}

# ================================
# API models
# ================================
class AnalyzeReq(BaseModel):
    case_text: str
    query: Optional[str] = None
    k: Optional[int] = Field(default=None)
    model: str = Field(default_factory=lambda: cfg("ollama", "model", default="llama3.1:8b"))
    ollama_url: Optional[str] = Field(default_factory=lambda: cfg("ollama", "base_url", default="http://host.docker.internal:11434"))

    if _P_V2:
        @field_validator("k", mode="before")
        def _coerce_k_v2(cls, v):
            if v in (None, "", "null"):
                return settings.RETR_TOP_K
            try:
                return int(v)
            except Exception:
                return settings.RETR_TOP_K
    else:
        @field_validator("k", pre=True)
        def _coerce_k_v1(cls, v):
            if v in (None, "", "null"):
                return settings.RETR_TOP_K
            try:
                return int(v)
            except Exception:
                return settings.RETR_TOP_K

# ================================
# Helpers
# ================================
def _resolve(name: str, default: str) -> str:
    return (os.getenv(name) or cfg(*name.lower().split("_"), default=None) or default)

# ================================
# Routes
# ================================
@app.get("/health")
def health():
    qdrant_url = settings.QDRANT_URL
    collection = settings.QDRANT_COLLECTION
    emb_backend = settings.EMB_BACKEND
    hf_model = settings.HF_MODEL
    device = settings.HF_DEVICE or "auto"
    return {
        "status": "ok",
        "app_env": os.getenv("APP_ENV", "dev"),
        "qdrant": qdrant_url,
        "qdrant_collection": collection,
        "llm_model": cfg("ollama", "model", default="llama3.1:8b"),
        "embed_backend": emb_backend,
        "embed_model": hf_model,
        "embed_device": device,
    }

@app.get("/debug/config")
def debug_config():
    return {
        "ollama": {
            "base_url": cfg_str("ollama", "base_url", default="http://host.docker.internal:11434"),
            "model": cfg_str("ollama", "model", default="llama3.1:8b"),
            "max_tokens": cfg_int("ollama", "max_tokens", default=2048),
            "timeout_s": cfg_int("ollama", "timeout_s", default=60),
            "temperature": cfg_float("ollama", "temperature", default=0.4),
            "top_p": cfg_float("ollama", "top_p", default=0.95),
            "num_ctx": cfg_int("ollama", "num_ctx", default=6144),
        },
        "qdrant": {
            "url": settings.QDRANT_URL,
            "collection": settings.QDRANT_COLLECTION,
        },
        "retrieval": {"k": settings.RETR_TOP_K},
        "chunking": {
            "child_w": cfg_int("chunking", "child_w", default=200),
            "child_overlap": cfg_int("chunking", "child_overlap", default=35),
            "parent_w": cfg_int("chunking", "parent_w", default=800),
        },
    }

@app.post("/config/reload")
def config_reload():
    global CONFIG
    CONFIG = load_config()
    try:
        settings.apply_env(force=True)
    except Exception:
        pass
    return {"status": "reloaded"}

def _compact_case_text(txt: str, target_chars: int = 1400) -> str:
    if not txt:
        return ""
    t = txt

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–±–µ–ª–æ–≤/–ø–µ—Ä–µ–≤–æ–¥–æ–≤
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t).strip()

    # –£–±–∏—Ä–∞–µ–º —á–∞—Å—Ç–æ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –æ—Ç—Ä–∏—Ü–∞–Ω–∏—è/—à—Ç–∞–º–ø—ã
    t = re.sub(r"\b(–æ—Ç—Ä–∏—Ü–∞–µ—Ç|–Ω–µ\s+–æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ|–Ω–µ\s+–Ω–∞—Ö–æ–¥–∏–ª—Å—è|–Ω–µ\s+–æ—Ç—è–≥–æ—â–µ–Ω)\b[.,;:\s]*", "–Ω–µ—Ç. ", t, flags=re.IGNORECASE)
    t = re.sub(r"\b(–±–µ–∑–±–æ–ª–µ–∑–Ω–µ–Ω–Ω–∞—è|–∫–æ–∂–∞ –æ–±—ã—á–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏|—É–º–µ—Ä–µ–Ω–Ω–æ –≤–ª–∞–∂–Ω–∞—è|—Å–≤–æ–±–æ–¥–Ω\w*|–ø–æ —Å—Ä–µ–¥–Ω–µ–π –ª–∏–Ω–∏–∏)\b[.,;:\s]*", "", t, flags=re.IGNORECASE)

    # –°–∂–∏–º–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —á–∏—Å–ª–æ–≤—ã–µ —Ö–≤–æ—Å—Ç—ã/–ø–æ–≤—Ç–æ—Ä—ã –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
    t = re.sub(r"[.;:,]\s*(?:[.;:,]\s*)+", ". ", t)
    t = re.sub(r"\s{2,}", " ", t)

    # –ì—Ä—É–±–∞—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
    seen = set()
    out = []
    for sent in re.split(r"(?<=[.!?])\s+", t):
        s = sent.strip()
        key = re.sub(r"\W+", "", s.lower())
        if len(key) < 5:
            continue
        if key in seen:
            continue
        seen.add(key)
        out.append(s)
    t = " ".join(out)

    # –£—Ä—ñ–∑–∞–µ–º –¥–æ —Ü–µ–ª–µ–≤–æ–≥–æ –æ–∫–Ω–∞, –Ω–æ —Å—Ç–∞—Ä–∞–µ–º—Å—è –Ω–µ —Ä–µ–∑–∞—Ç—å –ø–æ—Å—Ä–µ–¥–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    if len(t) > target_chars:
        cut = t[:target_chars]
        last_dot = cut.rfind(".")
        if last_dot > target_chars * 0.6:
            t = cut[:last_dot+1]
        else:
            t = cut

    return t.strip()

@app.post("/analyze")
def analyze_ep(req: AnalyzeReq):
    try:
        print("üöÄ /analyze")
        import os, json, time, re
        t0 = time.perf_counter()
        # —Ç–∞–π–º–µ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º
        t_r0 = t_r1 = t_l0 = t_l1 = t0

        if looks_meaningless(req.case_text):
            return {"result": {
                "score": 0, "subscores": {}, "critical_errors": [],
                "recommendations": [], "citations": [],
                "disclaimer": "–¢–µ–∫—Å—Ç –∫–µ–π—Å–∞ –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.",
            }}

        # --- –°–±–æ—Ä –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ ---
        def _smart_query(case_text: str) -> str:
            m = re.search(r"\b([A-Za-z]\d{1,2}(?:\.\d+)?)\b", case_text)
            if m:
                return m.group(1)
            t = re.sub(r"\s+", " ", (case_text or "")).strip()
            return t[:200]

        diag_query = (getattr(req, "query", "") or "").strip()
        user_input_text = (getattr(req, "case_text", "") or "").strip()
        k = req.k if isinstance(req.k, int) and 0 <= req.k <= 20 else settings.RETR_TOP_K

        if diag_query:
            search_q = f"{diag_query}\n{user_input_text[:10000]}".strip() if user_input_text else diag_query
        else:
            base = _smart_query(user_input_text)
            search_q = f"{base}\n{user_input_text[:10000]}".strip() if user_input_text else base

        print("üîç query =", search_q)

        # --- –†–µ—Ç—Ä–∏–≤ ---
        t_r0 = time.perf_counter()
        ctx_items = retrieve_hybrid(
            search_q, k,
            bm25_index_dir=settings.BM25_INDEX_DIR,
            qdrant_url=settings.QDRANT_URL,
            qdrant_collection=settings.QDRANT_COLLECTION,
            pages_dir=settings.PAGES_DIR,
            hf_model=settings.HF_MODEL,
            hf_device=settings.HF_DEVICE,
            hf_fp16=settings.HF_FP16,
            per_doc_limit=settings.RETR_PER_DOC_LIMIT,
            reranker_enabled=settings.RERANKER_ENABLED,
            rerank_top_k=settings.RERANK_TOP_K,
        )
        t_r1 = time.perf_counter()

        if not ctx_items:
            return {"result": {
                "score": 0, "subscores": {}, "critical_errors": [],
                "recommendations": [], "citations": [],
                "disclaimer": "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π ‚Äî –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –∫–µ–π—Å.",
            }}

        ctx = build_ctx_string(
            ctx_items,
            max_chars=min(6000, settings.LLM_NUM_CTX * 3),  # –æ–±—â–∏–π ¬´–ø–æ—Ç–æ–ª–æ–∫¬ª –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (‚âà3 —Å–∏–º–≤/—Ç–æ–∫–µ–Ω)
            per_text_limit=settings.CTX_SNIPPET_LIMIT       # –±–µ—Ä—ë–º –∏–∑ runtime_settings.py
        )
        print(f"üìè lengths: case={len(req.case_text)} ctx={len(ctx)} k={k}")

        # --- –ü—Ä–æ–º–ø—Ç ---
        DEFAULT_SYSTEM = (
            "–¢—ã ‚Äî –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï. "
            "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç –∫–∞–∫ –≤—Ä–∞—á–µ–±–Ω—ã–π –∫–µ–π—Å –∏ –≤–µ—Ä–Ω–∏ –°–¢–†–û–ì–û –í–ê–õ–ò–î–ù–´–ô JSON —Å–æ–≥–ª–∞—Å–Ω–æ —Å—Ö–µ–º–µ."
        )
        DEFAULT_USER_TPL = (
            "[–ö–ï–ô–°]\n{case_text}\n\n[–ö–û–ù–¢–ï–ö–°–¢]\n{ctx}\n\n"
            "–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û –æ–¥–∏–Ω –≤–∞–ª–∏–¥–Ω—ã–π JSON –ø–æ —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ö–µ–º–µ."
        )
        system = cfg("prompt", "system", default=DEFAULT_SYSTEM) or DEFAULT_SYSTEM
        user_t = cfg("prompt", "user_template", default=DEFAULT_USER_TPL) or DEFAULT_USER_TPL
        user = user_t.format(case_text=req.case_text, ctx=ctx)

        # --- LLM-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ ---
        conf = _llm_conf_from_settings()
        total_est = _approx_tokens(system) + _approx_tokens(user)

        num_ctx_cap   = int(conf["num_ctx_cap"])               # –Ω–∞–ø—Ä., 16384
        min_ctx_env   = int(os.getenv("LLM_MIN_CTX", "4096"))  # –¥–∞—ë–º —É–ø—Ä–∞–≤–ª—è—Ç—å —á–µ—Ä–µ–∑ ENV
        ctx_margin    = int(os.getenv("LLM_CTX_MARGIN", "768"))

        MIN_CTX = min(num_ctx_cap, max(1024, min_ctx_env))
        num_ctx = max(MIN_CTX, min(num_ctx_cap, total_est + ctx_margin))

        print(
            f"ü§ñ LLM url={req.ollama_url or cfg('ollama','base_url', default='N/A')} "
            f"model={req.model} used_ctx={num_ctx} cap_ctx={num_ctx_cap} "
            f"min_ctx={MIN_CTX} max_tokens={conf['max_tokens']} timeout={conf['timeout_s']}s"
        )
        # –ª–æ–≥ –ø–æ–ª–µ–∑–Ω–æ–π —á–∞—Å—Ç–∏ payload
        _sanitized = {
            "model": req.model,
            "options": {
                "num_ctx": num_ctx,
                "num_predict": conf["max_tokens"],
                "temperature": conf["temperature"],
                "top_p": conf["top_p"],
                "repeat_penalty": conf["repeat_penalty"],
                "num_gpu_layers": conf["gpu_layers"],
            },
            "keep_alive": conf["keep_alive"],
        }
        print("LLM PAYLOAD (sanitized):", json.dumps(_sanitized, ensure_ascii=False))

        # --- –í—ã–∑–æ–≤ LLM (—Å—Ç—Ä–∏–º –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî —Å–º. Patch 2) ---
        t_l0 = time.perf_counter()
        resp = call_ollama_json(
            req.ollama_url, req.model, system, user,
            read_timeout_s=float(conf["timeout_s"]),
            num_ctx=num_ctx,
            num_predict=int(conf["max_tokens"]),
            temperature=float(conf["temperature"]),
            options={
                "top_p": float(conf["top_p"]),
                "repeat_penalty": float(conf["repeat_penalty"]),
                "num_gpu_layers": int(conf["gpu_layers"]) if conf["gpu_layers"] is not None else -1,
            },
            keep_alive=conf["keep_alive"],
            force_stream=True,                                  # —Å—Ç—Ä–∏–º —Å—Ä–∞–∑—É
            per_chunk_timeout_s=float(settings.LLM_STREAM_CHUNK_TIMEOUT),  # —Ç–∞–π–º–∞—É—Ç ¬´—Ç–∏—à–∏–Ω—ã¬ª –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        )

        data = normalize_result(resp)
        t_l1 = time.perf_counter()

        def _is_empty(d):
            return (
                (d.get("score") in (None, 0)) and
                not d.get("subscores") and
                not d.get("critical_errors") and
                not d.get("recommendations")
            )
        timed_out = isinstance(resp, dict) and isinstance(resp.get("disclaimer"), str) and "timeout" in resp["disclaimer"]

        # --- Fast-retry –ø—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ/–ø—É—Å—Ç–æ—Ç–µ ---
        if timed_out or _is_empty(data):
            print("‚è© fast-retry: shrinking context and num_predict")

            ctx_small = build_ctx_string(
                ctx_items[:min(3, len(ctx_items))],
                max_chars=6000,
                per_text_limit=700
            )
            user_small = user_t.format(case_text=req.case_text, ctx=ctx_small)

            total_est_small = _approx_tokens(system) + _approx_tokens(user_small)
            num_ctx_small = min(num_ctx_cap, max(1024, total_est_small + 128))
            retry_tokens = min(60, conf["max_tokens"])

            retry_opts = {
                "top_p": float(conf["top_p"]),
                "repeat_penalty": float(conf["repeat_penalty"]),
            }
            if conf.get("gpu_layers") is not None:
                retry_opts["num_gpu_layers"] = int(conf["gpu_layers"])

            # –ø–æ–≤—Ç–æ—Ä ‚Äî —Ç–æ–∂–µ —Å—Ç—Ä–∏–º–∏–º
            t_l0 = time.perf_counter()
            resp2 = call_ollama_json(
                req.ollama_url, req.model, system, user_small,
                read_timeout_s=float(conf["timeout_s"]),
                num_ctx=int(num_ctx_small),
                num_predict=int(retry_tokens),
                temperature=max(0.0, float(conf["temperature"]) * 0.9),
                options=retry_opts,
                keep_alive=conf["keep_alive"],
                force_stream=True,
                per_chunk_timeout_s=float(settings.LLM_STREAM_CHUNK_TIMEOUT),
            )

            data2 = normalize_result(resp2)
            if not _is_empty(data2):
                data = data2
            t_l1 = time.perf_counter()

            if "disclaimer" in data and isinstance(data["disclaimer"], str):
                if "timeout" in data["disclaimer"]:
                    data["disclaimer"] += " (–≤—ã–ø–æ–ª–Ω–µ–Ω fast-retry, —Å–æ–∫—Ä–∞—Ç–∏–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç/–æ—Ç–≤–µ—Ç)"
                else:
                    data["disclaimer"] = (data["disclaimer"] + " ") if data["disclaimer"] else ""
                    data["disclaimer"] += "–í—ã–ø–æ–ª–Ω–µ–Ω fast-retry: –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ —É–º–µ–Ω—å—à–µ–Ω—ã."

            if _is_empty(data) and not data.get("recommendations"):
                data["recommendations"] = [{
                    "what_to_change": "–£–º–µ–Ω—å—à–∏—Ç–µ K (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ 4‚Äì6) –∏–ª–∏ —É–∫–æ—Ä–æ—Ç–∏—Ç–µ –∫–µ–π—Å",
                    "rationale": "–°–ª–∏—à–∫–æ–º —Ç—è–∂—ë–ª—ã–π –ø—Ä–æ–º–ø—Ç –∑–∞–º–µ–¥–ª—è–µ—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏."
                }]

        # --- –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–ª–∏–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ---
        ctx_len = sum(len(it.get("text", "")) for it in ctx_items)
        if ctx_len < 500:
            data["score"] = max(0, data.get("score", 0) * 0.5)
            data["disclaimer"] += " (–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π ‚Äî –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —Å–Ω–∏–∂–µ–Ω–∞.)"
        elif ctx_len < 1500:
            data["score"] = max(0, data.get("score", 0) * 0.8)
            data["disclaimer"] += " (–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω ‚Äî –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å —á–∞—Å—Ç–∏—á–Ω–æ —Å–Ω–∏–∂–µ–Ω–∞.)"

        # --- –¶–∏—Ç–∞—Ç—ã ---
        data["citations"] = build_context_citations(ctx_items, max_out=5) or [
            f"{it['doc_id']} —Å—Ç—Ä.{it['page_start']}-{it['page_end']}" for it in ctx_items[:5]
        ]

        # --- –®—Ç—Ä–∞—Ñ –∑–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ ---
        crit_count = len(data.get("critical_errors", []))
        if crit_count > 0:
            data["score"] = max(0, data["score"] - 10 * crit_count)
            data["disclaimer"] += f" (–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {crit_count} –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–æ–∫.)"

        # --- –ü–µ—Ä—Ñ–æ–º–∞–Ω—Å-–ª–æ–≥ ---
        t2 = time.perf_counter()
        def _ms(a, b): return int((b - a) * 1000) if a is not None and b is not None else 0
        print(f"‚è±Ô∏è perf: retrieval={_ms(t_r0, t_r1)}ms, llm={_ms(t_l0, t_l1)}ms, total={_ms(t0, t2)}ms")

        return {"result": data, "citations_used": [x["doc_id"] for x in ctx_items]}

    except Exception as e:
        import traceback
        print("‚ùå –û—à–∏–±–∫–∞ analyze_ep:\n", traceback.format_exc())
        return {
            "result": {
                "score": None, "subscores": {}, "critical_errors": [],
                "recommendations": [], "citations": [],
                "disclaimer": f"–û—à–∏–±–∫–∞ API: {e}",
            }
        }

# ================================
# UI
# ================================
UI_HTML = """<!doctype html><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤—Ä–∞—á–∞ (MVP21)</title>
<style>
body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial,sans-serif;background:#f6f7fb;margin:0;color:#101828}
.wrap{max-width:1100px;margin:20px auto;padding:16px}
.card{background:#fff;border:1px solid #e5e7eb;border-radius:16px;box-shadow:0 1px 3px rgba(16,24,40,.08);padding:16px;margin-bottom:16px}
h1{font-size:20px;margin:0 0 8px}
label{font-weight:600;font-size:14px;margin:6px 0;display:block}
input,select,textarea{width:100%;border:1px solid #d0d5dd;border-radius:10px;padding:10px;font-size:14px}
textarea{min-height:180px}
.row{display:grid;grid-template-columns:1fr 1fr;gap:12px}
.btn{background:#2563eb;color:#fff;border:none;border-radius:10px;padding:10px 14px;font-weight:600;cursor:pointer}
.btn:disabled{opacity:.6;cursor:not-allowed}
.badge{display:inline-block;border:1px solid #d0d5dd;border-radius:999px;padding:2px 8px;font-size:12px;margin-left:8px}
.mono{font-family:ui-monospace,Menlo,Consolas,monospace;font-size:12px}
.small{font-size:12px;color:#475467}
.err{color:#b91c1c}
.grid2{display:grid;grid-template-columns:1fr 1fr;gap:8px}
.help{font-size:12px;color:#667085;margin-top:4px}
</style>
<div class="wrap">
  <div class="card">
    <h1>AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤—Ä–∞—á–∞ (MVP12) <span id="score" class="badge">–æ—Ü–µ–Ω–∫–∞: ‚Äî</span></h1>
    <div class="small">API: <span id="api"></span></div>
  </div>
  <div class="card">
    <label>–¢–µ–∫—Å—Ç –∫–µ–π—Å–∞</label>
    <textarea id="case" placeholder="–í—Å—Ç–∞–≤—å—Ç–µ –∫–µ–π—Å: –∂–∞–ª–æ–±—ã, –∞–Ω–∞–º–Ω–µ–∑, –¥–∏–∞–≥–Ω–æ–∑, –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è..."></textarea>
    <div class="row">
      <div>
        <label>–ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ (–Ω–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ)</label>
        <input id="query" placeholder="–Ω–∞–ø—Ä–∏–º–µ—Ä: –≥–∏–ø–µ—Ä—Ç–æ–Ω–∏—á–µ—Å–∫–∞—è –±–æ–ª–µ–∑–Ω—å –ª–µ—á–µ–Ω–∏–µ —ç–Ω–∞–ª–∞–ø—Ä–∏–ª">
      </div>
      <div>
        <label>–ú–æ–¥–µ–ª—å / K</label>
        <div class="row" style="grid-template-columns:2fr 1fr;gap:8px">
          <select id="model"><option>llama3.1:8b</option><option>llama3.1:70b</option></select>
          <input id="k" type="number" value="" min="0" max="20" placeholder="–ø–æ —É–º–æ–ª—á.">
        </div>
        <div class="help">–û—Å—Ç–∞–≤—å—Ç–µ K –ø—É—Å—Ç—ã–º ‚Äî –≤–æ–∑—å–º—ë—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å–µ—Ä–≤–µ—Ä–∞</div>
      </div>
    </div>
    <div style="margin-top:10px;display:flex;gap:8px;align-items:center">
      <button id="run" class="btn">–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å</button>
      <button id="reindex" class="btn" style="background:#059669">üîÑ –û–±–Ω–æ–≤–∏—Ç—å –±–∞–∑—É</button>
      <span id="busy" class="small" style="display:none">‚è≥ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è‚Ä¶</span>
      <span id="error" class="small err"></span>
    </div>
  </div>
  <div class="card">
    <h3 style="margin:0 0 6px">–†–µ–∑—É–ª—å—Ç–∞—Ç</h3>
    <div class="grid2" id="subs"></div>
    <div><h4>–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏</h4><ul id="crit"></ul></div>
    <div><h4>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏</h4><ul id="recs"></ul></div>
    <div><h4>–ò—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ü–∏—Ç–∞—Ç—ã)</h4><ul id="cits"></ul></div>
    <details><summary class="small">–°—ã—Ä–æ–π JSON</summary><pre id="raw" class="mono"></pre></details>
  </div>
</div>
<script>
const API = window.location.origin;
document.getElementById('api').textContent = API;

const el = id => document.getElementById(id);
const show = (n,on) => n.style.display = on ? '' : 'none';

function colorForScore(s){ if(typeof s!=='number') return ''; if(s>=85) return '#dcfce7'; if(s>=65) return '#fef9c3'; return '#fee2e2'; }

function renderResult(r){
  const sc = r.score ?? '‚Äî';
  const sb = el('score');
  sb.textContent = '–æ—Ü–µ–Ω–∫–∞: ' + sc;
  sb.style.background = colorForScore(sc);

  const subs = el('subs'); subs.innerHTML = '';
  Object.entries(r.subscores||{}).forEach(([k,v])=>{
    const d=document.createElement('div'); d.className='card'; d.style.margin=0;
    d.innerHTML=`<div class="small">${labelMap[k] || k}</div><div style="font-weight:700">${v??'‚Äî'}</div>`;
    subs.appendChild(d);
  });

  const crit=el('crit'); crit.innerHTML=''; (r.critical_errors||[]).forEach(x=>{
    const li=document.createElement('li'); li.textContent=`${x.type}: ${x.explain}`; crit.appendChild(li);
  });

  const recs=el('recs'); recs.innerHTML=''; (r.recommendations||[]).forEach(x=>{
    const li=document.createElement('li'); li.textContent=`${x.what_to_change} ‚Äî ${x.rationale}`; recs.appendChild(li);
  });

  const cits=el('cits'); cits.innerHTML=''; (r.citations||[]).forEach(x=>{
    const li=document.createElement('li'); li.textContent=String(x); cits.appendChild(li);
  });

  el('raw').textContent = JSON.stringify(r,null,2);
}

const labelMap = {
  "diagnosis": "–î–∏–∞–≥–Ω–æ–∑",
  "diagnosis_match": "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–∏–∞–≥–Ω–æ–∑—É",
  "therapy": "–¢–µ—Ä–∞–ø–∏—è",
  "med_choice": "–í—ã–±–æ—Ä –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞",
  "dosage": "–î–æ–∑–∏—Ä–æ–≤–∫–∞",
  "dosing": "–î–æ–∑–∏—Ä–æ–≤–∫–∞",
  "interactions": "–õ–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è",
  "contraindications": "–ü—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è",
  "monitoring": "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥",
  "evidence": "–î–æ–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å"
};

async function run(){
  el('error').textContent = '';
  show(el('busy'), true);
  el('run').disabled = true;

  try{
    const body = {
      case_text: (el('case').value || '').trim(),
      query:     (el('query').value || '').trim() || null,
      model:     el('model').value || 'llama3.1:8b'
    };

    const kRaw = (el('k').value || '').trim();
    if (kRaw !== '') {
      const kParsed = parseInt(kRaw, 10);
      if (Number.isFinite(kParsed)) body.k = kParsed;
    }

    const res = await fetch(API + '/analyze', {
      method:'POST',
      headers:{'Content-Type':'application/json'},
      body: JSON.stringify(body)
    });

    const txt = await res.text();
    let data; try{ data = JSON.parse(txt); }catch(e){
      throw new Error('–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å JSON –æ—Ç–≤–µ—Ç–∞: ' + txt.slice(0,200));
    }
    renderResult(data.result || data);
  }catch(e){
    el('error').textContent = '–û—à–∏–±–∫–∞: ' + (e?.message || e);
  }finally{
    show(el('busy'), false);
    el('run').disabled = false;
  }
}
el('run').onclick = run;

el('reindex').onclick = async () => {
  show(el('busy'), true);
  el('error').textContent = '';
  try {
    const res = await fetch(API + '/reindex', { method: 'POST' });
    const data = await res.json();
    el('error').textContent = data.message || '';
  } catch(e) {
    el('error').textContent = '–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: ' + e.message;
  } finally {
    show(el('busy'), false);
  }
};

async function checkReindexStatus() {
  try {
    const res = await fetch(API + '/reindex/status');
    const data = await res.json();
    const msg = data.message || '';
    const state = data.state;
    if (state === 'running') {
      el('busy').textContent = 'üîÑ ' + msg;
      show(el('busy'), true);
    } else if (state === 'done') {
      el('busy').textContent = '‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞';
      setTimeout(() => show(el('busy'), false), 4000);
    } else if (state === 'error') {
      el('busy').textContent = '‚ùå ' + msg;
    }
  } catch(e) { console.error(e); }
}
setInterval(checkReindexStatus, 3000);
</script>
"""

@app.get("/", response_class=HTMLResponse)
def ui_root():
    return HTMLResponse(UI_HTML)

# ================================
# Reindex
# ================================
index_status = {"state": "idle", "message": "–û–∂–∏–¥–∞–Ω–∏–µ"}

@app.get("/runtime/defaults")
def runtime_defaults():
    return {
        "RETR_TOP_K": settings.RETR_TOP_K,
        "RERANKER_ENABLED": settings.RERANKER_ENABLED,
        "RERANK_TOP_K": settings.RERANK_TOP_K,
        "HF_MODEL": settings.HF_MODEL,
    }

@app.get("/reindex/status")
def reindex_status():
    return index_status

def run_reindex(*, full: bool = False):
    import os as _os
    import time as _time
    import socket as _socket
    import subprocess as _subprocess
    from pathlib import Path

    # –ü—Ä–æ–¥–∞–≤–∏–º –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ runtime_settings.CONTROL –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–µ,
    # —á—Ç–æ–±—ã –æ–Ω–∏ —Å—Ç–∞–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –ø—Ä–∞–≤–¥—ã –¥–ª—è –≤—Å–µ—Ö –ø–æ–¥–ø—Ä–æ—Ü–µ—Å—Å–æ–≤.
    try:
        settings.apply_env(force=True)
    except Exception:
        pass

    global index_status

    base = ROOT
    raw_dir = str(base / "raw_docs")
    data_dir = str(base / "data")
    ingest_py = str(base / "ingest_from_raw.py")
    build_bm25_py = str(base / "build_bm25.py")
    chunk_and_index_py = str(base / "chunk_and_index.py")

    # ---------- helpers ----------
    def _nz(val, default):
        s = (val or "").strip() if isinstance(val, str) else val
        return s if s not in (None, "", "None") else default

    def _as_int(val, fallback: int) -> int:
        try:
            if isinstance(val, (int, float)) and not isinstance(val, bool):
                return int(val)
            if isinstance(val, str) and val.strip() and val.strip().lower() != "none":
                return int(float(val.strip()))
        except Exception:
            pass
        return int(fallback)

    def _normalize_qdrant_url(url: str) -> str:
        try:
            if "qdrant:" in url:
                _socket.gethostbyname("qdrant")
        except Exception:
            return "http://localhost:7779"
        return url

    # --- —à—Ç–∞–º–ø —Å–≤–µ–∂–µ—Å—Ç–∏ BM25 ---
    STAMP_BM25 = Path("index/.bm25_last_build")

    def _latest_pages_mtime() -> float:
        pages = list(Path("data").glob("*.pages.jsonl"))
        return max((p.stat().st_mtime for p in pages), default=0.0)

    def _bm25_needs_rebuild() -> bool:
        last_pages = _latest_pages_mtime()
        if last_pages == 0.0:
            return False
        if not STAMP_BM25.exists():
            return True
        return last_pages > STAMP_BM25.stat().st_mtime

    def _touch_bm25_stamp():
        STAMP_BM25.parent.mkdir(parents=True, exist_ok=True)
        STAMP_BM25.write_text(str(_time.time()), encoding="utf-8")

    try:
        env = _os.environ.copy()
        env["QDRANT__PREFER_GRPC"] = "false"

        # --- –®–∞–≥ 1: Ingest ---
        index_status.update({"state": "running", "message": "üìÑ –®–∞–≥ 1: –ø–∞—Ä—Å–∏–Ω–≥ RAW ‚Üí JSONL (–∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ)..."})
        print("‚ñ∂Ô∏è ingest_from_raw.py ...")

        cmd_ingest = ["python", ingest_py, "--input-dir", raw_dir, "--out-dir", data_dir]

        man = Path(data_dir) / "manifest.json"
        try:
            first_run = not man.exists() or not (json.loads(man.read_text(encoding="utf-8") or "{}").get("docs") or [])
        except Exception:
            first_run = True

        if full or first_run:
            cmd_ingest.append("--force")

        _subprocess.run(cmd_ingest, check=True, env=env)

        # --- –†–µ–∑–æ–ª–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (–≤—Å—ë —á–µ—Ä–µ–∑ env, –≤—ã—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–µ settings.apply_env) ---
        qdrant_url = _normalize_qdrant_url(
            _nz(_os.getenv("QDRANT_URL") or cfg("qdrant", "url", default=settings.QDRANT_URL),
                settings.QDRANT_URL)
        )
        collection  = _nz(_os.getenv("QDRANT_COLLECTION") or cfg("qdrant", "collection", default=settings.QDRANT_COLLECTION),
                          settings.QDRANT_COLLECTION)
        emb_backend = _nz(_os.getenv("EMB_BACKEND") or cfg("embedding", "backend", default=settings.EMB_BACKEND), "hf")
        hf_model    = _nz(_os.getenv("HF_MODEL") or cfg("embedding", "model", default=settings.HF_MODEL), settings.HF_MODEL)

        # Dense-—á–∞–Ω–∫–∏–Ω–≥
        child_w       = _as_int(_os.getenv("CHILD_W"),       cfg("chunking", "child_w",       default=200))
        child_overlap = _as_int(_os.getenv("CHILD_OVERLAP"), cfg("chunking", "child_overlap", default=35))
        parent_w      = _as_int(_os.getenv("PARENT_W"),      cfg("chunking", "parent_w",      default=800))

        # BM25-—á–∞–Ω–∫–∏–Ω–≥/—è–∑—ã–∫ (—É–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è runtime_settings —á–µ—Ä–µ–∑ ENV)
        bm25_child_w  = _as_int(_os.getenv("BM25_CHILD_W"),         _as_int(_os.getenv("CHILD_W"), 200))
        bm25_overlap  = _as_int(_os.getenv("BM25_CHILD_OVERLAP"),   _as_int(_os.getenv("CHILD_OVERLAP"), 40))
        bm25_lang     = _nz(_os.getenv("BM25_LANGUAGE"),            "ru")

        print(
            "üîß RESOLVED ‚Üí "
            f"QDRANT_URL={qdrant_url}  QDRANT_COLLECTION={collection}  "
            f"EMB_BACKEND={emb_backend}  HF_MODEL={hf_model}  "
            f"child_w={child_w} child_overlap={child_overlap} parent_w={parent_w}  "
            f"[BM25 child_w={bm25_child_w} overlap={bm25_overlap} lang={bm25_lang}]"
        )

        # --- –®–∞–≥ 2: BM25 ---
        if full or _bm25_needs_rebuild():
            index_status["message"] = "üìö –®–∞–≥ 2: –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞..."
            print("‚ñ∂Ô∏è build_bm25.py ...")
            _subprocess.run(
                [
                    "python", build_bm25_py,
                    "--pages-glob", "data/*.pages.jsonl",
                    "--out-json",   "index/bm25_json",
                    "--index-dir",  "index/bm25_idx",
                    "--child-w",    str(bm25_child_w),
                    "--child-overlap", str(bm25_overlap),
                    "--language",   bm25_lang,
                ],
                check=True, env=env
            )
            _touch_bm25_stamp()
        else:
            index_status["message"] = "‚è≠Ô∏è  –®–∞–≥ 2 –ø—Ä–æ–ø—É—â–µ–Ω: –Ω–æ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è BM25 –Ω–µ—Ç"
            print(index_status["message"])

        # --- –®–∞–≥ 3: Dense ‚Üí Qdrant ---
        index_status["message"] = "üß† –®–∞–≥ 3: –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ Qdrant (dense)..."
        cmd_qdr = [
            "python", chunk_and_index_py,
            "--pages-glob",    "data/*.pages.jsonl",
            "--collection",    collection,
            "--qdrant-url",    qdrant_url,
            "--emb-backend",   emb_backend,
            "--hf-model",      hf_model,
            "--batch",         "128",
            "--child-w",       str(child_w),
            "--child-overlap", str(child_overlap),
            "--parent-w",      str(parent_w),
        ]
        cmd_qdr.append("--recreate" if full else "--only-new")
        print("‚ñ∂Ô∏è CMD:", " ".join(cmd_qdr))
        _subprocess.run(cmd_qdr, check=True, env=env)

        index_status.update({"state": "done", "message": "‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞."})
        print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    except _subprocess.CalledProcessError as e:
        index_status.update({"state": "error", "message": f"‚ùå –ü—Ä–æ—Ü–µ—Å—Å —É–ø–∞–ª: {e}"})
        print(index_status["message"])
    except Exception as e:
        index_status.update({"state": "error", "message": f"‚ùå –û—à–∏–±–∫–∞: {e}"})
        print(index_status["message"])

@app.post("/reindex")
def reindex_ep(full: bool = False):
    threading.Thread(target=run_reindex, kwargs={"full": bool(full)}, daemon=True).start()
    return {"status": "started", "message": "–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–ø—É—â–µ–Ω–∞", "full": bool(full)}
