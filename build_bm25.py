#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ BM25-–∏–Ω–¥–µ–∫—Å–∞ (Pyserini/Lucene) –∏–∑ data/*.pages.jsonl.

–ß—Ç–æ —É–ª—É—á—à–µ–Ω–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –∏—Å—Ö–æ–¥–Ω–æ–π –≤–µ—Ä—Å–∏–µ–π:
  ‚Ä¢ –°—Ç—Ä–æ–≥–æ —Ç–∞–∫–∞—è –∂–µ —Å—Ö–µ–º–∞ —á–∞–Ω–∫–∏–Ω–≥–∞, –∫–∞–∫ –≤ dense-–ø–∞–π–ø–ª–∞–π–Ω–µ: –ø–æ —Å–ª–æ–≤–∞–º —Å –æ–∫–Ω–∞–º–∏
    (child_w=180, overlap=40 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é). –≠—Ç–æ —Å–Ω–∏–∂–∞–µ—Ç —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω –º–µ–∂–¥—É BM25 –∏ dense.
  ‚Ä¢ –†–æ–±–∞—Å—Ç–Ω–æ–µ —á—Ç–µ–Ω–∏–µ .pages.jsonl (–≥—Ä—è–∑–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç—Å—è, –Ω–µ –≤–∞–ª–∏–º —Å–±–æ—Ä–∫—É).
  ‚Ä¢ –ß—É—Ç—å –∞–∫–∫—É—Ä–∞—Ç–Ω–µ–µ –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –º—É—Å–æ—Ä (–æ–≥—Ä—ã–∑–∫–∏, ¬´—Ç–∞–±–ª–∏—á–Ω—ã–π —à—É–º¬ª, —Ä–µ—Ñ–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ —Ö–≤–æ—Å—Ç—ã).
  ‚Ä¢ –û–±–Ω–æ–≤–ª—è–µ–º –ò –∏–∑–º–µ–Ω–∏–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (uniqueDocid –æ–±–µ—Å–ø–µ—á–∏—Ç –∑–∞–º–µ–Ω—É),
    –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏–∑–º–µ–Ω–∏–≤—à–∏–µ—Å—è —á–µ—Ä–µ–∑ --skip-changed.
  ‚Ä¢ –ß—ë—Ç–∫–∞—è ¬´–¥–µ–ª—å—Ç–∞¬ª: –≤ Lucene –∏–¥—ë—Ç —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –Ω—É–∂–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å.

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
  pip install pyserini tqdm
  sudo apt install -y openjdk-17-jre-headless  # –¥–ª—è Lucene

–ü—Ä–∏–º–µ—Ä—ã:
  python build_bm25.py \
    --pages-glob "data/*.pages.jsonl" \
    --out-json index/bm25_json --index-dir index/bm25_idx

  # –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ (—Å–Ω–æ—Å –∏–Ω–¥–µ–∫—Å–∞ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è):
  python build_bm25.py --pages-glob "data/*.pages.jsonl" \
    --out-json index/bm25_json --index-dir index/bm25_idx --recreate
"""
from __future__ import annotations

import os
import argparse
import json
import subprocess
import re
import shutil
import hashlib
import unicodedata
from pathlib import Path
from typing import Dict, Any, List, Tuple
from tqdm import tqdm


# ------------------------- —É—Ç–∏–ª–∏—Ç—ã -------------------------

def _clean_jsonl_line(s: str) -> str:
    """–ú—è–≥–∫–∞—è —á–∏—Å—Ç–∫–∞ —Å—Ç—Ä–æ–∫–∏ JSONL (—É–±–∏—Ä–∞–µ–º NUL/–∫–æ–Ω—Ç—Ä.—Å–∏–º–≤–æ–ª—ã, –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º NFKC)."""
    if not s:
        return ""
    s = s.replace("\x00", "")
    s = "".join(ch for ch in s if ch.isprintable() or ch in "\t\r\n")
    return unicodedata.normalize("NFKC", s)


def _read_pages_robust(p: Path) -> Tuple[List[Dict[str, Any]], int]:
    """
    –ß–∏—Ç–∞–µ—Ç .pages.jsonl –ø–æ—Å—Ç—Ä–æ—á–Ω–æ. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (pages, skipped_count).
    –ü–ª–æ—Ö–∏–µ —Å—Ç—Ä–æ–∫–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ–º (–Ω–µ –≤–∞–ª–∏–º –≤–µ—Å—å –ø—Ä–æ—Ü–µ—Å—Å).
    –û–∂–∏–¥–∞–µ–º—ã–µ –ø–æ–ª—è: page(int), text(str).
    """
    pages: List[Dict[str, Any]] = []
    skipped = 0
    with p.open("r", encoding="utf-8", errors="ignore") as f:
        for i, line in enumerate(f, 1):
            line = _clean_jsonl_line(line).strip()
            if not line:
                continue
            try:
                rec = json.loads(line)
                if isinstance(rec, dict):
                    pages.append({
                        "page": int(rec.get("page", 0) or 0),
                        "text": rec.get("text", "") or "",
                    })
            except Exception as e:
                skipped += 1
                print(f"‚ö†Ô∏è  {p.name}: –±–∏—Ç–∞—è JSONL-—Å—Ç—Ä–æ–∫–∞ #{i}: {e}")
    return pages, skipped


def sha1_file(path: Path) -> str:
    h = hashlib.sha1()
    with path.open("rb") as f:
        for chunk in iter(lambda: f.read(1 << 20), b""):
            h.update(chunk)
    return h.hexdigest()


def words(text: str) -> List[str]:
    return (text or "").split()


def chunk_words(tokens: List[str], max_len: int, overlap: int) -> List[List[str]]:
    chunks: List[List[str]] = []
    i, n = 0, len(tokens)
    while i < n:
        j = min(i + max_len, n)
        chunks.append(tokens[i:j])
        if j == n:
            break
        i = max(0, j - overlap)
    return chunks


# –ü—Ä–∏–∑–Ω–∞–∫–∏ ¬´–º—É—Å–æ—Ä–∞¬ª –¥–ª—è BM25 (–æ—á–µ–Ω—å –ø—Ä–æ—Å—Ç—ã–µ –∏ –¥–µ—à—ë–≤—ã–µ)
_CITATION_LINE_RE = re.compile(
    r"(?xi)(?:^\s*\d{1,3}[\).\]]\s+|et\s*al\.?|doi[:\s/]|10\.\d{3,9}/\S+|\b\d{4}\b\s*;\s*\d+)",
)
def _drop_citation_like_lines(text: str) -> str:
    if not text:
        return text
    parts = re.split(r"(?:\n+|(?<=\.)\s+)", text)
    kept = [ln for ln in parts if ln and not _CITATION_LINE_RE.search(ln)]
    return "\n".join(kept).strip()


def is_noise_text(text: str, min_chars: int = 90) -> bool:
    """
    –ì—Ä—É–±—ã–π —Ñ–∏–ª—å—Ç—Ä –º—É—Å–æ—Ä–Ω—ã—Ö –∫—É—Å–∫–æ–≤ (–æ–≥–ª–∞–≤–ª–µ–Ω–∏—è, —Ç–∞–±–ª–∏—Ü—ã-–æ–≥—Ä—ã–∑–∫–∏, —á–∏—Å—Ç—ã–µ —Å–ø–∏—Å–∫–∏ —Å—Å—ã–ª–æ–∫).
    –õ—É—á—à–µ –Ω–µ–¥–æ—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å, —á–µ–º –ø–µ—Ä–µ–±–æ—Ä—â–∏—Ç—å.
    """
    if not text:
        return True
    t = text.strip()
    if len(t) < min_chars:
        return True

    # –Ø–≤–Ω—ã–µ ¬´—Ç–∞–±–ª–∏—á–Ω—ã–µ¬ª/—Å–≤–µ—Ä—Ö—Ü–∏—Ñ—Ä–æ–≤—ã–µ –∫—É—Å–∫–∏
    digits = sum(ch.isdigit() for ch in t)
    if digits / max(len(t), 1) > 0.30:
        return True

    # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ ‚Üí —Å–ø–∏—Å–æ–∫/–æ–≥—Ä—ã–∑–æ–∫
    punct = sum(ch in ".,;:¬∑‚Ä¢‚ñ™-|_/\\[]()" for ch in t)
    if punct / max(len(t), 1) > 0.28:
        return True

    # –ö—É—Å–æ–∫ –ø–æ—á—Ç–∏ —Ü–µ–ª–∏–∫–æ–º —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –∫–æ—Ä–æ—Ç–∫–∏—Ö ¬´—Å—Ç—Ä–æ–∫¬ª-—ç–ª–µ–º–µ–Ω—Ç–æ–≤ (—Ç–∏–ø–∞ —Å–ø–∏—Å–∫–∞ –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã)
    lines = [ln.strip() for ln in re.split(r"(?:\n+|(?<=\.)\s+)", t) if ln.strip()]
    if lines and sum(1 for ln in lines if len(ln) < 35) / len(lines) > 0.7:
        return True

    return False


def load_manifest(manifest_path: Path) -> Dict[str, Dict[str, Any]]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–∞–ø—É doc_id -> {sha1, source_path, ...}.
    –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç ‚Äî –≤–µ—Ä–Ω—ë—Ç –ø—É—Å—Ç—É—é –º–∞–ø—É.
    """
    if not manifest_path.exists():
        return {}
    try:
        j = json.loads(manifest_path.read_text(encoding="utf-8"))
        docs = j.get("docs") or []
        out: Dict[str, Dict[str, Any]] = {}
        for d in docs:
            if not isinstance(d, dict):
                continue
            did = str(d.get("doc_id") or "").strip()
            if not did:
                continue
            out[did] = d
        return out
    except Exception:
        return {}


def load_state(state_path: Path) -> Dict[str, str]:
    """
    –°–æ—Å—Ç–æ—è–Ω–∏–µ: doc_id -> sha1, —á—Ç–æ —É–∂–µ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ.
    """
    if not state_path.exists():
        return {}
    try:
        j = json.loads(state_path.read_text(encoding="utf-8"))
        if isinstance(j, dict):
            return {str(k): str(v) for k, v in j.items()}
    except Exception:
        pass
    return {}


def save_state(state_path: Path, state: Dict[str, str]) -> None:
    state_path.parent.mkdir(parents=True, exist_ok=True)
    state_path.write_text(json.dumps(state, ensure_ascii=False, indent=2), encoding="utf-8")


def decide_doc_sha1(doc_id: str, manifest_map: Dict[str, Dict[str, Any]], pages_file: Path) -> str:
    """
    –û–ø—Ä–µ–¥–µ–ª—è–µ–º ¬´–∫–æ–Ω—Ç—Ä–æ–ª—å–Ω—É—é —Å—É–º–º—É¬ª –¥–æ–∫—É–º–µ–Ω—Ç–∞:
    - –µ—Å–ª–∏ –≤ manifest –µ—Å—Ç—å sha1 –ø–æ doc_id, –±–µ—Ä—ë–º –µ–≥–æ;
    - –∏–Ω–∞—á–µ —Ö–µ—à–∏—Ä—É–µ–º —Å–∞–º .pages.jsonl (–ø–æ—Ö—É–∂–µ, –Ω–æ –≥–æ–¥–∏—Ç—Å—è).
    """
    md = manifest_map.get(doc_id)
    if md and md.get("sha1"):
        return str(md["sha1"])
    # fallback: —Ö–µ—à —Ñ–∞–π–ª–∞ —Å—Ç—Ä–∞–Ω–∏—Ü
    return sha1_file(pages_file)


# ------------------------- –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ—Ü–µ—Å—Å -------------------------

def main() -> int:
    ap = argparse.ArgumentParser(description="Incremental BM25 build (Pyserini/Lucene)")
    ap.add_argument("--language", default="ru", help="Analyzer language for Lucene (e.g., ru, en, ...)")
    ap.add_argument("--threads", type=int, default=os.cpu_count() or 4)
    ap.add_argument("--pages-glob", required=True, help='–ù–∞–ø—Ä–∏–º–µ—Ä: "data/*.pages.jsonl"')
    ap.add_argument("--out-json", default="index/bm25_json", help="–ö—É–¥–∞ –ø–∏—Å–∞—Ç—å per-doc JSON –¥–ª—è Pyserini")
    ap.add_argument("--index-dir", default="index/bm25_idx", help="–ö–∞—Ç–∞–ª–æ–≥ Lucene –∏–Ω–¥–µ–∫—Å–∞")

    # –í–∞–∂–Ω–æ: —Å–∏–Ω—Ö—Ä–æ–Ω —Å dense-–ø–∞–π–ø–ª–∞–π–Ω–æ–º (—É–∑–∫–∏–µ –æ–∫–Ω–∞ –ø–æ–≤—ã—à–∞—é—Ç —Ç–æ—á–Ω–æ—Å—Ç—å)
    ap.add_argument("--child-w", type=int, default=180, help="–†–∞–∑–º–µ—Ä child-–æ–∫–Ω–∞ (—Å–ª–æ–≤)")
    ap.add_argument("--child-overlap", type=int, default=40, help="–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ child-–æ–∫–æ–Ω (—Å–ª–æ–≤)")

    ap.add_argument("--manifest", default="data/manifest.json", help="–ü—É—Ç—å –∫ manifest.json –∏–∑ ingest")
    ap.add_argument("--state-path", default="index/.bm25_state.json", help="–ì–¥–µ —Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ (doc_id -> sha1)")
    ap.add_argument("--recreate", action="store_true", help="–°–Ω–µ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, —Å–æ–±—Ä–∞—Ç—å –∑–∞–Ω–æ–≤–æ")

    # –ü–æ–ª–∏—Ç–∏–∫–∞ –¥–ª—è –∏–∑–º–µ–Ω–∏–≤—à–∏—Ö—Å—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    ap.add_argument("--skip-changed", action="store_true",
                    help="–ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω, –∏–∑–º–µ–Ω–∏–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –±—É–¥—É—Ç –ü–†–û–ü–£–©–ï–ù–´ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º).")

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
    ap.add_argument("--min-chunk-chars", type=int, default=110, help="–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –≤ —Å–∏–º–≤–æ–ª–∞—Ö")

    args = ap.parse_args()

    pages_files = sorted(Path().glob(args.pages_glob))
    if not pages_files:
        raise SystemExit(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø–æ –º–∞—Å–∫–µ: {args.pages_glob}")

    out_json_dir = Path(args.out_json)
    out_json_dir.mkdir(parents=True, exist_ok=True)

    index_dir = Path(args.index_dir)
    index_dir.parent.mkdir(parents=True, exist_ok=True)

    state_path = Path(args.state_path)
    manifest_map = load_manifest(Path(args.manifest))

    # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ –∑–∞–ø—Ä–æ—Å—É
    if args.recreate:
        if index_dir.exists():
            print(f"üß® –£–¥–∞–ª—è—é –∏–Ω–¥–µ–∫—Å {index_dir} (recreate)...")
            shutil.rmtree(index_dir, ignore_errors=True)
        if state_path.exists():
            print(f"üß® –£–¥–∞–ª—è—é state {state_path} (recreate)...")
            try:
                state_path.unlink()
            except Exception:
                pass

    state = load_state(state_path)

    # --- –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–µ–ª—å—Ç—É: –∫–∞–∫–∏–µ doc_id –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω–∏–≤—à–∏–µ—Å—è ---
    to_process: List[Tuple[str, Path]] = []
    new_docs: List[str] = []
    changed_docs: List[str] = []

    for fp in pages_files:
        doc_id = fp.stem.replace(".pages", "")
        sha = decide_doc_sha1(doc_id, manifest_map, fp)
        prev = state.get(doc_id)

        if prev is None:
            new_docs.append(doc_id)
            to_process.append((doc_id, fp))
        elif prev != sha:
            changed_docs.append(doc_id)
            if not args.skip_changed:
                to_process.append((doc_id, fp))
        # –µ—Å–ª–∏ prev == sha ‚Üí —É–∂–µ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω; –ø—Ä–æ–ø—É—Å–∫–∞–µ–º

    if changed_docs:
        if args.skip_changed:
            print("‚ÑπÔ∏è  –ò–∑–º–µ–Ω–∏–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã, –Ω–æ –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã (--skip-changed).")
        else:
            print("‚ôªÔ∏è  –ò–∑–º–µ–Ω–∏–≤—à–∏–µ—Å—è –¥–æ–∫—É–º–µ–Ω—Ç—ã –±—É–¥—É—Ç –æ–±–Ω–æ–≤–ª–µ–Ω—ã (uniqueDocid –æ–±–µ—Å–ø–µ—á–∏—Ç –∑–∞–º–µ–Ω—É):")
        print("   doc_id:", ", ".join(changed_docs[:25]) + (" ‚Ä¶" if len(changed_docs) > 25 else ""))

    if not to_process:
        print("‚úÖ –ù–µ—Ç –Ω–æ–≤—ã—Ö/–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Äî –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        return 0

    # --- –ì–æ—Ç–æ–≤–∏–º delta-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: —Ç–æ–ª—å–∫–æ —Ç–æ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º ---
    delta_dir = index_dir.parent / "bm25_json_delta"
    if delta_dir.exists():
        shutil.rmtree(delta_dir, ignore_errors=True)
    delta_dir.mkdir(parents=True, exist_ok=True)

    # --- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è JSON –¥–ª—è Pyserini (—Ç–æ–ª—å–∫–æ –¥–ª—è –Ω—É–∂–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤) ---
    total_kept, total_skipped = 0, 0
    written_json_files: List[Path] = []

    for doc_id, fp in tqdm(to_process, desc="JSON build (delta)"):
        pages, bad = _read_pages_robust(fp)
        if bad:
            print(f"‚ö†Ô∏è  {doc_id}: –ø—Ä–æ–ø—É—â–µ–Ω–æ –±–∏—Ç—ã—Ö —Å—Ç—Ä–æ–∫ JSONL: {bad}")

        if not any((p.get("text") or "").strip() for p in pages):
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {fp.name}")
            continue

        kept = 0
        skipped = 0

        # –û—Å–Ω–æ–≤–Ω–æ–π JSON (per-doc) –≤ out_json_dir
        full_json_path = out_json_dir / f"{doc_id}.json"
        with full_json_path.open("w", encoding="utf-8") as fout:
            for p in pages:
                txt = p.get("text") or ""
                if not txt.strip():
                    continue

                # –õ—ë–≥–∫–∞—è –∑–∞—á–∏—Å—Ç–∫–∞ —Ö–≤–æ—Å—Ç–æ–≤-¬´–ª–∏—Ç–µ—Ä–∞—Ç—É—Ä—ã¬ª, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –ø—Ä–æ—Å–æ—á–∏–ª–∏—Å—å
                txt = _drop_citation_like_lines(txt)

                toks = words(txt)
                if not toks:
                    continue

                chunks = chunk_words(toks, args.child_w, args.child_overlap)
                for c_i, chunk in enumerate(chunks, start=1):
                    text = " ".join(chunk).strip()
                    if is_noise_text(text, min_chars=args.min_chunk_chars):
                        skipped += 1
                        continue

                    kept += 1
                    chunk_id = f"{doc_id}_p{int(p['page'])}_c{c_i}"
                    obj = {
                        "id": chunk_id,
                        "contents": text,
                        "raw": json.dumps({
                            "doc_id": doc_id,
                            "page": int(p["page"]),
                            "child_idx": c_i
                        }, ensure_ascii=False)
                    }
                    fout.write(json.dumps(obj, ensure_ascii=False) + "\n")

        if kept == 0:
            print(f"‚ö†Ô∏è {doc_id}: –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –Ω–µ—á–µ–≥–æ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å (–≤–æ–∑–º–æ–∂–Ω–æ, –º—É—Å–æ—Ä/—Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ).")
            # –Ω–µ —Å–æ–∑–¥–∞—ë–º –¥–µ–ª—å—Ç—É, –Ω–æ –æ—Å–Ω–æ–≤–Ω–æ–π JSON —É–∂–µ –ª–µ–∂–∏—Ç (0 —Å—Ç—Ä–æ–∫)
            continue

        # –î–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ ‚Äî –∫–æ–ø–∏—é –≤ delta_dir
        delta_json_path = delta_dir / f"{doc_id}.json"
        try:
            shutil.copy2(full_json_path, delta_json_path)
        except Exception:
            shutil.copy(full_json_path, delta_json_path)

        written_json_files.append(delta_json_path)
        total_kept += kept
        total_skipped += skipped
        print(f"  ‚úÖ {doc_id}: —á–∞–Ω–∫–æ–≤={kept}, –ø—Ä–æ–ø—É—â–µ–Ω–æ(–º—É—Å–æ—Ä)={skipped}")

    if not written_json_files:
        print("‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö JSON –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (–≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—É—Å—Ç—ã–µ/–æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã?)")
        return 0

    # --- –ó–∞–ø—É—Å–∫ Pyserini –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞ –ø–æ delta-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ ---
    cmd = [
        "python", "-m", "pyserini.index.lucene",
        "--collection", "JsonCollection",
        "--input", str(delta_dir.resolve()),
        "--index", str(index_dir.resolve()),
        "--generator", "DefaultLuceneDocumentGenerator",
        "--threads", str(args.threads),
        "--language", args.language,
        "--storePositions", "--storeDocvectors", "--storeRaw",
        "--uniqueDocid"  # –≤–∞–∂–Ω–æ: —Ç–æ—Ç –∂–µ id ‚Üí –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ, –±–µ–∑ –¥—É–±–ª–µ–π
    ]

    print("\n‚Üí –ó–∞–ø—É—Å–∫ –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä–∞ —Ç–æ–ª—å–∫–æ –ø–æ Œî (–Ω–æ–≤—ã–µ/–æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã):", " ".join(cmd))
    subprocess.run(cmd, check=True)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–µ–ª—å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    # --- –û–±–Ω–æ–≤–ª—è–µ–º state –ø–æ —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º ---
    state = load_state(state_path)  # –ø–µ—Ä–µ—á–∏—Ç–∞—Ç—å, –µ—Å–ª–∏ –∫—Ç–æ-—Ç–æ –ø–∏—Å–∞–ª –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    for doc_id, fp in to_process:
        sha = decide_doc_sha1(doc_id, manifest_map, fp)
        state[doc_id] = sha
    save_state(state_path, state)

    print("\nüìä –ò—Ç–æ–≥:")
    print(f"  –ù–æ–≤—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(new_docs)}")
    print(f"  –û–±–Ω–æ–≤–ª—ë–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {0 if args.skip_changed else len(changed_docs)}")
    print(f"  –î–æ–±–∞–≤–ª–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {total_kept}")
    print(f"  –ü—Ä–æ–ø—É—â–µ–Ω–æ –º—É—Å–æ—Ä–Ω—ã—Ö —á–∞–Ω–∫–æ–≤: {total_skipped}")
    print(f"  –ò–Ω–¥–µ–∫—Å: {index_dir}")
    print("‚úÖ –ì–æ—Ç–æ–≤–æ.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
