version: "3.9"

services:
  qdrant:
    image: qdrant/qdrant:latest
    container_name: medai_qdrant_dev
    restart: unless-stopped
    ports:
      - "7779:6333"
    volumes:
      - qdrant_storage_dev:/qdrant/storage

  app:
    # GPU-профиль включён по умолчанию через .env.dev → COMPOSE_PROFILES=gpu
    profiles:
      - gpu

    build:
      context: .
      # Поддержка CUDA-сборки torch из Dockerfile:
      # TORCH_CHANNEL пустой => CPU; cu124 => CUDA 12.4 и т.п.
      args:
        TORCH_CHANNEL: ${TORCH_CHANNEL:-https://download.pytorch.org/whl/cu128}
        TORCH_VERSION: ${TORCH_VERSION:-2.9.*}

    container_name: medai_app_dev
    restart: unless-stopped
    depends_on:
      - qdrant

    env_file:
      - .env.dev

    # GPU
    gpus: all
    shm_size: "1g"

    environment:
      # PyTorch allocator (ВАЖНО: именно двоеточие, не равно)
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128,garbage_collection_threshold:0.7,expandable_segments:True"
      TOKENIZERS_PARALLELISM: "false"
      CUDA_VISIBLE_DEVICES: "0"

      APP_HOST: "0.0.0.0"
      APP_PORT: "8000"
      QDRANT_URL: "http://qdrant:6333"
      LLM_BASE_URL: "http://host.docker.internal:11434"

      # Кэши под root в контейнере
      EASYOCR_DIR: "/root/.EasyOCR"
      EASYOCR_ALLOW_DOWNLOADS: "1"
      HF_HOME: "/root/.cache/huggingface"
      TRANSFORMERS_CACHE: "/root/.cache/huggingface"
      HUGGINGFACE_HUB_CACHE: "/root/.cache/huggingface"

      # GPU по умолчанию (runtime_settings может переопределить при apply_env)
      HF_DEVICE: "cuda"

    ports:
      - "7050:8000"

    volumes:
      - ./:/app
      - easyocr_cache:/root/.EasyOCR
      - hf_cache:/root/.cache/huggingface

    extra_hosts:
      - "host.docker.internal:host-gateway"

    command: ["bash", "start.sh"]

volumes:
  qdrant_storage_dev:
  easyocr_cache:
  hf_cache:
