services:
  qdrant:
    image: qdrant/qdrant:latest
    container_name: medai_qdrant_dev
    restart: unless-stopped
    ports:
      - "7779:6333"
    volumes:
      - qdrant_storage_dev:/qdrant/storage

  ollama:
    image: ollama/ollama:latest
    container_name: medai_ollama_dev
    restart: unless-stopped
    # ВАЖНО: порт наружу не публикуем — app ходит по http://ollama:11434 внутри сети
    gpus: all
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      OLLAMA_NUM_PARALLEL: "2"
      OLLAMA_KEEP_ALIVE: "30m"
    volumes:
      - ollama_models:/root/.ollama

  app:
    build:
      context: .
      args:
        TORCH_CHANNEL: ${TORCH_CHANNEL:-https://download.pytorch.org/whl/cu128}
        TORCH_VERSION: ${TORCH_VERSION:-2.9.*}
    container_name: medai_app_dev
    restart: unless-stopped
    depends_on:
      - qdrant
      - ollama
    env_file:
      - .env.dev
    gpus: all
    environment:
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:128,garbage_collection_threshold:0.7,expandable_segments:True"
      TOKENIZERS_PARALLELISM: "false"
      CUDA_VISIBLE_DEVICES: "0"

      APP_HOST: "0.0.0.0"
      APP_PORT: "8000"
      QDRANT_URL: "http://qdrant:6333"

      # Теперь ходим к Ollama по имени сервиса
      LLM_BASE_URL: "http://ollama:11434"

      # Кэши под root в контейнере
      EASYOCR_DIR: "/root/.EasyOCR"
      EASYOCR_ALLOW_DOWNLOADS: "1"
      HF_HOME: "/root/.cache/huggingface"
      TRANSFORMERS_CACHE: "/root/.cache/huggingface"
      HUGGINGFACE_HUB_CACHE: "/root/.cache/huggingface"

      HF_DEVICE: "auto"   # auto → увидит CUDA в контейнере
    ports:
      - "7050:8000"
    volumes:
      - ./:/app
      - easyocr_cache:/root/.EasyOCR
      - hf_cache:/root/.cache/huggingface
    extra_hosts:
      - "host.docker.internal:host-gateway"
    command: ["bash", "start.sh"]

volumes:
  qdrant_storage_dev:
  easyocr_cache:
  hf_cache:
  ollama_models:
