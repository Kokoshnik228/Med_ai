#!/usr/bin/env python3
"""
–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ BM25 (Lucene/Anserini/Pyserini).

–ë–µ—Ä—ë–º data/*.pages.jsonl (–ø–æ –æ–¥–Ω–æ–º—É —Ñ–∞–π–ª—É –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç) –∏ –≥–µ–Ω–µ—Ä–∏–º:
  - index/bm25_json/<doc_id>/*.json   ‚Äî per-doc/per-page JSON –¥–ª—è JsonCollection
  - index/bm25_idx/                   ‚Äî Lucene –∏–Ω–¥–µ–∫—Å
  - index/bm25_manifest.json          ‚Äî –º–∞–Ω–∏—Ñ–µ—Å—Ç —Å SHA1 –ø–æ pages.jsonl

–†–µ–∂–∏–º—ã:
  --recreate  : –ø–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ (—Å–Ω–æ—Å index/bm25_idx, –≥–µ–Ω–µ—Ä–∞—Ü–∏—è JSON –¥–ª—è –≤—Å–µ—Ö, –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ–≥–æ)
  --append    : (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é) –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ ‚Äî —Å–æ–∑–¥–∞—Ç—å JSON –∏ –¥–æ–∑–∞–ª–∏—Ç—å –≤ –∏–Ω–¥–µ–∫—Å —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:
  - openjdk —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–µ—Å—Ç—å –≤ Dockerfile)
  - pyserini –≤ requirements
  - –ü–∞–ø–∫–∞ —Å json'–∞–º–∏ –∏ –∏–Ω–¥–µ–∫—Å–æ–º –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞ –∑–∞–ø–∏—Å—å
"""

from __future__ import annotations
import argparse
import json
import os
import shutil
import sys
from pathlib import Path
from typing import Dict, List, Tuple
import hashlib
import glob
import subprocess

# ------------------------ utils ------------------------

def read_jsonl_pages(p: Path) -> List[Dict]:
    out = []
    with p.open("r", encoding="utf-8", errors="ignore") as f:
        for ln in f:
            ln = ln.strip()
            if not ln:
                continue
            try:
                obj = json.loads(ln)
                out.append(obj)
            except Exception:
                continue
    return out

def sha1_file(p: Path) -> str:
    h = hashlib.sha1()
    with p.open("rb") as f:
        for chunk in iter(lambda: f.read(1 << 20), b""):
            h.update(chunk)
    return h.hexdigest()

def ensure_dir(p: Path):
    p.mkdir(parents=True, exist_ok=True)

def rm_tree(p: Path):
    if p.exists():
        shutil.rmtree(p)

def hardlink_or_copy(src: Path, dst: Path):
    ensure_dir(dst.parent)
    try:
        os.link(src, dst)  # —ç–∫–æ–Ω–æ–º–∏–º –º–µ—Å—Ç–æ –∏ –≤—Ä–µ–º—è
    except OSError:
        shutil.copy2(src, dst)

# --------------------- json writer ---------------------

def write_doc_pages_json(
    pages: List[Dict],
    out_root: Path,
    doc_id: str
) -> List[Path]:
    """
    –î–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ doc_id —Å–æ–∑–¥–∞—ë–º –Ω–∞–±–æ—Ä JSON-—Ñ–∞–π–ª–æ–≤ —Ñ–æ—Ä–º–∞—Ç–∞ JsonCollection:
      { "id": "<doc_id>#p<page>", "contents": "<text>", "raw": "{\"doc_id\":...,\"page\":...}" }
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –∑–∞–ø–∏—Å–∞–ª–∏.
    """
    dst_dir = out_root / doc_id
    if dst_dir.exists():
        # –¥–æ–∫—É–º–µ–Ω—Ç –æ–±–Ω–æ–≤–∏–ª—Å—è ‚Äî —É–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã, —á—Ç–æ–±—ã –Ω–µ –æ—Å—Ç–∞–ª–æ—Å—å –º—É—Å–æ—Ä–∞
        shutil.rmtree(dst_dir)
    ensure_dir(dst_dir)

    written: List[Path] = []
    for rec in pages:
        page = int(rec.get("page", 1) or 1)
        text = (rec.get("text") or "").strip()
        # –¥–∞–∂–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å–æ–∑–¥–∞–¥–∏–º (–ø–æ—Ç–æ–º –º–æ–∂–Ω–æ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ –ø–æ–∏—Å–∫–µ)
        obj = {
            "id": f"{doc_id}#p{page}",
            "contents": text,
            "raw": json.dumps({"doc_id": doc_id, "page": page}, ensure_ascii=False),
        }
        out_file = dst_dir / f"{doc_id}_p{page}.json"
        out_file.write_text(json.dumps(obj, ensure_ascii=False), encoding="utf-8")
        written.append(out_file)
    return written

# --------------------- manifest ------------------------

def load_manifest(p: Path) -> Dict:
    if not p.exists():
        return {"docs": {}}
    try:
        data = json.loads(p.read_text(encoding="utf-8"))
        if not isinstance(data, dict) or "docs" not in data:
            return {"docs": {}}
        if not isinstance(data["docs"], dict):
            data["docs"] = {}
        return data
    except Exception:
        return {"docs": {}}

def save_manifest(p: Path, data: Dict):
    ensure_dir(p.parent)
    p.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")

# --------------------- indexer -------------------------

def run_pyserini_indexer(
    input_dir: Path,
    index_dir: Path,
    threads: int = 8,
    language: str = "ru",
    append: bool = False
):
    """
    –í—ã–∑—ã–≤–∞–µ–º Pyserini/Anserini –∏–Ω–¥–µ–∫—Å–∞—Ç–æ—Ä.
    –í–∞–∂–Ω–æ–µ: –¥–ª—è append –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—É—é input_dir, –≥–¥–µ –ª–µ–∂–∞—Ç –¢–û–õ–¨–ö–û –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã.
    """
    cmd = [
        sys.executable, "-m", "pyserini.index.lucene",
        "--collection", "JsonCollection",
        "--input", str(input_dir),
        "--index", str(index_dir),
        "--generator", "DefaultLuceneDocumentGenerator",
        "--threads", str(threads),
        "--storePositions", "--storeDocvectors", "--storeRaw",
        "--language", language,
    ]
    if append:
        cmd.append("--append")

    print("‚ñ∂Ô∏è  INDEX:", " ".join(cmd))
    subprocess.run(cmd, check=True)

# ---------------------- main --------------------------

def main() -> int:
    ap = argparse.ArgumentParser("BM25 builder (incremental)")
    ap.add_argument("--pages-glob", default="data/*.pages.jsonl",
                    help="–ì–ª–æ–± –ø–æ —Ñ–∞–π–ª–∞–º —Å—Ç—Ä–∞–Ω–∏—Ü (–ø–æ –æ–¥–Ω–æ–º—É jsonl –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç)")
    ap.add_argument("--out-json", default="index/bm25_json",
                    help="–ö—É–¥–∞ –∫–ª–∞—Å—Ç—å per-doc JSON –¥–ª—è JsonCollection")
    ap.add_argument("--index-dir", default="index/bm25_idx",
                    help="–ü–∞–ø–∫–∞ Lucene –∏–Ω–¥–µ–∫—Å–∞")
    ap.add_argument("--threads", type=int, default=max(2, (os.cpu_count() or 4) // 2))
    ap.add_argument("--language", default="ru", help="–Ø–∑—ã–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ Lucene (–Ω–∞–ø—Ä., ru, en)")
    ap.add_argument("--recreate", action="store_true", help="–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞")
    ap.add_argument("--append", dest="append", action="store_true", help="–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)")
    ap.add_argument("--only-new", dest="append", action="store_true", help="–°–∏–Ω–æ–Ω–∏–º append")
    ap.set_defaults(append=True)

    args = ap.parse_args()

    pages_files = sorted(glob.glob(args.pages_glob))
    if not pages_files:
        print(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –ø–æ —à–∞–±–ª–æ–Ω—É: {args.pages_glob}", file=sys.stderr)
        return 1

    out_json_root = Path(args.out_json)
    index_dir = Path(args.index_dir)
    manifest_path = out_json_root / "bm25_manifest.json"
    stage_dir = out_json_root.parent / "_bm25_stage"  # –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –Ω–æ–≤—ã—Ö/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö

    ensure_dir(out_json_root)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–π –º–∞–Ω–∏—Ñ–µ—Å—Ç
    manifest = load_manifest(manifest_path)
    docs_state: Dict[str, Dict] = manifest.get("docs", {})

    # –°–±–æ—Ä —Å–ø–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (doc_id -> (pages_path, sha1))
    docs_found: Dict[str, Tuple[Path, str]] = {}
    for pth in pages_files:
        p = Path(pth)
        # doc_id –±–µ—Ä—ë–º –∏–∑ —Å–∞–º–æ–≥–æ —Ñ–∞–π–ª–∞ (—Å—Ç—Ä–æ–∫–∏ jsonl —Å–æ–¥–µ—Ä–∂–∞—Ç doc_id)
        try:
            first_line = next(iter(read_jsonl_pages(p)), None)
            if not first_line:
                continue
            doc_id = str(first_line.get("doc_id") or p.stem)
        except StopIteration:
            continue
        sha = sha1_file(p)
        docs_found[doc_id] = (p, sha)

    if args.recreate:
        print("‚ôªÔ∏è  –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞: –æ—á–∏—â–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏ json‚Ä¶")
        rm_tree(index_dir)
        # JSON –ø–µ—Ä–µ–≥–µ–Ω–µ—Ä–∏–º –¥–ª—è –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        to_generate = list(docs_found.items())
        # —á–∏—Å—Ç–∏–º —Å—Ç–∞—Ä—ã–µ per-doc –ø–æ–¥–∫–∞—Ç–∞–ª–æ–≥–∏ (–∫—Ä–æ–º–µ —Å–ª—É–∂–µ–±–Ω—ã—Ö)
        for child in out_json_root.iterdir():
            if child.is_dir() and child.name not in (".", ".."):
                shutil.rmtree(child)
        docs_state = {}
    else:
        # –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ: –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–µ
        to_generate = []
        for doc_id, (pages_path, sha) in docs_found.items():
            prev = docs_state.get(doc_id)
            if not prev or prev.get("pages_sha1") != sha:
                to_generate.append((doc_id, (pages_path, sha)))

    print(f"üìÑ –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(docs_found)}; –∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {len(to_generate)}")

    # –ì–µ–Ω–µ—Ä–∏–º per-doc JSON
    generated_any = False
    for i, (doc_id, (pages_path, sha)) in enumerate(to_generate, 1):
        pages = read_jsonl_pages(pages_path)
        # safety: —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ page
        pages.sort(key=lambda r: int(r.get("page", 1) or 1))
        written = write_doc_pages_json(pages, out_json_root, doc_id)
        docs_state[doc_id] = {
            "pages_sha1": sha,
            "json_count": len(written),
            "json_dir": str((out_json_root / doc_id).resolve()),
            "pages_file": str(pages_path.resolve()),
        }
        generated_any = True
        if i % 20 == 0 or i == len(to_generate):
            print(f"  ‚îî‚îÄ [{i}/{len(to_generate)}] {doc_id}: —Å—Ç—Ä–∞–Ω–∏—Ü={len(written)}")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç
    manifest["docs"] = docs_state
    save_manifest(manifest_path, manifest)

    # –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    if args.recreate:
        # –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –≤–µ—Å—å out_json_root
        run_pyserini_indexer(out_json_root, index_dir, threads=args.threads,
                             language=args.language, append=False)
        print("‚úÖ –ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ BM25 –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
        return 0

    if not generated_any:
        print("‚è≠Ô∏è  –ù–µ—Ç –Ω–æ–≤—ã—Ö/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Äî –ø–µ—Ä–µ—Å–±–æ—Ä–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
        return 0

    # –°–æ–∑–¥–∞—ë–º stage —Å –¢–û–õ–¨–ö–û –Ω–æ–≤—ã–º–∏/–∏–∑–º–µ–Ω—ë–Ω–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    if stage_dir.exists():
        shutil.rmtree(stage_dir)
    ensure_dir(stage_dir)
    for doc_id, (_pages_path, _sha) in to_generate:
        src_dir = out_json_root / doc_id
        dst_dir = stage_dir / doc_id
        ensure_dir(dst_dir)
        for jf in src_dir.glob("*.json"):
            hardlink_or_copy(jf, dst_dir / jf.name)

    # append –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ stage
    run_pyserini_indexer(stage_dir, index_dir, threads=args.threads,
                         language=args.language, append=True)

    # —É–±–∏—Ä–∞–µ–º stage
    shutil.rmtree(stage_dir, ignore_errors=True)

    print("‚úÖ –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ BM25 –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")
    return 0


if __name__ == "__main__":
    raise SystemExit(main())
